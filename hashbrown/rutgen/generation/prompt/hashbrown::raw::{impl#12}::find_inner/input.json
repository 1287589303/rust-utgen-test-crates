{
  "system_pt": "As a software testing expert, infer the test input conditions or ranges based on the provided information. Follow these guidelines:\n1. Provide test input conditions or ranges in one line in plain text only, without additional explanations or Markdown formatting\n2. Analyze the function under test, context, preconditions, and expected return values or types to determine appropriate test input conditions or ranges\n3. The inferred test input conditions or ranges should comprehensively satisfy all provided preconditions simultaneously.\n4. Ensure the test input conditions or ranges cover boundary cases and edge scenarios\n",
  "static_pt": "The context for the focal function is as follows:\n// src/raw/mod.rs\n// crate name is hashbrown\nuse crate::alloc::alloc::{handle_alloc_error, Layout};\nuse crate::control::{BitMaskIter, Group, Tag, TagSliceExt};\nuse crate::scopeguard::{guard, ScopeGuard};\nuse crate::util::{invalid_mut, likely, unlikely};\nuse crate::TryReserveError;\nuse core::array;\nuse core::iter::FusedIterator;\nuse core::marker::PhantomData;\nuse core::mem;\nuse core::ptr::NonNull;\nuse core::slice;\nuse core::{hint, ptr};\n#[cfg(test)]\npub(crate) use self::alloc::AllocError;\npub(crate) use self::alloc::{do_alloc, Allocator, Global};\nstruct RawTableInner {\n    bucket_mask: usize,\n    ctrl: NonNull<u8>,\n    growth_left: usize,\n    items: usize,\n}\n#[derive(Copy, Clone)]\npub(crate) struct BitMask(pub(crate) BitMaskWord);\n#[derive(Clone)]\npub(crate) struct BitMaskIter(pub(crate) BitMask);\n#[derive(Clone)]\nstruct ProbeSeq {\n    pos: usize,\n    stride: usize,\n}\n#[derive(Copy, Clone, PartialEq, Eq)]\n#[repr(transparent)]\npub(crate) struct Tag(pub(super) u8);\nimpl RawTableInner {\n    #[cfg_attr(feature = \"inline-more\", inline)]\n    unsafe fn new_uninitialized<A>(\n        alloc: &A,\n        table_layout: TableLayout,\n        buckets: usize,\n        fallibility: Fallibility,\n    ) -> Result<Self, TryReserveError>\n    where\n        A: Allocator,\n    {\n        debug_assert!(buckets.is_power_of_two());\n        let (layout, ctrl_offset) = match table_layout.calculate_layout_for(buckets) {\n            Some(lco) => lco,\n            None => return Err(fallibility.capacity_overflow()),\n        };\n        let ptr: NonNull<u8> = match do_alloc(alloc, layout) {\n            Ok(block) => block.cast(),\n            Err(_) => return Err(fallibility.alloc_err(layout)),\n        };\n        let ctrl = NonNull::new_unchecked(ptr.as_ptr().add(ctrl_offset));\n        Ok(Self {\n            ctrl,\n            bucket_mask: buckets - 1,\n            items: 0,\n            growth_left: bucket_mask_to_capacity(buckets - 1),\n        })\n    }\n    #[inline]\n    fn fallible_with_capacity<A>(\n        alloc: &A,\n        table_layout: TableLayout,\n        capacity: usize,\n        fallibility: Fallibility,\n    ) -> Result<Self, TryReserveError>\n    where\n        A: Allocator,\n    {\n        if capacity == 0 {\n            Ok(Self::NEW)\n        } else {\n            unsafe {\n                let buckets = capacity_to_buckets(capacity)\n                    .ok_or_else(|| fallibility.capacity_overflow())?;\n                let mut result = Self::new_uninitialized(\n                    alloc,\n                    table_layout,\n                    buckets,\n                    fallibility,\n                )?;\n                result.ctrl_slice().fill_empty();\n                Ok(result)\n            }\n        }\n    }\n    fn with_capacity<A>(alloc: &A, table_layout: TableLayout, capacity: usize) -> Self\n    where\n        A: Allocator,\n    {\n        match Self::fallible_with_capacity(\n            alloc,\n            table_layout,\n            capacity,\n            Fallibility::Infallible,\n        ) {\n            Ok(table_inner) => table_inner,\n            Err(_) => unsafe { hint::unreachable_unchecked() }\n        }\n    }\n    #[inline]\n    unsafe fn fix_insert_slot(&self, mut index: usize) -> InsertSlot {}\n    #[inline]\n    fn find_insert_slot_in_group(\n        &self,\n        group: &Group,\n        probe_seq: &ProbeSeq,\n    ) -> Option<usize> {}\n    #[inline]\n    unsafe fn find_or_find_insert_slot_inner(\n        &self,\n        hash: u64,\n        eq: &mut dyn FnMut(usize) -> bool,\n    ) -> Result<usize, InsertSlot> {}\n    #[inline]\n    unsafe fn prepare_insert_slot(&mut self, hash: u64) -> (usize, Tag) {}\n    #[inline]\n    unsafe fn find_insert_slot(&self, hash: u64) -> InsertSlot {}\n    #[inline(always)]\n    unsafe fn find_inner(\n        &self,\n        hash: u64,\n        eq: &mut dyn FnMut(usize) -> bool,\n    ) -> Option<usize> {\n        let tag_hash = Tag::full(hash);\n        let mut probe_seq = self.probe_seq(hash);\n        loop {\n            let group = unsafe { Group::load(self.ctrl(probe_seq.pos)) };\n            for bit in group.match_tag(tag_hash) {\n                let index = (probe_seq.pos + bit) & self.bucket_mask;\n                if likely(eq(index)) {\n                    return Some(index);\n                }\n            }\n            if likely(group.match_empty().any_bit_set()) {\n                return None;\n            }\n            probe_seq.move_next(self.bucket_mask);\n        }\n    }\n    #[allow(clippy::mut_mut)]\n    #[inline]\n    unsafe fn prepare_rehash_in_place(&mut self) {}\n    #[inline]\n    unsafe fn iter<T>(&self) -> RawIter<T> {}\n    unsafe fn drop_elements<T>(&mut self) {}\n    unsafe fn drop_inner_table<T, A: Allocator>(\n        &mut self,\n        alloc: &A,\n        table_layout: TableLayout,\n    ) {}\n    #[inline]\n    unsafe fn bucket<T>(&self, index: usize) -> Bucket<T> {}\n    #[inline]\n    unsafe fn bucket_ptr(&self, index: usize, size_of: usize) -> *mut u8 {}\n    #[inline]\n    fn data_end<T>(&self) -> NonNull<T> {}\n    #[inline]\n    fn probe_seq(&self, hash: u64) -> ProbeSeq {\n        ProbeSeq {\n            pos: h1(hash) & self.bucket_mask,\n            stride: 0,\n        }\n    }\n    #[inline]\n    unsafe fn record_item_insert_at(&mut self, index: usize, old_ctrl: Tag, hash: u64) {}\n    #[inline]\n    fn is_in_same_group(&self, i: usize, new_i: usize, hash: u64) -> bool {}\n    #[inline]\n    unsafe fn set_ctrl_hash(&mut self, index: usize, hash: u64) {}\n    #[inline]\n    unsafe fn replace_ctrl_hash(&mut self, index: usize, hash: u64) -> Tag {}\n    #[inline]\n    unsafe fn set_ctrl(&mut self, index: usize, ctrl: Tag) {}\n    #[inline]\n    unsafe fn ctrl(&self, index: usize) -> *mut Tag {\n        debug_assert!(index < self.num_ctrl_bytes());\n        self.ctrl.as_ptr().add(index).cast()\n    }\n    fn ctrl_slice(&mut self) -> &mut [Tag] {}\n    #[inline]\n    fn buckets(&self) -> usize {}\n    #[inline]\n    unsafe fn is_bucket_full(&self, index: usize) -> bool {}\n    #[inline]\n    fn num_ctrl_bytes(&self) -> usize {}\n    #[inline]\n    fn is_empty_singleton(&self) -> bool {}\n    #[allow(clippy::mut_mut)]\n    #[inline]\n    fn prepare_resize<'a, A>(\n        &self,\n        alloc: &'a A,\n        table_layout: TableLayout,\n        capacity: usize,\n        fallibility: Fallibility,\n    ) -> Result<\n        crate::scopeguard::ScopeGuard<Self, impl FnMut(&mut Self) + 'a>,\n        TryReserveError,\n    >\n    where\n        A: Allocator,\n    {\n        debug_assert!(self.items <= capacity);\n        let new_table = RawTableInner::fallible_with_capacity(\n            alloc,\n            table_layout,\n            capacity,\n            fallibility,\n        )?;\n        Ok(\n            guard(\n                new_table,\n                move |self_| {\n                    if !self_.is_empty_singleton() {\n                        unsafe { self_.free_buckets(alloc, table_layout) };\n                    }\n                },\n            ),\n        )\n    }\n    #[allow(clippy::inline_always)]\n    #[inline(always)]\n    unsafe fn reserve_rehash_inner<A>(\n        &mut self,\n        alloc: &A,\n        additional: usize,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        fallibility: Fallibility,\n        layout: TableLayout,\n        drop: Option<unsafe fn(*mut u8)>,\n    ) -> Result<(), TryReserveError>\n    where\n        A: Allocator,\n    {}\n    #[inline(always)]\n    unsafe fn full_buckets_indices(&self) -> FullBucketsIndices {}\n    #[allow(clippy::inline_always)]\n    #[inline(always)]\n    unsafe fn resize_inner<A>(\n        &mut self,\n        alloc: &A,\n        capacity: usize,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        fallibility: Fallibility,\n        layout: TableLayout,\n    ) -> Result<(), TryReserveError>\n    where\n        A: Allocator,\n    {}\n    #[allow(clippy::inline_always)]\n    #[cfg_attr(feature = \"inline-more\", inline(always))]\n    #[cfg_attr(not(feature = \"inline-more\"), inline)]\n    unsafe fn rehash_in_place(\n        &mut self,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        size_of: usize,\n        drop: Option<unsafe fn(*mut u8)>,\n    ) {}\n    #[inline]\n    unsafe fn free_buckets<A>(&mut self, alloc: &A, table_layout: TableLayout)\n    where\n        A: Allocator,\n    {}\n    #[inline]\n    unsafe fn allocation_info(\n        &self,\n        table_layout: TableLayout,\n    ) -> (NonNull<u8>, Layout) {}\n    #[inline]\n    unsafe fn allocation_size_or_zero(&self, table_layout: TableLayout) -> usize {}\n    #[inline]\n    fn clear_no_drop(&mut self) {}\n    #[inline]\n    unsafe fn erase(&mut self, index: usize) {}\n}\nimpl IntoIterator for BitMask {\n    type Item = usize;\n    type IntoIter = BitMaskIter;\n    #[inline]\n    fn into_iter(self) -> BitMaskIter {\n        BitMaskIter(BitMask(self.0 & BITMASK_ITER_MASK))\n    }\n}\nimpl Iterator for BitMaskIter {\n    type Item = usize;\n    #[inline]\n    fn next(&mut self) -> Option<usize> {\n        let bit = self.0.lowest_set_bit()?;\n        self.0 = self.0.remove_lowest_bit();\n        Some(bit)\n    }\n}\n#[allow(clippy::use_self)]\nimpl BitMask {\n    #[inline]\n    #[must_use]\n    #[allow(dead_code)]\n    pub(crate) fn invert(self) -> Self {\n        BitMask(self.0 ^ BITMASK_MASK)\n    }\n    #[inline]\n    #[must_use]\n    fn remove_lowest_bit(self) -> Self {\n        BitMask(self.0 & (self.0 - 1))\n    }\n    #[inline]\n    pub(crate) fn any_bit_set(self) -> bool {\n        self.0 != 0\n    }\n    #[inline]\n    pub(crate) fn lowest_set_bit(self) -> Option<usize> {}\n    #[inline]\n    pub(crate) fn trailing_zeros(self) -> usize {}\n    #[inline]\n    fn nonzero_trailing_zeros(nonzero: NonZeroBitMaskWord) -> usize {}\n    #[inline]\n    pub(crate) fn leading_zeros(self) -> usize {}\n}\nimpl ProbeSeq {\n    #[inline]\n    fn move_next(&mut self, bucket_mask: usize) {\n        debug_assert!(self.stride <= bucket_mask, \"Went past end of probe sequence\");\n        self.stride += Group::WIDTH;\n        self.pos += self.stride;\n        self.pos &= bucket_mask;\n    }\n}\nimpl Tag {\n    pub(crate) const EMPTY: Tag = Tag(0b1111_1111);\n    pub(crate) const DELETED: Tag = Tag(0b1000_0000);\n    #[inline]\n    pub(crate) const fn is_full(self) -> bool {}\n    #[inline]\n    pub(crate) const fn is_special(self) -> bool {}\n    #[inline]\n    pub(crate) const fn special_is_empty(self) -> bool {}\n    #[inline]\n    #[allow(clippy::cast_possible_truncation)]\n    pub(crate) const fn full(hash: u64) -> Tag {\n        const MIN_HASH_LEN: usize = if mem::size_of::<usize>() < mem::size_of::<u64>() {\n            mem::size_of::<usize>()\n        } else {\n            mem::size_of::<u64>()\n        };\n        let top7 = hash >> (MIN_HASH_LEN * 8 - 7);\n        Tag((top7 & 0x7f) as u8)\n    }\n}\n\nThe function to be tested is presented with each line formatted as 'line number + code':\n/// Searches for an element in a table, returning the `index` of the found element.\n/// This uses dynamic dispatch to reduce the amount of code generated, but it is\n/// eliminated by LLVM optimizations.\n///\n/// This function does not make any changes to the `data` part of the table, or any\n/// changes to the `items` or `growth_left` field of the table.\n///\n/// The table must have at least 1 empty `bucket`, otherwise, if the\n/// `eq: &mut dyn FnMut(usize) -> bool` function does not return `true`,\n/// this function will also never return (will go into an infinite loop).\n///\n/// This function is guaranteed to provide the `eq: &mut dyn FnMut(usize) -> bool`\n/// function with only `FULL` buckets' indices and return the `index` of the found\n/// element as `Some(index)`, so the index will always be in the range\n/// `0..self.buckets()`.\n///\n/// # Safety\n///\n/// The [`RawTableInner`] must have properly initialized control bytes otherwise calling\n/// this function results in [`undefined behavior`].\n///\n/// [`undefined behavior`]: https://doc.rust-lang.org/reference/behavior-considered-undefined.html\n1864 unsafe fn find_inner(&self, hash: u64, eq: &mut dyn FnMut(usize) -> bool) -> Option<usize> {\n1865     let tag_hash = Tag::full(hash);\n1866     let mut probe_seq = self.probe_seq(hash);\n1867 \n1868     loop {\n1869         // SAFETY:\n1870         // * Caller of this function ensures that the control bytes are properly initialized.\n1871         //\n1872         // * `ProbeSeq.pos` cannot be greater than `self.bucket_mask = self.buckets() - 1`\n1873         //   of the table due to masking with `self.bucket_mask`.\n1874         //\n1875         // * Even if `ProbeSeq.pos` returns `position == self.bucket_mask`, it is safe to\n1876         //   call `Group::load` due to the extended control bytes range, which is\n1877         //  `self.bucket_mask + 1 + Group::WIDTH` (in fact, this means that the last control\n1878         //   byte will never be read for the allocated table);\n1879         //\n1880         // * Also, even if `RawTableInner` is not already allocated, `ProbeSeq.pos` will\n1881         //   always return \"0\" (zero), so Group::load will read unaligned `Group::static_empty()`\n1882         //   bytes, which is safe (see RawTableInner::new_in).\n1883         let group = unsafe { Group::load(self.ctrl(probe_seq.pos)) };\n1884 \n1885         for bit in group.match_tag(tag_hash) {\n1886             // This is the same as `(probe_seq.pos + bit) % self.buckets()` because the number\n1887             // of buckets is a power of two, and `self.bucket_mask = self.buckets() - 1`.\n1888             let index = (probe_seq.pos + bit) & self.bucket_mask;\n1889 \n1890             if likely(eq(index)) {\n1891                 return Some(index);\n1892             }\n1893         }\n1894 \n1895         if likely(group.match_empty().any_bit_set()) {\n1896             return None;\n1897         }\n1898 \n1899         probe_seq.move_next(self.bucket_mask);\n1900     }\n1901 }\n\nWhen inferring test input conditions or ranges, consider the following preconditions and expected return values or types:\n"
}