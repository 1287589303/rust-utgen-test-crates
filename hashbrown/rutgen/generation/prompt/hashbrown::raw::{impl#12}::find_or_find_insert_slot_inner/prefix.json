{
  "system_pt": "As a software testing expert, please generate Rust test functions based on the following guidelines, focusing solely on constructing test inputs and calling the functions under test:\n1. Provide the code in plain text format, without explanations or Markdown.\n2. If the method under test belongs to a trait, construct appropriate structs within the test function, but avoid method overrides. If the method under test uses generics, instantiate them with suitable types based on the context.\n3. Generate test code with minimal scope: avoid creating external structures or implementations. Instead, define any necessary helper structures or implementations directly within the test function when required.\n4. Whenever possible, initialize the corresponding data structures using the initialization methods provided in the context.\n5. Ensure the generated function is fully implemented and can be compiled and executed directly without any missing parts.\n6. Create a minimal yet complete set of test functions, ensuring they adhere to all provided preconditions and cover boundary conditions, and reference the inferred test input conditions or ranges.\n7. Ensure the test inputs are designed to cover the inferred test input conditions or ranges as comprehensively as possible, with particular emphasis on boundary cases.\n8. Focus on crafting test inputs that effectively reveal potential bugs while meeting the specified requirements.\n9. Omit test oracles and assertions; concentrate on generating test inputs and function calls.\n10. Do not create a test module, but include intrinsic attributes like #[test] or #[should_panic] where necessary.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/raw/mod.rs\n// crate name is hashbrown\nuse crate::alloc::alloc::{handle_alloc_error, Layout};\nuse crate::control::{BitMaskIter, Group, Tag, TagSliceExt};\nuse crate::scopeguard::{guard, ScopeGuard};\nuse crate::util::{invalid_mut, likely, unlikely};\nuse crate::TryReserveError;\nuse core::array;\nuse core::iter::FusedIterator;\nuse core::marker::PhantomData;\nuse core::mem;\nuse core::ptr::NonNull;\nuse core::slice;\nuse core::{hint, ptr};\n#[cfg(test)]\npub(crate) use self::alloc::AllocError;\npub(crate) use self::alloc::{do_alloc, Allocator, Global};\nstruct RawTableInner {\n    bucket_mask: usize,\n    ctrl: NonNull<u8>,\n    growth_left: usize,\n    items: usize,\n}\n#[derive(Clone)]\npub(crate) struct BitMaskIter(pub(crate) BitMask);\n#[derive(Copy, Clone)]\npub(crate) struct BitMask(pub(crate) BitMaskWord);\n#[derive(Clone)]\nstruct ProbeSeq {\n    pos: usize,\n    stride: usize,\n}\n#[derive(Copy, Clone, PartialEq, Eq)]\n#[repr(transparent)]\npub(crate) struct Tag(pub(super) u8);\npub struct InsertSlot {\n    index: usize,\n}\nimpl RawTableInner {\n    #[cfg_attr(feature = \"inline-more\", inline)]\n    unsafe fn new_uninitialized<A>(\n        alloc: &A,\n        table_layout: TableLayout,\n        buckets: usize,\n        fallibility: Fallibility,\n    ) -> Result<Self, TryReserveError>\n    where\n        A: Allocator,\n    {\n        debug_assert!(buckets.is_power_of_two());\n        let (layout, ctrl_offset) = match table_layout.calculate_layout_for(buckets) {\n            Some(lco) => lco,\n            None => return Err(fallibility.capacity_overflow()),\n        };\n        let ptr: NonNull<u8> = match do_alloc(alloc, layout) {\n            Ok(block) => block.cast(),\n            Err(_) => return Err(fallibility.alloc_err(layout)),\n        };\n        let ctrl = NonNull::new_unchecked(ptr.as_ptr().add(ctrl_offset));\n        Ok(Self {\n            ctrl,\n            bucket_mask: buckets - 1,\n            items: 0,\n            growth_left: bucket_mask_to_capacity(buckets - 1),\n        })\n    }\n    #[inline]\n    fn fallible_with_capacity<A>(\n        alloc: &A,\n        table_layout: TableLayout,\n        capacity: usize,\n        fallibility: Fallibility,\n    ) -> Result<Self, TryReserveError>\n    where\n        A: Allocator,\n    {\n        if capacity == 0 {\n            Ok(Self::NEW)\n        } else {\n            unsafe {\n                let buckets = capacity_to_buckets(capacity)\n                    .ok_or_else(|| fallibility.capacity_overflow())?;\n                let mut result = Self::new_uninitialized(\n                    alloc,\n                    table_layout,\n                    buckets,\n                    fallibility,\n                )?;\n                result.ctrl_slice().fill_empty();\n                Ok(result)\n            }\n        }\n    }\n    fn with_capacity<A>(alloc: &A, table_layout: TableLayout, capacity: usize) -> Self\n    where\n        A: Allocator,\n    {\n        match Self::fallible_with_capacity(\n            alloc,\n            table_layout,\n            capacity,\n            Fallibility::Infallible,\n        ) {\n            Ok(table_inner) => table_inner,\n            Err(_) => unsafe { hint::unreachable_unchecked() }\n        }\n    }\n    #[inline]\n    unsafe fn fix_insert_slot(&self, mut index: usize) -> InsertSlot {\n        if unlikely(self.is_bucket_full(index)) {\n            debug_assert!(self.bucket_mask < Group::WIDTH);\n            index = Group::load_aligned(self.ctrl(0))\n                .match_empty_or_deleted()\n                .lowest_set_bit()\n                .unwrap_unchecked();\n        }\n        InsertSlot { index }\n    }\n    #[inline]\n    fn find_insert_slot_in_group(\n        &self,\n        group: &Group,\n        probe_seq: &ProbeSeq,\n    ) -> Option<usize> {\n        let bit = group.match_empty_or_deleted().lowest_set_bit();\n        if likely(bit.is_some()) {\n            Some((probe_seq.pos + bit.unwrap()) & self.bucket_mask)\n        } else {\n            None\n        }\n    }\n    #[inline]\n    unsafe fn find_or_find_insert_slot_inner(\n        &self,\n        hash: u64,\n        eq: &mut dyn FnMut(usize) -> bool,\n    ) -> Result<usize, InsertSlot> {\n        let mut insert_slot = None;\n        let tag_hash = Tag::full(hash);\n        let mut probe_seq = self.probe_seq(hash);\n        loop {\n            let group = unsafe { Group::load(self.ctrl(probe_seq.pos)) };\n            for bit in group.match_tag(tag_hash) {\n                let index = (probe_seq.pos + bit) & self.bucket_mask;\n                if likely(eq(index)) {\n                    return Ok(index);\n                }\n            }\n            if likely(insert_slot.is_none()) {\n                insert_slot = self.find_insert_slot_in_group(&group, &probe_seq);\n            }\n            if likely(group.match_empty().any_bit_set()) {\n                unsafe {\n                    return Err(self.fix_insert_slot(insert_slot.unwrap_unchecked()));\n                }\n            }\n            probe_seq.move_next(self.bucket_mask);\n        }\n    }\n    #[inline]\n    unsafe fn prepare_insert_slot(&mut self, hash: u64) -> (usize, Tag) {}\n    #[inline]\n    unsafe fn find_insert_slot(&self, hash: u64) -> InsertSlot {}\n    #[inline(always)]\n    unsafe fn find_inner(\n        &self,\n        hash: u64,\n        eq: &mut dyn FnMut(usize) -> bool,\n    ) -> Option<usize> {}\n    #[allow(clippy::mut_mut)]\n    #[inline]\n    unsafe fn prepare_rehash_in_place(&mut self) {}\n    #[inline]\n    unsafe fn iter<T>(&self) -> RawIter<T> {}\n    unsafe fn drop_elements<T>(&mut self) {}\n    unsafe fn drop_inner_table<T, A: Allocator>(\n        &mut self,\n        alloc: &A,\n        table_layout: TableLayout,\n    ) {}\n    #[inline]\n    unsafe fn bucket<T>(&self, index: usize) -> Bucket<T> {}\n    #[inline]\n    unsafe fn bucket_ptr(&self, index: usize, size_of: usize) -> *mut u8 {}\n    #[inline]\n    fn data_end<T>(&self) -> NonNull<T> {}\n    #[inline]\n    fn probe_seq(&self, hash: u64) -> ProbeSeq {\n        ProbeSeq {\n            pos: h1(hash) & self.bucket_mask,\n            stride: 0,\n        }\n    }\n    #[inline]\n    unsafe fn record_item_insert_at(&mut self, index: usize, old_ctrl: Tag, hash: u64) {}\n    #[inline]\n    fn is_in_same_group(&self, i: usize, new_i: usize, hash: u64) -> bool {}\n    #[inline]\n    unsafe fn set_ctrl_hash(&mut self, index: usize, hash: u64) {}\n    #[inline]\n    unsafe fn replace_ctrl_hash(&mut self, index: usize, hash: u64) -> Tag {}\n    #[inline]\n    unsafe fn set_ctrl(&mut self, index: usize, ctrl: Tag) {}\n    #[inline]\n    unsafe fn ctrl(&self, index: usize) -> *mut Tag {\n        debug_assert!(index < self.num_ctrl_bytes());\n        self.ctrl.as_ptr().add(index).cast()\n    }\n    fn ctrl_slice(&mut self) -> &mut [Tag] {}\n    #[inline]\n    fn buckets(&self) -> usize {}\n    #[inline]\n    unsafe fn is_bucket_full(&self, index: usize) -> bool {}\n    #[inline]\n    fn num_ctrl_bytes(&self) -> usize {}\n    #[inline]\n    fn is_empty_singleton(&self) -> bool {}\n    #[allow(clippy::mut_mut)]\n    #[inline]\n    fn prepare_resize<'a, A>(\n        &self,\n        alloc: &'a A,\n        table_layout: TableLayout,\n        capacity: usize,\n        fallibility: Fallibility,\n    ) -> Result<\n        crate::scopeguard::ScopeGuard<Self, impl FnMut(&mut Self) + 'a>,\n        TryReserveError,\n    >\n    where\n        A: Allocator,\n    {\n        debug_assert!(self.items <= capacity);\n        let new_table = RawTableInner::fallible_with_capacity(\n            alloc,\n            table_layout,\n            capacity,\n            fallibility,\n        )?;\n        Ok(\n            guard(\n                new_table,\n                move |self_| {\n                    if !self_.is_empty_singleton() {\n                        unsafe { self_.free_buckets(alloc, table_layout) };\n                    }\n                },\n            ),\n        )\n    }\n    #[allow(clippy::inline_always)]\n    #[inline(always)]\n    unsafe fn reserve_rehash_inner<A>(\n        &mut self,\n        alloc: &A,\n        additional: usize,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        fallibility: Fallibility,\n        layout: TableLayout,\n        drop: Option<unsafe fn(*mut u8)>,\n    ) -> Result<(), TryReserveError>\n    where\n        A: Allocator,\n    {}\n    #[inline(always)]\n    unsafe fn full_buckets_indices(&self) -> FullBucketsIndices {}\n    #[allow(clippy::inline_always)]\n    #[inline(always)]\n    unsafe fn resize_inner<A>(\n        &mut self,\n        alloc: &A,\n        capacity: usize,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        fallibility: Fallibility,\n        layout: TableLayout,\n    ) -> Result<(), TryReserveError>\n    where\n        A: Allocator,\n    {}\n    #[allow(clippy::inline_always)]\n    #[cfg_attr(feature = \"inline-more\", inline(always))]\n    #[cfg_attr(not(feature = \"inline-more\"), inline)]\n    unsafe fn rehash_in_place(\n        &mut self,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        size_of: usize,\n        drop: Option<unsafe fn(*mut u8)>,\n    ) {}\n    #[inline]\n    unsafe fn free_buckets<A>(&mut self, alloc: &A, table_layout: TableLayout)\n    where\n        A: Allocator,\n    {}\n    #[inline]\n    unsafe fn allocation_info(\n        &self,\n        table_layout: TableLayout,\n    ) -> (NonNull<u8>, Layout) {}\n    #[inline]\n    unsafe fn allocation_size_or_zero(&self, table_layout: TableLayout) -> usize {}\n    #[inline]\n    fn clear_no_drop(&mut self) {}\n    #[inline]\n    unsafe fn erase(&mut self, index: usize) {}\n}\nimpl Iterator for BitMaskIter {\n    type Item = usize;\n    #[inline]\n    fn next(&mut self) -> Option<usize> {\n        let bit = self.0.lowest_set_bit()?;\n        self.0 = self.0.remove_lowest_bit();\n        Some(bit)\n    }\n}\nimpl IntoIterator for BitMask {\n    type Item = usize;\n    type IntoIter = BitMaskIter;\n    #[inline]\n    fn into_iter(self) -> BitMaskIter {\n        BitMaskIter(BitMask(self.0 & BITMASK_ITER_MASK))\n    }\n}\n#[allow(clippy::use_self)]\nimpl BitMask {\n    #[inline]\n    #[must_use]\n    #[allow(dead_code)]\n    pub(crate) fn invert(self) -> Self {\n        BitMask(self.0 ^ BITMASK_MASK)\n    }\n    #[inline]\n    #[must_use]\n    fn remove_lowest_bit(self) -> Self {\n        BitMask(self.0 & (self.0 - 1))\n    }\n    #[inline]\n    pub(crate) fn any_bit_set(self) -> bool {\n        self.0 != 0\n    }\n    #[inline]\n    pub(crate) fn lowest_set_bit(self) -> Option<usize> {}\n    #[inline]\n    pub(crate) fn trailing_zeros(self) -> usize {}\n    #[inline]\n    fn nonzero_trailing_zeros(nonzero: NonZeroBitMaskWord) -> usize {}\n    #[inline]\n    pub(crate) fn leading_zeros(self) -> usize {}\n}\nimpl ProbeSeq {\n    #[inline]\n    fn move_next(&mut self, bucket_mask: usize) {\n        debug_assert!(self.stride <= bucket_mask, \"Went past end of probe sequence\");\n        self.stride += Group::WIDTH;\n        self.pos += self.stride;\n        self.pos &= bucket_mask;\n    }\n}\nimpl Tag {\n    pub(crate) const EMPTY: Tag = Tag(0b1111_1111);\n    pub(crate) const DELETED: Tag = Tag(0b1000_0000);\n    #[inline]\n    pub(crate) const fn is_full(self) -> bool {}\n    #[inline]\n    pub(crate) const fn is_special(self) -> bool {}\n    #[inline]\n    pub(crate) const fn special_is_empty(self) -> bool {}\n    #[inline]\n    #[allow(clippy::cast_possible_truncation)]\n    pub(crate) const fn full(hash: u64) -> Tag {\n        const MIN_HASH_LEN: usize = if mem::size_of::<usize>() < mem::size_of::<u64>() {\n            mem::size_of::<usize>()\n        } else {\n            mem::size_of::<u64>()\n        };\n        let top7 = hash >> (MIN_HASH_LEN * 8 - 7);\n        Tag((top7 & 0x7f) as u8)\n    }\n}\n\nThe function to be tested is presented with each line formatted as 'line number + code':\n/// Searches for an element in the table, or a potential slot where that element could\n/// be inserted (an empty or deleted [`Bucket`] index).\n///\n/// This uses dynamic dispatch to reduce the amount of code generated, but that is\n/// eliminated by LLVM optimizations.\n///\n/// This function does not make any changes to the `data` part of the table, or any\n/// changes to the `items` or `growth_left` field of the table.\n///\n/// The table must have at least 1 empty or deleted `bucket`, otherwise, if the\n/// `eq: &mut dyn FnMut(usize) -> bool` function does not return `true`, this function\n/// will never return (will go into an infinite loop) for tables larger than the group\n/// width, or return an index outside of the table indices range if the table is less\n/// than the group width.\n///\n/// This function is guaranteed to provide the `eq: &mut dyn FnMut(usize) -> bool`\n/// function with only `FULL` buckets' indices and return the `index` of the found\n/// element (as `Ok(index)`). If the element is not found and there is at least 1\n/// empty or deleted [`Bucket`] in the table, the function is guaranteed to return\n/// [`InsertSlot`] with an index in the range `0..self.buckets()`, but in any case,\n/// if this function returns [`InsertSlot`], it will contain an index in the range\n/// `0..=self.buckets()`.\n///\n/// # Safety\n///\n/// The [`RawTableInner`] must have properly initialized control bytes otherwise calling\n/// this function results in [`undefined behavior`].\n///\n/// Attempt to write data at the [`InsertSlot`] returned by this function when the table is\n/// less than the group width and if there was not at least one empty or deleted bucket in\n/// the table will cause immediate [`undefined behavior`]. This is because in this case the\n/// function will return `self.bucket_mask + 1` as an index due to the trailing [`Tag::EMPTY`]\n/// control bytes outside the table range.\n///\n/// [`undefined behavior`]: https://doc.rust-lang.org/reference/behavior-considered-undefined.html\n1650 unsafe fn find_or_find_insert_slot_inner(\n1651     &self,\n1652     hash: u64,\n1653     eq: &mut dyn FnMut(usize) -> bool,\n1654 ) -> Result<usize, InsertSlot> {\n1655     let mut insert_slot = None;\n1656 \n1657     let tag_hash = Tag::full(hash);\n1658     let mut probe_seq = self.probe_seq(hash);\n1659 \n1660     loop {\n1661         // SAFETY:\n1662         // * Caller of this function ensures that the control bytes are properly initialized.\n1663         //\n1664         // * `ProbeSeq.pos` cannot be greater than `self.bucket_mask = self.buckets() - 1`\n1665         //   of the table due to masking with `self.bucket_mask` and also because the number\n1666         //   of buckets is a power of two (see `self.probe_seq` function).\n1667         //\n1668         // * Even if `ProbeSeq.pos` returns `position == self.bucket_mask`, it is safe to\n1669         //   call `Group::load` due to the extended control bytes range, which is\n1670         //  `self.bucket_mask + 1 + Group::WIDTH` (in fact, this means that the last control\n1671         //   byte will never be read for the allocated table);\n1672         //\n1673         // * Also, even if `RawTableInner` is not already allocated, `ProbeSeq.pos` will\n1674         //   always return \"0\" (zero), so Group::load will read unaligned `Group::static_empty()`\n1675         //   bytes, which is safe (see RawTableInner::new).\n1676         let group = unsafe { Group::load(self.ctrl(probe_seq.pos)) };\n1677 \n1678         for bit in group.match_tag(tag_hash) {\n1679             let index = (probe_seq.pos + bit) & self.bucket_mask;\n1680 \n1681             if likely(eq(index)) {\n1682                 return Ok(index);\n1683             }\n1684         }\n1685 \n1686         // We didn't find the element we were looking for in the group, try to get an\n1687         // insertion slot from the group if we don't have one yet.\n1688         if likely(insert_slot.is_none()) {\n1689             insert_slot = self.find_insert_slot_in_group(&group, &probe_seq);\n1690         }\n1691 \n1692         // Only stop the search if the group contains at least one empty element.\n1693         // Otherwise, the element that we are looking for might be in a following group.\n1694         if likely(group.match_empty().any_bit_set()) {\n1695             // We must have found a insert slot by now, since the current group contains at\n1696             // least one. For tables smaller than the group width, there will still be an\n1697             // empty element in the current (and only) group due to the load factor.\n1698             unsafe {\n1699                 // SAFETY:\n1700                 // * Caller of this function ensures that the control bytes are properly initialized.\n1701                 //\n1702                 // * We use this function with the slot / index found by `self.find_insert_slot_in_group`\n1703                 return Err(self.fix_insert_slot(insert_slot.unwrap_unchecked()));\n1704             }\n1705         }\n1706 \n1707         probe_seq.move_next(self.bucket_mask);\n1708     }\n1709 }\n\nGenerate each test function in such a manner that it concurrently satisfies all the following preconditions:\n",
  "depend_pt": ""
}