{
  "system_pt": "As a software testing expert, please generate accurate test oracles code based on the provided information. Follow these guidelines:\n1. Generate executable test oracle codes in plain text format, one per line. Do not output complete test functions, avoid additional explanations, and do not use Markdown.\n2. Combine the given function under test, context, relevant documentation, preconditions, expected return values or types, test input conditions or ranges, and existing test function prefixes to infer and generate corresponding test oracles code.\n3. Only generate necessary test oracles to ensure comprehensive validation.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/raw/mod.rs\n// crate name is hashbrown\nuse crate::alloc::alloc::{handle_alloc_error, Layout};\nuse crate::control::{BitMaskIter, Group, Tag, TagSliceExt};\nuse crate::scopeguard::{guard, ScopeGuard};\nuse crate::util::{invalid_mut, likely, unlikely};\nuse crate::TryReserveError;\nuse core::array;\nuse core::iter::FusedIterator;\nuse core::marker::PhantomData;\nuse core::mem;\nuse core::ptr::NonNull;\nuse core::slice;\nuse core::{hint, ptr};\n#[cfg(test)]\npub(crate) use self::alloc::AllocError;\npub(crate) use self::alloc::{do_alloc, Allocator, Global};\nstruct RawTableInner {\n    bucket_mask: usize,\n    ctrl: NonNull<u8>,\n    growth_left: usize,\n    items: usize,\n}\npub struct Bucket<T> {\n    ptr: NonNull<T>,\n}\nimpl RawTableInner {\n    #[cfg_attr(feature = \"inline-more\", inline)]\n    unsafe fn new_uninitialized<A>(\n        alloc: &A,\n        table_layout: TableLayout,\n        buckets: usize,\n        fallibility: Fallibility,\n    ) -> Result<Self, TryReserveError>\n    where\n        A: Allocator,\n    {\n        debug_assert!(buckets.is_power_of_two());\n        let (layout, ctrl_offset) = match table_layout.calculate_layout_for(buckets) {\n            Some(lco) => lco,\n            None => return Err(fallibility.capacity_overflow()),\n        };\n        let ptr: NonNull<u8> = match do_alloc(alloc, layout) {\n            Ok(block) => block.cast(),\n            Err(_) => return Err(fallibility.alloc_err(layout)),\n        };\n        let ctrl = NonNull::new_unchecked(ptr.as_ptr().add(ctrl_offset));\n        Ok(Self {\n            ctrl,\n            bucket_mask: buckets - 1,\n            items: 0,\n            growth_left: bucket_mask_to_capacity(buckets - 1),\n        })\n    }\n    #[inline]\n    fn fallible_with_capacity<A>(\n        alloc: &A,\n        table_layout: TableLayout,\n        capacity: usize,\n        fallibility: Fallibility,\n    ) -> Result<Self, TryReserveError>\n    where\n        A: Allocator,\n    {\n        if capacity == 0 {\n            Ok(Self::NEW)\n        } else {\n            unsafe {\n                let buckets = capacity_to_buckets(capacity)\n                    .ok_or_else(|| fallibility.capacity_overflow())?;\n                let mut result = Self::new_uninitialized(\n                    alloc,\n                    table_layout,\n                    buckets,\n                    fallibility,\n                )?;\n                result.ctrl_slice().fill_empty();\n                Ok(result)\n            }\n        }\n    }\n    fn with_capacity<A>(alloc: &A, table_layout: TableLayout, capacity: usize) -> Self\n    where\n        A: Allocator,\n    {\n        match Self::fallible_with_capacity(\n            alloc,\n            table_layout,\n            capacity,\n            Fallibility::Infallible,\n        ) {\n            Ok(table_inner) => table_inner,\n            Err(_) => unsafe { hint::unreachable_unchecked() }\n        }\n    }\n    #[inline]\n    unsafe fn fix_insert_slot(&self, mut index: usize) -> InsertSlot {}\n    #[inline]\n    fn find_insert_slot_in_group(\n        &self,\n        group: &Group,\n        probe_seq: &ProbeSeq,\n    ) -> Option<usize> {}\n    #[inline]\n    unsafe fn find_or_find_insert_slot_inner(\n        &self,\n        hash: u64,\n        eq: &mut dyn FnMut(usize) -> bool,\n    ) -> Result<usize, InsertSlot> {}\n    #[inline]\n    unsafe fn prepare_insert_slot(&mut self, hash: u64) -> (usize, Tag) {}\n    #[inline]\n    unsafe fn find_insert_slot(&self, hash: u64) -> InsertSlot {}\n    #[inline(always)]\n    unsafe fn find_inner(\n        &self,\n        hash: u64,\n        eq: &mut dyn FnMut(usize) -> bool,\n    ) -> Option<usize> {}\n    #[allow(clippy::mut_mut)]\n    #[inline]\n    unsafe fn prepare_rehash_in_place(&mut self) {}\n    #[inline]\n    unsafe fn iter<T>(&self) -> RawIter<T> {}\n    unsafe fn drop_elements<T>(&mut self) {}\n    unsafe fn drop_inner_table<T, A: Allocator>(\n        &mut self,\n        alloc: &A,\n        table_layout: TableLayout,\n    ) {}\n    #[inline]\n    unsafe fn bucket<T>(&self, index: usize) -> Bucket<T> {\n        debug_assert_ne!(self.bucket_mask, 0);\n        debug_assert!(index < self.buckets());\n        Bucket::from_base_index(self.data_end(), index)\n    }\n    #[inline]\n    unsafe fn bucket_ptr(&self, index: usize, size_of: usize) -> *mut u8 {}\n    #[inline]\n    fn data_end<T>(&self) -> NonNull<T> {\n        self.ctrl.cast()\n    }\n    #[inline]\n    fn probe_seq(&self, hash: u64) -> ProbeSeq {}\n    #[inline]\n    unsafe fn record_item_insert_at(&mut self, index: usize, old_ctrl: Tag, hash: u64) {}\n    #[inline]\n    fn is_in_same_group(&self, i: usize, new_i: usize, hash: u64) -> bool {}\n    #[inline]\n    unsafe fn set_ctrl_hash(&mut self, index: usize, hash: u64) {}\n    #[inline]\n    unsafe fn replace_ctrl_hash(&mut self, index: usize, hash: u64) -> Tag {}\n    #[inline]\n    unsafe fn set_ctrl(&mut self, index: usize, ctrl: Tag) {}\n    #[inline]\n    unsafe fn ctrl(&self, index: usize) -> *mut Tag {}\n    fn ctrl_slice(&mut self) -> &mut [Tag] {}\n    #[inline]\n    fn buckets(&self) -> usize {\n        self.bucket_mask + 1\n    }\n    #[inline]\n    unsafe fn is_bucket_full(&self, index: usize) -> bool {}\n    #[inline]\n    fn num_ctrl_bytes(&self) -> usize {}\n    #[inline]\n    fn is_empty_singleton(&self) -> bool {}\n    #[allow(clippy::mut_mut)]\n    #[inline]\n    fn prepare_resize<'a, A>(\n        &self,\n        alloc: &'a A,\n        table_layout: TableLayout,\n        capacity: usize,\n        fallibility: Fallibility,\n    ) -> Result<\n        crate::scopeguard::ScopeGuard<Self, impl FnMut(&mut Self) + 'a>,\n        TryReserveError,\n    >\n    where\n        A: Allocator,\n    {\n        debug_assert!(self.items <= capacity);\n        let new_table = RawTableInner::fallible_with_capacity(\n            alloc,\n            table_layout,\n            capacity,\n            fallibility,\n        )?;\n        Ok(\n            guard(\n                new_table,\n                move |self_| {\n                    if !self_.is_empty_singleton() {\n                        unsafe { self_.free_buckets(alloc, table_layout) };\n                    }\n                },\n            ),\n        )\n    }\n    #[allow(clippy::inline_always)]\n    #[inline(always)]\n    unsafe fn reserve_rehash_inner<A>(\n        &mut self,\n        alloc: &A,\n        additional: usize,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        fallibility: Fallibility,\n        layout: TableLayout,\n        drop: Option<unsafe fn(*mut u8)>,\n    ) -> Result<(), TryReserveError>\n    where\n        A: Allocator,\n    {}\n    #[inline(always)]\n    unsafe fn full_buckets_indices(&self) -> FullBucketsIndices {}\n    #[allow(clippy::inline_always)]\n    #[inline(always)]\n    unsafe fn resize_inner<A>(\n        &mut self,\n        alloc: &A,\n        capacity: usize,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        fallibility: Fallibility,\n        layout: TableLayout,\n    ) -> Result<(), TryReserveError>\n    where\n        A: Allocator,\n    {}\n    #[allow(clippy::inline_always)]\n    #[cfg_attr(feature = \"inline-more\", inline(always))]\n    #[cfg_attr(not(feature = \"inline-more\"), inline)]\n    unsafe fn rehash_in_place(\n        &mut self,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        size_of: usize,\n        drop: Option<unsafe fn(*mut u8)>,\n    ) {}\n    #[inline]\n    unsafe fn free_buckets<A>(&mut self, alloc: &A, table_layout: TableLayout)\n    where\n        A: Allocator,\n    {}\n    #[inline]\n    unsafe fn allocation_info(\n        &self,\n        table_layout: TableLayout,\n    ) -> (NonNull<u8>, Layout) {}\n    #[inline]\n    unsafe fn allocation_size_or_zero(&self, table_layout: TableLayout) -> usize {}\n    #[inline]\n    fn clear_no_drop(&mut self) {}\n    #[inline]\n    unsafe fn erase(&mut self, index: usize) {}\n}\nimpl<T> Bucket<T> {\n    #[inline]\n    unsafe fn from_base_index(base: NonNull<T>, index: usize) -> Self {\n        let ptr = if T::IS_ZERO_SIZED {\n            invalid_mut(index + 1)\n        } else {\n            base.as_ptr().sub(index)\n        };\n        Self {\n            ptr: NonNull::new_unchecked(ptr),\n        }\n    }\n    #[inline]\n    unsafe fn to_base_index(&self, base: NonNull<T>) -> usize {}\n    #[inline]\n    pub fn as_ptr(&self) -> *mut T {}\n    #[inline]\n    fn as_non_null(&self) -> NonNull<T> {}\n    #[inline]\n    unsafe fn next_n(&self, offset: usize) -> Self {\n        let ptr = if T::IS_ZERO_SIZED {\n            invalid_mut(self.ptr.as_ptr() as usize + offset)\n        } else {\n            self.ptr.as_ptr().sub(offset)\n        };\n        Self {\n            ptr: NonNull::new_unchecked(ptr),\n        }\n    }\n    #[cfg_attr(feature = \"inline-more\", inline)]\n    pub(crate) unsafe fn drop(&self) {}\n    #[inline]\n    pub(crate) unsafe fn read(&self) -> T {}\n    #[inline]\n    pub(crate) unsafe fn write(&self, val: T) {}\n    #[inline]\n    pub unsafe fn as_ref<'a>(&self) -> &'a T {}\n    #[inline]\n    pub unsafe fn as_mut<'a>(&self) -> &'a mut T {}\n}\n\nThe function to be tested is presented with each line formatted as 'line number + code':\n/// Returns a pointer to an element in the table (convenience for\n/// `Bucket::from_base_index(self.data_end::<T>(), index)`).\n///\n/// The caller must ensure that the `RawTableInner` outlives the returned [`Bucket<T>`],\n/// otherwise using it may result in [`undefined behavior`].\n///\n/// # Safety\n///\n/// If `mem::size_of::<T>() != 0`, then the safety rules are directly derived from the\n/// safety rules of the [`Bucket::from_base_index`] function. Therefore, when calling\n/// this function, the following safety rules must be observed:\n///\n/// * The table must already be allocated;\n///\n/// * The `index` must not be greater than the number returned by the [`RawTableInner::buckets`]\n///   function, i.e. `(index + 1) <= self.buckets()`.\n///\n/// * The type `T` must be the actual type of the elements stored in the table, otherwise\n///   using the returned [`Bucket`] may result in [`undefined behavior`].\n///\n/// It is safe to call this function with index of zero (`index == 0`) on a table that has\n/// not been allocated, but using the returned [`Bucket`] results in [`undefined behavior`].\n///\n/// If `mem::size_of::<T>() == 0`, then the only requirement is that the `index` must\n/// not be greater than the number returned by the [`RawTable::buckets`] function, i.e.\n/// `(index + 1) <= self.buckets()`.\n///\n/// ```none\n/// If mem::size_of::<T>() != 0 then return a pointer to the `element` in the `data part` of the table\n/// (we start counting from \"0\", so that in the expression T[n], the \"n\" index actually one less than\n/// the \"buckets\" number of our `RawTableInner`, i.e. \"n = RawTableInner::buckets() - 1\"):\n///\n///           `table.bucket(3).as_ptr()` returns a pointer that points here in the `data`\n///           part of the `RawTableInner`, i.e. to the start of T3 (see [`Bucket::as_ptr`])\n///                  |\n///                  |               `base = table.data_end::<T>()` points here\n///                  |               (to the start of CT0 or to the end of T0)\n///                  v                 v\n/// [Pad], T_n, ..., |T3|, T2, T1, T0, |CT0, CT1, CT2, CT3, ..., CT_n, CTa_0, CTa_1, ..., CTa_m\n///                     ^                                              \\__________  __________/\n///        `table.bucket(3)` returns a pointer that points                        \\/\n///         here in the `data` part of the `RawTableInner`             additional control bytes\n///         (to the end of T3)                                          `m = Group::WIDTH - 1`\n///\n/// where: T0...T_n  - our stored data;\n///        CT0...CT_n - control bytes or metadata for `data`;\n///        CTa_0...CTa_m - additional control bytes (so that the search with loading `Group` bytes from\n///                        the heap works properly, even if the result of `h1(hash) & self.bucket_mask`\n///                        is equal to `self.bucket_mask`). See also `RawTableInner::set_ctrl` function.\n///\n/// P.S. `h1(hash) & self.bucket_mask` is the same as `hash as usize % self.buckets()` because the number\n/// of buckets is a power of two, and `self.bucket_mask = self.buckets() - 1`.\n/// ```\n///\n/// [`Bucket::from_base_index`]: Bucket::from_base_index\n/// [`RawTableInner::buckets`]: RawTableInner::buckets\n/// [`undefined behavior`]: https://doc.rust-lang.org/reference/behavior-considered-undefined.html\n2197 unsafe fn bucket<T>(&self, index: usize) -> Bucket<T> {\n2198     debug_assert_ne!(self.bucket_mask, 0);\n2199     debug_assert!(index < self.buckets());\n2200     Bucket::from_base_index(self.data_end(), index)\n2201 }\n\nThe path conditions that the generated test functions should satisfy are as follows:\n"
}