{
  "system_pt": "As a software testing expert, please generate Rust test functions based on the following guidelines, focusing solely on constructing test inputs and calling the functions under test:\n1. Provide the code in plain text format, without explanations or Markdown.\n2. If the method under test belongs to a trait, construct appropriate structs within the test function, but avoid method overrides. If the method under test uses generics, instantiate them with suitable types based on the context.\n3. Generate test code with minimal scope: avoid creating external structures or implementations. Instead, define any necessary helper structures or implementations directly within the test function when required.\n4. Whenever possible, initialize the corresponding data structures using the initialization methods provided in the context.\n5. Ensure the generated function is fully implemented and can be compiled and executed directly without any missing parts.\n6. Create a minimal yet complete set of test functions, ensuring they adhere to all provided preconditions and cover boundary conditions, and reference the inferred test input conditions or ranges.\n7. Ensure the test inputs are designed to cover the inferred test input conditions or ranges as comprehensively as possible, with particular emphasis on boundary cases.\n8. Focus on crafting test inputs that effectively reveal potential bugs while meeting the specified requirements.\n9. Omit test oracles and assertions; concentrate on generating test inputs and function calls.\n10. Do not create a test module, but include intrinsic attributes like #[test] or #[should_panic] where necessary.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/raw/mod.rs\n// crate name is hashbrown\nuse crate::alloc::alloc::{handle_alloc_error, Layout};\nuse crate::control::{BitMaskIter, Group, Tag, TagSliceExt};\nuse crate::scopeguard::{guard, ScopeGuard};\nuse crate::util::{invalid_mut, likely, unlikely};\nuse crate::TryReserveError;\nuse core::array;\nuse core::iter::FusedIterator;\nuse core::marker::PhantomData;\nuse core::mem;\nuse core::ptr::NonNull;\nuse core::slice;\nuse core::{hint, ptr};\n#[cfg(test)]\npub(crate) use self::alloc::AllocError;\npub(crate) use self::alloc::{do_alloc, Allocator, Global};\nstruct RawTableInner {\n    bucket_mask: usize,\n    ctrl: NonNull<u8>,\n    growth_left: usize,\n    items: usize,\n}\n#[derive(Copy, Clone)]\npub(crate) struct BitMask(pub(crate) BitMaskWord);\n#[derive(Copy, Clone, PartialEq, Eq)]\n#[repr(transparent)]\npub(crate) struct Tag(pub(super) u8);\nimpl RawTableInner {\n    #[cfg_attr(feature = \"inline-more\", inline)]\n    unsafe fn new_uninitialized<A>(\n        alloc: &A,\n        table_layout: TableLayout,\n        buckets: usize,\n        fallibility: Fallibility,\n    ) -> Result<Self, TryReserveError>\n    where\n        A: Allocator,\n    {\n        debug_assert!(buckets.is_power_of_two());\n        let (layout, ctrl_offset) = match table_layout.calculate_layout_for(buckets) {\n            Some(lco) => lco,\n            None => return Err(fallibility.capacity_overflow()),\n        };\n        let ptr: NonNull<u8> = match do_alloc(alloc, layout) {\n            Ok(block) => block.cast(),\n            Err(_) => return Err(fallibility.alloc_err(layout)),\n        };\n        let ctrl = NonNull::new_unchecked(ptr.as_ptr().add(ctrl_offset));\n        Ok(Self {\n            ctrl,\n            bucket_mask: buckets - 1,\n            items: 0,\n            growth_left: bucket_mask_to_capacity(buckets - 1),\n        })\n    }\n    #[inline]\n    fn fallible_with_capacity<A>(\n        alloc: &A,\n        table_layout: TableLayout,\n        capacity: usize,\n        fallibility: Fallibility,\n    ) -> Result<Self, TryReserveError>\n    where\n        A: Allocator,\n    {\n        if capacity == 0 {\n            Ok(Self::NEW)\n        } else {\n            unsafe {\n                let buckets = capacity_to_buckets(capacity)\n                    .ok_or_else(|| fallibility.capacity_overflow())?;\n                let mut result = Self::new_uninitialized(\n                    alloc,\n                    table_layout,\n                    buckets,\n                    fallibility,\n                )?;\n                result.ctrl_slice().fill_empty();\n                Ok(result)\n            }\n        }\n    }\n    fn with_capacity<A>(alloc: &A, table_layout: TableLayout, capacity: usize) -> Self\n    where\n        A: Allocator,\n    {\n        match Self::fallible_with_capacity(\n            alloc,\n            table_layout,\n            capacity,\n            Fallibility::Infallible,\n        ) {\n            Ok(table_inner) => table_inner,\n            Err(_) => unsafe { hint::unreachable_unchecked() }\n        }\n    }\n    #[inline]\n    unsafe fn fix_insert_slot(&self, mut index: usize) -> InsertSlot {}\n    #[inline]\n    fn find_insert_slot_in_group(\n        &self,\n        group: &Group,\n        probe_seq: &ProbeSeq,\n    ) -> Option<usize> {}\n    #[inline]\n    unsafe fn find_or_find_insert_slot_inner(\n        &self,\n        hash: u64,\n        eq: &mut dyn FnMut(usize) -> bool,\n    ) -> Result<usize, InsertSlot> {}\n    #[inline]\n    unsafe fn prepare_insert_slot(&mut self, hash: u64) -> (usize, Tag) {}\n    #[inline]\n    unsafe fn find_insert_slot(&self, hash: u64) -> InsertSlot {}\n    #[inline(always)]\n    unsafe fn find_inner(\n        &self,\n        hash: u64,\n        eq: &mut dyn FnMut(usize) -> bool,\n    ) -> Option<usize> {}\n    #[allow(clippy::mut_mut)]\n    #[inline]\n    unsafe fn prepare_rehash_in_place(&mut self) {}\n    #[inline]\n    unsafe fn iter<T>(&self) -> RawIter<T> {}\n    unsafe fn drop_elements<T>(&mut self) {}\n    unsafe fn drop_inner_table<T, A: Allocator>(\n        &mut self,\n        alloc: &A,\n        table_layout: TableLayout,\n    ) {}\n    #[inline]\n    unsafe fn bucket<T>(&self, index: usize) -> Bucket<T> {}\n    #[inline]\n    unsafe fn bucket_ptr(&self, index: usize, size_of: usize) -> *mut u8 {}\n    #[inline]\n    fn data_end<T>(&self) -> NonNull<T> {}\n    #[inline]\n    fn probe_seq(&self, hash: u64) -> ProbeSeq {}\n    #[inline]\n    unsafe fn record_item_insert_at(&mut self, index: usize, old_ctrl: Tag, hash: u64) {}\n    #[inline]\n    fn is_in_same_group(&self, i: usize, new_i: usize, hash: u64) -> bool {}\n    #[inline]\n    unsafe fn set_ctrl_hash(&mut self, index: usize, hash: u64) {}\n    #[inline]\n    unsafe fn replace_ctrl_hash(&mut self, index: usize, hash: u64) -> Tag {}\n    #[inline]\n    unsafe fn set_ctrl(&mut self, index: usize, ctrl: Tag) {\n        let index2 = ((index.wrapping_sub(Group::WIDTH)) & self.bucket_mask)\n            + Group::WIDTH;\n        *self.ctrl(index) = ctrl;\n        *self.ctrl(index2) = ctrl;\n    }\n    #[inline]\n    unsafe fn ctrl(&self, index: usize) -> *mut Tag {\n        debug_assert!(index < self.num_ctrl_bytes());\n        self.ctrl.as_ptr().add(index).cast()\n    }\n    fn ctrl_slice(&mut self) -> &mut [Tag] {}\n    #[inline]\n    fn buckets(&self) -> usize {}\n    #[inline]\n    unsafe fn is_bucket_full(&self, index: usize) -> bool {\n        debug_assert!(index < self.buckets());\n        (*self.ctrl(index)).is_full()\n    }\n    #[inline]\n    fn num_ctrl_bytes(&self) -> usize {}\n    #[inline]\n    fn is_empty_singleton(&self) -> bool {}\n    #[allow(clippy::mut_mut)]\n    #[inline]\n    fn prepare_resize<'a, A>(\n        &self,\n        alloc: &'a A,\n        table_layout: TableLayout,\n        capacity: usize,\n        fallibility: Fallibility,\n    ) -> Result<\n        crate::scopeguard::ScopeGuard<Self, impl FnMut(&mut Self) + 'a>,\n        TryReserveError,\n    >\n    where\n        A: Allocator,\n    {\n        debug_assert!(self.items <= capacity);\n        let new_table = RawTableInner::fallible_with_capacity(\n            alloc,\n            table_layout,\n            capacity,\n            fallibility,\n        )?;\n        Ok(\n            guard(\n                new_table,\n                move |self_| {\n                    if !self_.is_empty_singleton() {\n                        unsafe { self_.free_buckets(alloc, table_layout) };\n                    }\n                },\n            ),\n        )\n    }\n    #[allow(clippy::inline_always)]\n    #[inline(always)]\n    unsafe fn reserve_rehash_inner<A>(\n        &mut self,\n        alloc: &A,\n        additional: usize,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        fallibility: Fallibility,\n        layout: TableLayout,\n        drop: Option<unsafe fn(*mut u8)>,\n    ) -> Result<(), TryReserveError>\n    where\n        A: Allocator,\n    {}\n    #[inline(always)]\n    unsafe fn full_buckets_indices(&self) -> FullBucketsIndices {}\n    #[allow(clippy::inline_always)]\n    #[inline(always)]\n    unsafe fn resize_inner<A>(\n        &mut self,\n        alloc: &A,\n        capacity: usize,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        fallibility: Fallibility,\n        layout: TableLayout,\n    ) -> Result<(), TryReserveError>\n    where\n        A: Allocator,\n    {}\n    #[allow(clippy::inline_always)]\n    #[cfg_attr(feature = \"inline-more\", inline(always))]\n    #[cfg_attr(not(feature = \"inline-more\"), inline)]\n    unsafe fn rehash_in_place(\n        &mut self,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        size_of: usize,\n        drop: Option<unsafe fn(*mut u8)>,\n    ) {}\n    #[inline]\n    unsafe fn free_buckets<A>(&mut self, alloc: &A, table_layout: TableLayout)\n    where\n        A: Allocator,\n    {}\n    #[inline]\n    unsafe fn allocation_info(\n        &self,\n        table_layout: TableLayout,\n    ) -> (NonNull<u8>, Layout) {}\n    #[inline]\n    unsafe fn allocation_size_or_zero(&self, table_layout: TableLayout) -> usize {}\n    #[inline]\n    fn clear_no_drop(&mut self) {}\n    #[inline]\n    unsafe fn erase(&mut self, index: usize) {\n        debug_assert!(self.is_bucket_full(index));\n        let index_before = index.wrapping_sub(Group::WIDTH) & self.bucket_mask;\n        let empty_before = Group::load(self.ctrl(index_before)).match_empty();\n        let empty_after = Group::load(self.ctrl(index)).match_empty();\n        let ctrl = if empty_before.leading_zeros() + empty_after.trailing_zeros()\n            >= Group::WIDTH\n        {\n            Tag::DELETED\n        } else {\n            self.growth_left += 1;\n            Tag::EMPTY\n        };\n        self.set_ctrl(index, ctrl);\n        self.items -= 1;\n    }\n}\n#[allow(clippy::use_self)]\nimpl BitMask {\n    #[inline]\n    #[must_use]\n    #[allow(dead_code)]\n    pub(crate) fn invert(self) -> Self {\n        BitMask(self.0 ^ BITMASK_MASK)\n    }\n    #[inline]\n    #[must_use]\n    fn remove_lowest_bit(self) -> Self {\n        BitMask(self.0 & (self.0 - 1))\n    }\n    #[inline]\n    pub(crate) fn any_bit_set(self) -> bool {}\n    #[inline]\n    pub(crate) fn lowest_set_bit(self) -> Option<usize> {}\n    #[inline]\n    pub(crate) fn trailing_zeros(self) -> usize {\n        if cfg!(target_arch = \"arm\") && BITMASK_STRIDE % 8 == 0 {\n            self.0.swap_bytes().leading_zeros() as usize / BITMASK_STRIDE\n        } else {\n            self.0.trailing_zeros() as usize / BITMASK_STRIDE\n        }\n    }\n    #[inline]\n    fn nonzero_trailing_zeros(nonzero: NonZeroBitMaskWord) -> usize {}\n    #[inline]\n    pub(crate) fn leading_zeros(self) -> usize {\n        self.0.leading_zeros() as usize / BITMASK_STRIDE\n    }\n}\n\nThe function to be tested is presented with each line formatted as 'line number + code':\n/// Erases the [`Bucket`]'s control byte at the given index so that it does not\n/// triggered as full, decreases the `items` of the table and, if it can be done,\n/// increases `self.growth_left`.\n///\n/// This function does not actually erase / drop the [`Bucket`] itself, i.e. it\n/// does not make any changes to the `data` parts of the table. The caller of this\n/// function must take care to properly drop the `data`, otherwise calling this\n/// function may result in a memory leak.\n///\n/// # Safety\n///\n/// You must observe the following safety rules when calling this function:\n///\n/// * The [`RawTableInner`] has already been allocated;\n///\n/// * It must be the full control byte at the given position;\n///\n/// * The `index` must not be greater than the `RawTableInner.bucket_mask`, i.e.\n///   `index <= RawTableInner.bucket_mask` or, in other words, `(index + 1)` must\n///   be no greater than the number returned by the function [`RawTableInner::buckets`].\n///\n/// Calling this function on a table that has not been allocated results in [`undefined behavior`].\n///\n/// Calling this function on a table with no elements is unspecified, but calling subsequent\n/// functions is likely to result in [`undefined behavior`] due to overflow subtraction\n/// (`self.items -= 1 cause overflow when self.items == 0`).\n///\n/// See also [`Bucket::as_ptr`] method, for more information about of properly removing\n/// or saving `data element` from / into the [`RawTable`] / [`RawTableInner`].\n///\n/// [`RawTableInner::buckets`]: RawTableInner::buckets\n/// [`Bucket::as_ptr`]: Bucket::as_ptr\n/// [`undefined behavior`]: https://doc.rust-lang.org/reference/behavior-considered-undefined.html\n3064 unsafe fn erase(&mut self, index: usize) {\n3065     debug_assert!(self.is_bucket_full(index));\n3066 \n3067     // This is the same as `index.wrapping_sub(Group::WIDTH) % self.buckets()` because\n3068     // the number of buckets is a power of two, and `self.bucket_mask = self.buckets() - 1`.\n3069     let index_before = index.wrapping_sub(Group::WIDTH) & self.bucket_mask;\n3070     // SAFETY:\n3071     // - The caller must uphold the safety contract for `erase` method;\n3072     // - `index_before` is guaranteed to be in range due to masking with `self.bucket_mask`\n3073     let empty_before = Group::load(self.ctrl(index_before)).match_empty();\n3074     let empty_after = Group::load(self.ctrl(index)).match_empty();\n3075 \n3076     // Inserting and searching in the map is performed by two key functions:\n3077     //\n3078     // - The `find_insert_slot` function that looks up the index of any `Tag::EMPTY` or `Tag::DELETED`\n3079     //   slot in a group to be able to insert. If it doesn't find an `Tag::EMPTY` or `Tag::DELETED`\n3080     //   slot immediately in the first group, it jumps to the next `Group` looking for it,\n3081     //   and so on until it has gone through all the groups in the control bytes.\n3082     //\n3083     // - The `find_inner` function that looks for the index of the desired element by looking\n3084     //   at all the `FULL` bytes in the group. If it did not find the element right away, and\n3085     //   there is no `Tag::EMPTY` byte in the group, then this means that the `find_insert_slot`\n3086     //   function may have found a suitable slot in the next group. Therefore, `find_inner`\n3087     //   jumps further, and if it does not find the desired element and again there is no `Tag::EMPTY`\n3088     //   byte, then it jumps further, and so on. The search stops only if `find_inner` function\n3089     //   finds the desired element or hits an `Tag::EMPTY` slot/byte.\n3090     //\n3091     // Accordingly, this leads to two consequences:\n3092     //\n3093     // - The map must have `Tag::EMPTY` slots (bytes);\n3094     //\n3095     // - You can't just mark the byte to be erased as `Tag::EMPTY`, because otherwise the `find_inner`\n3096     //   function may stumble upon an `Tag::EMPTY` byte before finding the desired element and stop\n3097     //   searching.\n3098     //\n3099     // Thus it is necessary to check all bytes after and before the erased element. If we are in\n3100     // a contiguous `Group` of `FULL` or `Tag::DELETED` bytes (the number of `FULL` or `Tag::DELETED` bytes\n3101     // before and after is greater than or equal to `Group::WIDTH`), then we must mark our byte as\n3102     // `Tag::DELETED` in order for the `find_inner` function to go further. On the other hand, if there\n3103     // is at least one `Tag::EMPTY` slot in the `Group`, then the `find_inner` function will still stumble\n3104     // upon an `Tag::EMPTY` byte, so we can safely mark our erased byte as `Tag::EMPTY` as well.\n3105     //\n3106     // Finally, since `index_before == (index.wrapping_sub(Group::WIDTH) & self.bucket_mask) == index`\n3107     // and given all of the above, tables smaller than the group width (self.buckets() < Group::WIDTH)\n3108     // cannot have `Tag::DELETED` bytes.\n3109     //\n3110     // Note that in this context `leading_zeros` refers to the bytes at the end of a group, while\n3111     // `trailing_zeros` refers to the bytes at the beginning of a group.\n3112     let ctrl = if empty_before.leading_zeros() + empty_after.trailing_zeros() >= Group::WIDTH {\n3113         Tag::DELETED\n3114     } else {\n3115         self.growth_left += 1;\n3116         Tag::EMPTY\n3117     };\n3118     // SAFETY: the caller must uphold the safety contract for `erase` method.\n3119     self.set_ctrl(index, ctrl);\n3120     self.items -= 1;\n3121 }\n\nGenerate each test function in such a manner that it concurrently satisfies all the following preconditions:\n",
  "depend_pt": ""
}