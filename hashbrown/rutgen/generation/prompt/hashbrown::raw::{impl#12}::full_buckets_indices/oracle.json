{
  "system_pt": "As a software testing expert, please generate accurate test oracles code based on the provided information. Follow these guidelines:\n1. Generate executable test oracle codes in plain text format, one per line. Do not output complete test functions, avoid additional explanations, and do not use Markdown.\n2. Combine the given function under test, context, relevant documentation, preconditions, expected return values or types, test input conditions or ranges, and existing test function prefixes to infer and generate corresponding test oracles code.\n3. Only generate necessary test oracles to ensure comprehensive validation.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/raw/mod.rs\n// crate name is hashbrown\nuse crate::alloc::alloc::{handle_alloc_error, Layout};\nuse crate::control::{BitMaskIter, Group, Tag, TagSliceExt};\nuse crate::scopeguard::{guard, ScopeGuard};\nuse crate::util::{invalid_mut, likely, unlikely};\nuse crate::TryReserveError;\nuse core::array;\nuse core::iter::FusedIterator;\nuse core::marker::PhantomData;\nuse core::mem;\nuse core::ptr::NonNull;\nuse core::slice;\nuse core::{hint, ptr};\n#[cfg(test)]\npub(crate) use self::alloc::AllocError;\npub(crate) use self::alloc::{do_alloc, Allocator, Global};\nstruct RawTableInner {\n    bucket_mask: usize,\n    ctrl: NonNull<u8>,\n    growth_left: usize,\n    items: usize,\n}\n#[derive(Copy, Clone)]\npub(crate) struct BitMask(pub(crate) BitMaskWord);\n#[derive(Clone)]\npub(crate) struct BitMaskIter(pub(crate) BitMask);\n#[derive(Copy, Clone, PartialEq, Eq)]\n#[repr(transparent)]\npub(crate) struct Tag(pub(super) u8);\npub(crate) struct FullBucketsIndices {\n    current_group: BitMaskIter,\n    group_first_index: usize,\n    ctrl: NonNull<u8>,\n    items: usize,\n}\nimpl RawTableInner {\n    #[cfg_attr(feature = \"inline-more\", inline)]\n    unsafe fn new_uninitialized<A>(\n        alloc: &A,\n        table_layout: TableLayout,\n        buckets: usize,\n        fallibility: Fallibility,\n    ) -> Result<Self, TryReserveError>\n    where\n        A: Allocator,\n    {\n        debug_assert!(buckets.is_power_of_two());\n        let (layout, ctrl_offset) = match table_layout.calculate_layout_for(buckets) {\n            Some(lco) => lco,\n            None => return Err(fallibility.capacity_overflow()),\n        };\n        let ptr: NonNull<u8> = match do_alloc(alloc, layout) {\n            Ok(block) => block.cast(),\n            Err(_) => return Err(fallibility.alloc_err(layout)),\n        };\n        let ctrl = NonNull::new_unchecked(ptr.as_ptr().add(ctrl_offset));\n        Ok(Self {\n            ctrl,\n            bucket_mask: buckets - 1,\n            items: 0,\n            growth_left: bucket_mask_to_capacity(buckets - 1),\n        })\n    }\n    #[inline]\n    fn fallible_with_capacity<A>(\n        alloc: &A,\n        table_layout: TableLayout,\n        capacity: usize,\n        fallibility: Fallibility,\n    ) -> Result<Self, TryReserveError>\n    where\n        A: Allocator,\n    {\n        if capacity == 0 {\n            Ok(Self::NEW)\n        } else {\n            unsafe {\n                let buckets = capacity_to_buckets(capacity)\n                    .ok_or_else(|| fallibility.capacity_overflow())?;\n                let mut result = Self::new_uninitialized(\n                    alloc,\n                    table_layout,\n                    buckets,\n                    fallibility,\n                )?;\n                result.ctrl_slice().fill_empty();\n                Ok(result)\n            }\n        }\n    }\n    fn with_capacity<A>(alloc: &A, table_layout: TableLayout, capacity: usize) -> Self\n    where\n        A: Allocator,\n    {\n        match Self::fallible_with_capacity(\n            alloc,\n            table_layout,\n            capacity,\n            Fallibility::Infallible,\n        ) {\n            Ok(table_inner) => table_inner,\n            Err(_) => unsafe { hint::unreachable_unchecked() }\n        }\n    }\n    #[inline]\n    unsafe fn fix_insert_slot(&self, mut index: usize) -> InsertSlot {}\n    #[inline]\n    fn find_insert_slot_in_group(\n        &self,\n        group: &Group,\n        probe_seq: &ProbeSeq,\n    ) -> Option<usize> {}\n    #[inline]\n    unsafe fn find_or_find_insert_slot_inner(\n        &self,\n        hash: u64,\n        eq: &mut dyn FnMut(usize) -> bool,\n    ) -> Result<usize, InsertSlot> {}\n    #[inline]\n    unsafe fn prepare_insert_slot(&mut self, hash: u64) -> (usize, Tag) {}\n    #[inline]\n    unsafe fn find_insert_slot(&self, hash: u64) -> InsertSlot {}\n    #[inline(always)]\n    unsafe fn find_inner(\n        &self,\n        hash: u64,\n        eq: &mut dyn FnMut(usize) -> bool,\n    ) -> Option<usize> {}\n    #[allow(clippy::mut_mut)]\n    #[inline]\n    unsafe fn prepare_rehash_in_place(&mut self) {}\n    #[inline]\n    unsafe fn iter<T>(&self) -> RawIter<T> {}\n    unsafe fn drop_elements<T>(&mut self) {}\n    unsafe fn drop_inner_table<T, A: Allocator>(\n        &mut self,\n        alloc: &A,\n        table_layout: TableLayout,\n    ) {}\n    #[inline]\n    unsafe fn bucket<T>(&self, index: usize) -> Bucket<T> {}\n    #[inline]\n    unsafe fn bucket_ptr(&self, index: usize, size_of: usize) -> *mut u8 {}\n    #[inline]\n    fn data_end<T>(&self) -> NonNull<T> {}\n    #[inline]\n    fn probe_seq(&self, hash: u64) -> ProbeSeq {}\n    #[inline]\n    unsafe fn record_item_insert_at(&mut self, index: usize, old_ctrl: Tag, hash: u64) {}\n    #[inline]\n    fn is_in_same_group(&self, i: usize, new_i: usize, hash: u64) -> bool {}\n    #[inline]\n    unsafe fn set_ctrl_hash(&mut self, index: usize, hash: u64) {}\n    #[inline]\n    unsafe fn replace_ctrl_hash(&mut self, index: usize, hash: u64) -> Tag {}\n    #[inline]\n    unsafe fn set_ctrl(&mut self, index: usize, ctrl: Tag) {}\n    #[inline]\n    unsafe fn ctrl(&self, index: usize) -> *mut Tag {\n        debug_assert!(index < self.num_ctrl_bytes());\n        self.ctrl.as_ptr().add(index).cast()\n    }\n    fn ctrl_slice(&mut self) -> &mut [Tag] {}\n    #[inline]\n    fn buckets(&self) -> usize {}\n    #[inline]\n    unsafe fn is_bucket_full(&self, index: usize) -> bool {}\n    #[inline]\n    fn num_ctrl_bytes(&self) -> usize {}\n    #[inline]\n    fn is_empty_singleton(&self) -> bool {}\n    #[allow(clippy::mut_mut)]\n    #[inline]\n    fn prepare_resize<'a, A>(\n        &self,\n        alloc: &'a A,\n        table_layout: TableLayout,\n        capacity: usize,\n        fallibility: Fallibility,\n    ) -> Result<\n        crate::scopeguard::ScopeGuard<Self, impl FnMut(&mut Self) + 'a>,\n        TryReserveError,\n    >\n    where\n        A: Allocator,\n    {\n        debug_assert!(self.items <= capacity);\n        let new_table = RawTableInner::fallible_with_capacity(\n            alloc,\n            table_layout,\n            capacity,\n            fallibility,\n        )?;\n        Ok(\n            guard(\n                new_table,\n                move |self_| {\n                    if !self_.is_empty_singleton() {\n                        unsafe { self_.free_buckets(alloc, table_layout) };\n                    }\n                },\n            ),\n        )\n    }\n    #[allow(clippy::inline_always)]\n    #[inline(always)]\n    unsafe fn reserve_rehash_inner<A>(\n        &mut self,\n        alloc: &A,\n        additional: usize,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        fallibility: Fallibility,\n        layout: TableLayout,\n        drop: Option<unsafe fn(*mut u8)>,\n    ) -> Result<(), TryReserveError>\n    where\n        A: Allocator,\n    {}\n    #[inline(always)]\n    unsafe fn full_buckets_indices(&self) -> FullBucketsIndices {\n        let ctrl = NonNull::new_unchecked(self.ctrl(0).cast::<u8>());\n        FullBucketsIndices {\n            current_group: Group::load_aligned(ctrl.as_ptr().cast())\n                .match_full()\n                .into_iter(),\n            group_first_index: 0,\n            ctrl,\n            items: self.items,\n        }\n    }\n    #[allow(clippy::inline_always)]\n    #[inline(always)]\n    unsafe fn resize_inner<A>(\n        &mut self,\n        alloc: &A,\n        capacity: usize,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        fallibility: Fallibility,\n        layout: TableLayout,\n    ) -> Result<(), TryReserveError>\n    where\n        A: Allocator,\n    {}\n    #[allow(clippy::inline_always)]\n    #[cfg_attr(feature = \"inline-more\", inline(always))]\n    #[cfg_attr(not(feature = \"inline-more\"), inline)]\n    unsafe fn rehash_in_place(\n        &mut self,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        size_of: usize,\n        drop: Option<unsafe fn(*mut u8)>,\n    ) {}\n    #[inline]\n    unsafe fn free_buckets<A>(&mut self, alloc: &A, table_layout: TableLayout)\n    where\n        A: Allocator,\n    {}\n    #[inline]\n    unsafe fn allocation_info(\n        &self,\n        table_layout: TableLayout,\n    ) -> (NonNull<u8>, Layout) {}\n    #[inline]\n    unsafe fn allocation_size_or_zero(&self, table_layout: TableLayout) -> usize {}\n    #[inline]\n    fn clear_no_drop(&mut self) {}\n    #[inline]\n    unsafe fn erase(&mut self, index: usize) {}\n}\nimpl IntoIterator for BitMask {\n    type Item = usize;\n    type IntoIter = BitMaskIter;\n    #[inline]\n    fn into_iter(self) -> BitMaskIter {\n        BitMaskIter(BitMask(self.0 & BITMASK_ITER_MASK))\n    }\n}\n\nThe function to be tested is presented with each line formatted as 'line number + code':\n/// Returns an iterator over full buckets indices in the table.\n///\n/// # Safety\n///\n/// Behavior is undefined if any of the following conditions are violated:\n///\n/// * The caller has to ensure that the `RawTableInner` outlives the\n///   `FullBucketsIndices`. Because we cannot make the `next` method\n///   unsafe on the `FullBucketsIndices` struct, we have to make the\n///   `full_buckets_indices` method unsafe.\n///\n/// * The [`RawTableInner`] must have properly initialized control bytes.\n2662 unsafe fn full_buckets_indices(&self) -> FullBucketsIndices {\n2663     // SAFETY:\n2664     // 1. Since the caller of this function ensures that the control bytes\n2665     //    are properly initialized and `self.ctrl(0)` points to the start\n2666     //    of the array of control bytes, therefore: `ctrl` is valid for reads,\n2667     //    properly aligned to `Group::WIDTH` and points to the properly initialized\n2668     //    control bytes.\n2669     // 2. The value of `items` is equal to the amount of data (values) added\n2670     //    to the table.\n2671     //\n2672     //                         `ctrl` points here (to the start\n2673     //                         of the first control byte `CT0`)\n2674     //                          âˆ¨\n2675     // [Pad], T_n, ..., T1, T0, |CT0, CT1, ..., CT_n|, Group::WIDTH\n2676     //                           \\________  ________/\n2677     //                                    \\/\n2678     //       `n = buckets - 1`, i.e. `RawTableInner::buckets() - 1`\n2679     //\n2680     // where: T0...T_n  - our stored data;\n2681     //        CT0...CT_n - control bytes or metadata for `data`.\n2682     let ctrl = NonNull::new_unchecked(self.ctrl(0).cast::<u8>());\n2683 \n2684     FullBucketsIndices {\n2685         // Load the first group\n2686         // SAFETY: See explanation above.\n2687         current_group: Group::load_aligned(ctrl.as_ptr().cast())\n2688             .match_full()\n2689             .into_iter(),\n2690         group_first_index: 0,\n2691         ctrl,\n2692         items: self.items,\n2693     }\n2694 }\n\nThe path conditions that the generated test functions should satisfy are as follows:\n"
}