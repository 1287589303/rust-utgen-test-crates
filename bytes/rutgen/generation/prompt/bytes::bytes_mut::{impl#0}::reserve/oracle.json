{
  "system_pt": "As a software testing expert, please generate accurate test oracles code based on the provided information. Follow these guidelines:\n1. Generate executable test oracle codes in plain text format, one per line. Do not output complete test functions, avoid additional explanations, and do not use Markdown.\n2. Combine the given function under test, context, relevant documentation, preconditions, expected return values or types, test input conditions or ranges, and existing test function prefixes to infer and generate corresponding test oracles code.\n3. Only generate necessary test oracles to ensure comprehensive validation.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/bytes_mut.rs\n// crate name is bytes\nuse core::iter::FromIterator;\nuse core::mem::{self, ManuallyDrop, MaybeUninit};\nuse core::ops::{Deref, DerefMut};\nuse core::ptr::{self, NonNull};\nuse core::{cmp, fmt, hash, isize, slice, usize};\nuse alloc::{\n    borrow::{Borrow, BorrowMut},\n    boxed::Box, string::String, vec, vec::Vec,\n};\nuse crate::buf::{IntoIter, UninitSlice};\nuse crate::bytes::Vtable;\n#[allow(unused)]\nuse crate::loom::sync::atomic::AtomicMut;\nuse crate::loom::sync::atomic::{AtomicPtr, AtomicUsize, Ordering};\nuse crate::{offset_from, Buf, BufMut, Bytes, TryGetError};\nstatic SHARED_VTABLE: Vtable = Vtable {\n    clone: shared_v_clone,\n    into_vec: shared_v_to_vec,\n    into_mut: shared_v_to_mut,\n    is_unique: shared_v_is_unique,\n    drop: shared_v_drop,\n};\nconst _: [(); 0 - mem::align_of::<Shared>() % 2] = [];\nconst KIND_ARC: usize = 0b0;\nconst KIND_VEC: usize = 0b1;\nconst KIND_MASK: usize = 0b1;\nconst MAX_ORIGINAL_CAPACITY_WIDTH: usize = 17;\nconst MIN_ORIGINAL_CAPACITY_WIDTH: usize = 10;\nconst ORIGINAL_CAPACITY_MASK: usize = 0b11100;\nconst ORIGINAL_CAPACITY_OFFSET: usize = 2;\nconst VEC_POS_OFFSET: usize = 5;\nconst MAX_VEC_POS: usize = usize::MAX >> VEC_POS_OFFSET;\nconst NOT_VEC_POS_MASK: usize = 0b11111;\n#[cfg(target_pointer_width = \"64\")]\nconst PTR_WIDTH: usize = 64;\n#[cfg(target_pointer_width = \"32\")]\nconst PTR_WIDTH: usize = 32;\npub struct BytesMut {\n    ptr: NonNull<u8>,\n    len: usize,\n    cap: usize,\n    data: *mut Shared,\n}\nstruct Shared {\n    vec: Vec<u8>,\n    original_capacity_repr: usize,\n    ref_count: AtomicUsize,\n}\nstruct Shared {\n    buf: *mut u8,\n    cap: usize,\n    ref_cnt: AtomicUsize,\n}\nimpl BytesMut {\n    #[inline]\n    pub fn with_capacity(capacity: usize) -> BytesMut {}\n    #[inline]\n    pub fn new() -> BytesMut {}\n    #[inline]\n    pub fn len(&self) -> usize {\n        self.len\n    }\n    #[inline]\n    pub fn is_empty(&self) -> bool {}\n    #[inline]\n    pub fn capacity(&self) -> usize {\n        self.cap\n    }\n    #[inline]\n    pub fn freeze(self) -> Bytes {}\n    pub fn zeroed(len: usize) -> BytesMut {}\n    #[must_use = \"consider BytesMut::truncate if you don't need the other half\"]\n    pub fn split_off(&mut self, at: usize) -> BytesMut {}\n    #[must_use = \"consider BytesMut::clear if you don't need the other half\"]\n    pub fn split(&mut self) -> BytesMut {}\n    #[must_use = \"consider BytesMut::advance if you don't need the other half\"]\n    pub fn split_to(&mut self, at: usize) -> BytesMut {}\n    pub fn truncate(&mut self, len: usize) {}\n    pub fn clear(&mut self) {}\n    pub fn resize(&mut self, new_len: usize, value: u8) {}\n    #[inline]\n    pub unsafe fn set_len(&mut self, len: usize) {}\n    #[inline]\n    pub fn reserve(&mut self, additional: usize) {\n        let len = self.len();\n        let rem = self.capacity() - len;\n        if additional <= rem {\n            return;\n        }\n        let _ = self.reserve_inner(additional, true);\n    }\n    fn reserve_inner(&mut self, additional: usize, allocate: bool) -> bool {\n        let len = self.len();\n        let kind = self.kind();\n        if kind == KIND_VEC {\n            unsafe {\n                let off = self.get_vec_pos();\n                if self.capacity() - self.len() + off >= additional && off >= self.len()\n                {\n                    let base_ptr = self.ptr.as_ptr().sub(off);\n                    ptr::copy_nonoverlapping(self.ptr.as_ptr(), base_ptr, self.len);\n                    self.ptr = vptr(base_ptr);\n                    self.set_vec_pos(0);\n                    self.cap += off;\n                } else {\n                    if !allocate {\n                        return false;\n                    }\n                    let mut v = ManuallyDrop::new(\n                        rebuild_vec(self.ptr.as_ptr(), self.len, self.cap, off),\n                    );\n                    v.reserve(additional);\n                    self.ptr = vptr(v.as_mut_ptr().add(off));\n                    self.cap = v.capacity() - off;\n                    debug_assert_eq!(self.len, v.len() - off);\n                }\n                return true;\n            }\n        }\n        debug_assert_eq!(kind, KIND_ARC);\n        let shared: *mut Shared = self.data;\n        let mut new_cap = match len.checked_add(additional) {\n            Some(new_cap) => new_cap,\n            None if !allocate => return false,\n            None => panic!(\"overflow\"),\n        };\n        unsafe {\n            if (*shared).is_unique() {\n                let v = &mut (*shared).vec;\n                let v_capacity = v.capacity();\n                let ptr = v.as_mut_ptr();\n                let offset = offset_from(self.ptr.as_ptr(), ptr);\n                if v_capacity >= new_cap + offset {\n                    self.cap = new_cap;\n                } else if v_capacity >= new_cap && offset >= len {\n                    ptr::copy_nonoverlapping(self.ptr.as_ptr(), ptr, len);\n                    self.ptr = vptr(ptr);\n                    self.cap = v.capacity();\n                } else {\n                    if !allocate {\n                        return false;\n                    }\n                    let off = (self.ptr.as_ptr() as usize) - (v.as_ptr() as usize);\n                    new_cap = new_cap.checked_add(off).expect(\"overflow\");\n                    let double = v.capacity().checked_shl(1).unwrap_or(new_cap);\n                    new_cap = cmp::max(double, new_cap);\n                    debug_assert!(off + len <= v.capacity());\n                    v.set_len(off + len);\n                    v.reserve(new_cap - v.len());\n                    self.ptr = vptr(v.as_mut_ptr().add(off));\n                    self.cap = v.capacity() - off;\n                }\n                return true;\n            }\n        }\n        if !allocate {\n            return false;\n        }\n        let original_capacity_repr = unsafe { (*shared).original_capacity_repr };\n        let original_capacity = original_capacity_from_repr(original_capacity_repr);\n        new_cap = cmp::max(new_cap, original_capacity);\n        let mut v = ManuallyDrop::new(Vec::with_capacity(new_cap));\n        v.extend_from_slice(self.as_ref());\n        unsafe { release_shared(shared) };\n        let data = (original_capacity_repr << ORIGINAL_CAPACITY_OFFSET) | KIND_VEC;\n        self.data = invalid_ptr(data);\n        self.ptr = vptr(v.as_mut_ptr());\n        self.cap = v.capacity();\n        debug_assert_eq!(self.len, v.len());\n        return true;\n    }\n    #[inline]\n    #[must_use = \"consider BytesMut::reserve if you need an infallible reservation\"]\n    pub fn try_reclaim(&mut self, additional: usize) -> bool {}\n    #[inline]\n    pub fn extend_from_slice(&mut self, extend: &[u8]) {}\n    pub fn unsplit(&mut self, other: BytesMut) {}\n    #[inline]\n    pub(crate) fn from_vec(vec: Vec<u8>) -> BytesMut {}\n    #[inline]\n    fn as_slice(&self) -> &[u8] {}\n    #[inline]\n    fn as_slice_mut(&mut self) -> &mut [u8] {}\n    pub(crate) unsafe fn advance_unchecked(&mut self, count: usize) {}\n    fn try_unsplit(&mut self, other: BytesMut) -> Result<(), BytesMut> {}\n    #[inline]\n    fn kind(&self) -> usize {}\n    unsafe fn promote_to_shared(&mut self, ref_cnt: usize) {}\n    #[inline]\n    unsafe fn shallow_clone(&mut self) -> BytesMut {}\n    #[inline]\n    unsafe fn get_vec_pos(&self) -> usize {}\n    #[inline]\n    unsafe fn set_vec_pos(&mut self, pos: usize) {}\n    #[inline]\n    pub fn spare_capacity_mut(&mut self) -> &mut [MaybeUninit<u8>] {}\n}\n\nThe function to be tested is presented with each line formatted as 'line number + code':\n/// Reserves capacity for at least `additional` more bytes to be inserted\n/// into the given `BytesMut`.\n///\n/// More than `additional` bytes may be reserved in order to avoid frequent\n/// reallocations. A call to `reserve` may result in an allocation.\n///\n/// Before allocating new buffer space, the function will attempt to reclaim\n/// space in the existing buffer. If the current handle references a view\n/// into a larger original buffer, and all other handles referencing part\n/// of the same original buffer have been dropped, then the current view\n/// can be copied/shifted to the front of the buffer and the handle can take\n/// ownership of the full buffer, provided that the full buffer is large\n/// enough to fit the requested additional capacity.\n///\n/// This optimization will only happen if shifting the data from the current\n/// view to the front of the buffer is not too expensive in terms of the\n/// (amortized) time required. The precise condition is subject to change;\n/// as of now, the length of the data being shifted needs to be at least as\n/// large as the distance that it's shifted by. If the current view is empty\n/// and the original buffer is large enough to fit the requested additional\n/// capacity, then reallocations will never happen.\n///\n/// # Examples\n///\n/// In the following example, a new buffer is allocated.\n///\n/// ```\n/// use bytes::BytesMut;\n///\n/// let mut buf = BytesMut::from(&b\"hello\"[..]);\n/// buf.reserve(64);\n/// assert!(buf.capacity() >= 69);\n/// ```\n///\n/// In the following example, the existing buffer is reclaimed.\n///\n/// ```\n/// use bytes::{BytesMut, BufMut};\n///\n/// let mut buf = BytesMut::with_capacity(128);\n/// buf.put(&[0; 64][..]);\n///\n/// let ptr = buf.as_ptr();\n/// let other = buf.split();\n///\n/// assert!(buf.is_empty());\n/// assert_eq!(buf.capacity(), 64);\n///\n/// drop(other);\n/// buf.reserve(128);\n///\n/// assert_eq!(buf.capacity(), 128);\n/// assert_eq!(buf.as_ptr(), ptr);\n/// ```\n///\n/// # Panics\n///\n/// Panics if the new capacity overflows `usize`.\n592 pub fn reserve(&mut self, additional: usize) {\n593     let len = self.len();\n594     let rem = self.capacity() - len;\n595 \n596     if additional <= rem {\n597         // The handle can already store at least `additional` more bytes, so\n598         // there is no further work needed to be done.\n599         return;\n600     }\n601 \n602     // will always succeed\n603     let _ = self.reserve_inner(additional, true);\n604 }\n\nThe path conditions that the generated test functions should satisfy are as follows:\n"
}