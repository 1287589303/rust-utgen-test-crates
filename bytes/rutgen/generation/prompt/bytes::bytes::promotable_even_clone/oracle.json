{
  "system_pt": "As a software testing expert, please generate accurate test oracles code based on the provided information. Follow these guidelines:\n1. Generate executable test oracle codes in plain text format, one per line. Do not output complete test functions, avoid additional explanations, and do not use Markdown.\n2. Combine the given function under test, context, relevant documentation, preconditions, expected return values or types, test input conditions or ranges, and existing test function prefixes to infer and generate corresponding test oracles code.\n3. Only generate necessary test oracles to ensure comprehensive validation.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/bytes.rs\n// crate name is bytes\nuse core::iter::FromIterator;\nuse core::mem::{self, ManuallyDrop};\nuse core::ops::{Deref, RangeBounds};\nuse core::ptr::NonNull;\nuse core::{cmp, fmt, hash, ptr, slice, usize};\nuse alloc::{\n    alloc::{dealloc, Layout},\n    borrow::Borrow, boxed::Box, string::String, vec::Vec,\n};\nuse crate::buf::IntoIter;\n#[allow(unused)]\nuse crate::loom::sync::atomic::AtomicMut;\nuse crate::loom::sync::atomic::{AtomicPtr, AtomicUsize, Ordering};\nuse crate::{offset_from, Buf, BytesMut};\nstatic OWNED_VTABLE: Vtable = Vtable {\n    clone: owned_clone,\n    into_vec: owned_to_vec,\n    into_mut: owned_to_mut,\n    is_unique: owned_is_unique,\n    drop: owned_drop,\n};\nstatic PROMOTABLE_EVEN_VTABLE: Vtable = Vtable {\n    clone: promotable_even_clone,\n    into_vec: promotable_even_to_vec,\n    into_mut: promotable_even_to_mut,\n    is_unique: promotable_is_unique,\n    drop: promotable_even_drop,\n};\nstatic PROMOTABLE_ODD_VTABLE: Vtable = Vtable {\n    clone: promotable_odd_clone,\n    into_vec: promotable_odd_to_vec,\n    into_mut: promotable_odd_to_mut,\n    is_unique: promotable_is_unique,\n    drop: promotable_odd_drop,\n};\nstatic SHARED_VTABLE: Vtable = Vtable {\n    clone: shared_clone,\n    into_vec: shared_to_vec,\n    into_mut: shared_to_mut,\n    is_unique: shared_is_unique,\n    drop: shared_drop,\n};\nconst STATIC_VTABLE: Vtable = Vtable {\n    clone: static_clone,\n    into_vec: static_to_vec,\n    into_mut: static_to_mut,\n    is_unique: static_is_unique,\n    drop: static_drop,\n};\nconst _: [(); 0 - mem::align_of::<Shared>() % 2] = [];\nconst KIND_ARC: usize = 0b0;\nconst KIND_VEC: usize = 0b1;\nconst KIND_MASK: usize = 0b1;\npub struct Bytes {\n    ptr: *const u8,\n    len: usize,\n    data: AtomicPtr<()>,\n    vtable: &'static Vtable,\n}\nstruct Shared {\n    buf: *mut u8,\n    cap: usize,\n    ref_cnt: AtomicUsize,\n}\nunsafe fn promotable_even_clone(\n    data: &AtomicPtr<()>,\n    ptr: *const u8,\n    len: usize,\n) -> Bytes {\n    let shared = data.load(Ordering::Acquire);\n    let kind = shared as usize & KIND_MASK;\n    if kind == KIND_ARC {\n        shallow_clone_arc(shared.cast(), ptr, len)\n    } else {\n        debug_assert_eq!(kind, KIND_VEC);\n        let buf = ptr_map(shared.cast(), |addr| addr & !KIND_MASK);\n        shallow_clone_vec(data, shared, buf, ptr, len)\n    }\n}\n#[cfg(not(miri))]\nfn ptr_map<F>(ptr: *mut u8, f: F) -> *mut u8\nwhere\n    F: FnOnce(usize) -> usize,\n{\n    let old_addr = ptr as usize;\n    let new_addr = f(old_addr);\n    new_addr as *mut u8\n}\nunsafe fn shallow_clone_arc(shared: *mut Shared, ptr: *const u8, len: usize) -> Bytes {\n    let old_size = (*shared).ref_cnt.fetch_add(1, Ordering::Relaxed);\n    if old_size > usize::MAX >> 1 {\n        crate::abort();\n    }\n    Bytes {\n        ptr,\n        len,\n        data: AtomicPtr::new(shared as _),\n        vtable: &SHARED_VTABLE,\n    }\n}\n#[cold]\nunsafe fn shallow_clone_vec(\n    atom: &AtomicPtr<()>,\n    ptr: *const (),\n    buf: *mut u8,\n    offset: *const u8,\n    len: usize,\n) -> Bytes {\n    let shared = Box::new(Shared {\n        buf,\n        cap: offset_from(offset, buf) + len,\n        ref_cnt: AtomicUsize::new(2),\n    });\n    let shared = Box::into_raw(shared);\n    debug_assert!(\n        0 == (shared as usize & KIND_MASK),\n        \"internal: Box<Shared> should have an aligned pointer\",\n    );\n    match atom\n        .compare_exchange(ptr as _, shared as _, Ordering::AcqRel, Ordering::Acquire)\n    {\n        Ok(actual) => {\n            debug_assert!(actual as usize == ptr as usize);\n            Bytes {\n                ptr: offset,\n                len,\n                data: AtomicPtr::new(shared as _),\n                vtable: &SHARED_VTABLE,\n            }\n        }\n        Err(actual) => {\n            let shared = Box::from_raw(shared);\n            mem::forget(*shared);\n            shallow_clone_arc(actual as _, offset, len)\n        }\n    }\n}\n\nThe function to be tested is presented with each line formatted as 'line number + code':\n1208 unsafe fn promotable_even_clone(data: &AtomicPtr<()>, ptr: *const u8, len: usize) -> Bytes {\n1209     let shared = data.load(Ordering::Acquire);\n1210     let kind = shared as usize & KIND_MASK;\n1211 \n1212     if kind == KIND_ARC {\n1213         shallow_clone_arc(shared.cast(), ptr, len)\n1214     } else {\n1215         debug_assert_eq!(kind, KIND_VEC);\n1216         let buf = ptr_map(shared.cast(), |addr| addr & !KIND_MASK);\n1217         shallow_clone_vec(data, shared, buf, ptr, len)\n1218     }\n1219 }\n\nThe path conditions that the generated test functions should satisfy are as follows:\n"
}