{
  "system_pt": "As a software testing expert, please generate accurate test oracles code based on the provided information. Follow these guidelines:\n1. Generate executable test oracle codes in plain text format, one per line. Do not output complete test functions, avoid additional explanations, and do not use Markdown.\n2. Combine the given function under test, context, relevant documentation, preconditions, expected return values or types, test input conditions or ranges, and existing test function prefixes to infer and generate corresponding test oracles code.\n3. Only generate necessary test oracles to ensure comprehensive validation.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/bytes.rs\n// crate name is bytes\nuse core::iter::FromIterator;\nuse core::mem::{self, ManuallyDrop, MaybeUninit};\nuse core::ops::{Deref, DerefMut};\nuse core::ptr::{self, NonNull};\nuse core::{cmp, fmt, hash, isize, slice, usize};\nuse alloc::{\n    borrow::{Borrow, BorrowMut},\n    boxed::Box, string::String, vec, vec::Vec,\n};\nuse crate::buf::{IntoIter, UninitSlice};\nuse crate::bytes::Vtable;\n#[allow(unused)]\nuse crate::loom::sync::atomic::AtomicMut;\nuse crate::loom::sync::atomic::{AtomicPtr, AtomicUsize, Ordering};\nuse crate::{offset_from, Buf, BufMut, Bytes, TryGetError};\nstatic SHARED_VTABLE: Vtable = Vtable {\n    clone: shared_v_clone,\n    into_vec: shared_v_to_vec,\n    into_mut: shared_v_to_mut,\n    is_unique: shared_v_is_unique,\n    drop: shared_v_drop,\n};\nconst _: [(); 0 - mem::align_of::<Shared>() % 2] = [];\nconst KIND_ARC: usize = 0b0;\nconst KIND_VEC: usize = 0b1;\nconst KIND_MASK: usize = 0b1;\nconst MAX_ORIGINAL_CAPACITY_WIDTH: usize = 17;\nconst MIN_ORIGINAL_CAPACITY_WIDTH: usize = 10;\nconst ORIGINAL_CAPACITY_MASK: usize = 0b11100;\nconst ORIGINAL_CAPACITY_OFFSET: usize = 2;\nconst VEC_POS_OFFSET: usize = 5;\nconst MAX_VEC_POS: usize = usize::MAX >> VEC_POS_OFFSET;\nconst NOT_VEC_POS_MASK: usize = 0b11111;\n#[cfg(target_pointer_width = \"64\")]\nconst PTR_WIDTH: usize = 64;\n#[cfg(target_pointer_width = \"32\")]\nconst PTR_WIDTH: usize = 32;\npub trait Buf {\n    fn remaining(&self) -> usize;\n    #[cfg_attr(docsrs, doc(alias = \"bytes\"))]\n    fn chunk(&self) -> &[u8];\n    #[cfg(feature = \"std\")]\n    #[cfg_attr(docsrs, doc(cfg(feature = \"std\")))]\n    fn chunks_vectored<'a>(&'a self, dst: &mut [IoSlice<'a>]) -> usize;\n    fn advance(&mut self, cnt: usize);\n    fn has_remaining(&self) -> bool;\n    fn copy_to_slice(&mut self, dst: &mut [u8]);\n    fn get_u8(&mut self) -> u8;\n    fn get_i8(&mut self) -> i8;\n    fn get_u16(&mut self) -> u16;\n    fn get_u16_le(&mut self) -> u16;\n    fn get_u16_ne(&mut self) -> u16;\n    fn get_i16(&mut self) -> i16;\n    fn get_i16_le(&mut self) -> i16;\n    fn get_i16_ne(&mut self) -> i16;\n    fn get_u32(&mut self) -> u32;\n    fn get_u32_le(&mut self) -> u32;\n    fn get_u32_ne(&mut self) -> u32;\n    fn get_i32(&mut self) -> i32;\n    fn get_i32_le(&mut self) -> i32;\n    fn get_i32_ne(&mut self) -> i32;\n    fn get_u64(&mut self) -> u64;\n    fn get_u64_le(&mut self) -> u64;\n    fn get_u64_ne(&mut self) -> u64;\n    fn get_i64(&mut self) -> i64;\n    fn get_i64_le(&mut self) -> i64;\n    fn get_i64_ne(&mut self) -> i64;\n    fn get_u128(&mut self) -> u128;\n    fn get_u128_le(&mut self) -> u128;\n    fn get_u128_ne(&mut self) -> u128;\n    fn get_i128(&mut self) -> i128;\n    fn get_i128_le(&mut self) -> i128;\n    fn get_i128_ne(&mut self) -> i128;\n    fn get_uint(&mut self, nbytes: usize) -> u64;\n    fn get_uint_le(&mut self, nbytes: usize) -> u64;\n    fn get_uint_ne(&mut self, nbytes: usize) -> u64;\n    fn get_int(&mut self, nbytes: usize) -> i64;\n    fn get_int_le(&mut self, nbytes: usize) -> i64;\n    fn get_int_ne(&mut self, nbytes: usize) -> i64;\n    fn get_f32(&mut self) -> f32;\n    fn get_f32_le(&mut self) -> f32;\n    fn get_f32_ne(&mut self) -> f32;\n    fn get_f64(&mut self) -> f64;\n    fn get_f64_le(&mut self) -> f64;\n    fn get_f64_ne(&mut self) -> f64;\n    fn try_copy_to_slice(&mut self, mut dst: &mut [u8]) -> Result<(), TryGetError>;\n    fn try_get_u8(&mut self) -> Result<u8, TryGetError>;\n    fn try_get_i8(&mut self) -> Result<i8, TryGetError>;\n    fn try_get_u16(&mut self) -> Result<u16, TryGetError>;\n    fn try_get_u16_le(&mut self) -> Result<u16, TryGetError>;\n    fn try_get_u16_ne(&mut self) -> Result<u16, TryGetError>;\n    fn try_get_i16(&mut self) -> Result<i16, TryGetError>;\n    fn try_get_i16_le(&mut self) -> Result<i16, TryGetError>;\n    fn try_get_i16_ne(&mut self) -> Result<i16, TryGetError>;\n    fn try_get_u32(&mut self) -> Result<u32, TryGetError>;\n    fn try_get_u32_le(&mut self) -> Result<u32, TryGetError>;\n    fn try_get_u32_ne(&mut self) -> Result<u32, TryGetError>;\n    fn try_get_i32(&mut self) -> Result<i32, TryGetError>;\n    fn try_get_i32_le(&mut self) -> Result<i32, TryGetError>;\n    fn try_get_i32_ne(&mut self) -> Result<i32, TryGetError>;\n    fn try_get_u64(&mut self) -> Result<u64, TryGetError>;\n    fn try_get_u64_le(&mut self) -> Result<u64, TryGetError>;\n    fn try_get_u64_ne(&mut self) -> Result<u64, TryGetError>;\n    fn try_get_i64(&mut self) -> Result<i64, TryGetError>;\n    fn try_get_i64_le(&mut self) -> Result<i64, TryGetError>;\n    fn try_get_i64_ne(&mut self) -> Result<i64, TryGetError>;\n    fn try_get_u128(&mut self) -> Result<u128, TryGetError>;\n    fn try_get_u128_le(&mut self) -> Result<u128, TryGetError>;\n    fn try_get_u128_ne(&mut self) -> Result<u128, TryGetError>;\n    fn try_get_i128(&mut self) -> Result<i128, TryGetError>;\n    fn try_get_i128_le(&mut self) -> Result<i128, TryGetError>;\n    fn try_get_i128_ne(&mut self) -> Result<i128, TryGetError>;\n    fn try_get_uint(&mut self, nbytes: usize) -> Result<u64, TryGetError>;\n    fn try_get_uint_le(&mut self, nbytes: usize) -> Result<u64, TryGetError>;\n    fn try_get_uint_ne(&mut self, nbytes: usize) -> Result<u64, TryGetError>;\n    fn try_get_int(&mut self, nbytes: usize) -> Result<i64, TryGetError>;\n    fn try_get_int_le(&mut self, nbytes: usize) -> Result<i64, TryGetError>;\n    fn try_get_int_ne(&mut self, nbytes: usize) -> Result<i64, TryGetError>;\n    fn try_get_f32(&mut self) -> Result<f32, TryGetError>;\n    fn try_get_f32_le(&mut self) -> Result<f32, TryGetError>;\n    fn try_get_f32_ne(&mut self) -> Result<f32, TryGetError>;\n    fn try_get_f64(&mut self) -> Result<f64, TryGetError>;\n    fn try_get_f64_le(&mut self) -> Result<f64, TryGetError>;\n    fn try_get_f64_ne(&mut self) -> Result<f64, TryGetError>;\n    fn copy_to_bytes(&mut self, len: usize) -> crate::Bytes;\n    fn take(self, limit: usize) -> Take<Self>\n    where\n        Self: Sized,\n    {\n        take::new(self, limit)\n    }\n    fn chain<U: Buf>(self, next: U) -> Chain<Self, U>\n    where\n        Self: Sized,\n    {\n        Chain::new(self, next)\n    }\n    #[cfg(feature = \"std\")]\n    #[cfg_attr(docsrs, doc(cfg(feature = \"std\")))]\n    fn reader(self) -> Reader<Self>\n    where\n        Self: Sized,\n    {\n        reader::new(self)\n    }\n}\npub unsafe trait BufMut {\n    fn remaining_mut(&self) -> usize;\n    unsafe fn advance_mut(&mut self, cnt: usize);\n    #[inline]\n    fn has_remaining_mut(&self) -> bool;\n    #[cfg_attr(docsrs, doc(alias = \"bytes_mut\"))]\n    fn chunk_mut(&mut self) -> &mut UninitSlice;\n    #[inline]\n    fn put<T: super::Buf>(&mut self, mut src: T)\n    where\n        Self: Sized;\n    #[inline]\n    fn put_slice(&mut self, mut src: &[u8]);\n    #[inline]\n    fn put_bytes(&mut self, val: u8, mut cnt: usize);\n    #[inline]\n    fn put_u8(&mut self, n: u8);\n    #[inline]\n    fn put_i8(&mut self, n: i8);\n    #[inline]\n    fn put_u16(&mut self, n: u16);\n    #[inline]\n    fn put_u16_le(&mut self, n: u16);\n    #[inline]\n    fn put_u16_ne(&mut self, n: u16);\n    #[inline]\n    fn put_i16(&mut self, n: i16);\n    #[inline]\n    fn put_i16_le(&mut self, n: i16);\n    #[inline]\n    fn put_i16_ne(&mut self, n: i16);\n    #[inline]\n    fn put_u32(&mut self, n: u32);\n    #[inline]\n    fn put_u32_le(&mut self, n: u32);\n    #[inline]\n    fn put_u32_ne(&mut self, n: u32);\n    #[inline]\n    fn put_i32(&mut self, n: i32);\n    #[inline]\n    fn put_i32_le(&mut self, n: i32);\n    #[inline]\n    fn put_i32_ne(&mut self, n: i32);\n    #[inline]\n    fn put_u64(&mut self, n: u64);\n    #[inline]\n    fn put_u64_le(&mut self, n: u64);\n    #[inline]\n    fn put_u64_ne(&mut self, n: u64);\n    #[inline]\n    fn put_i64(&mut self, n: i64);\n    #[inline]\n    fn put_i64_le(&mut self, n: i64);\n    #[inline]\n    fn put_i64_ne(&mut self, n: i64);\n    #[inline]\n    fn put_u128(&mut self, n: u128);\n    #[inline]\n    fn put_u128_le(&mut self, n: u128);\n    #[inline]\n    fn put_u128_ne(&mut self, n: u128);\n    #[inline]\n    fn put_i128(&mut self, n: i128);\n    #[inline]\n    fn put_i128_le(&mut self, n: i128);\n    #[inline]\n    fn put_i128_ne(&mut self, n: i128);\n    #[inline]\n    fn put_uint(&mut self, n: u64, nbytes: usize);\n    #[inline]\n    fn put_uint_le(&mut self, n: u64, nbytes: usize);\n    #[inline]\n    fn put_uint_ne(&mut self, n: u64, nbytes: usize);\n    #[inline]\n    fn put_int(&mut self, n: i64, nbytes: usize);\n    #[inline]\n    fn put_int_le(&mut self, n: i64, nbytes: usize);\n    #[inline]\n    fn put_int_ne(&mut self, n: i64, nbytes: usize);\n    #[inline]\n    fn put_f32(&mut self, n: f32);\n    #[inline]\n    fn put_f32_le(&mut self, n: f32);\n    #[inline]\n    fn put_f32_ne(&mut self, n: f32);\n    #[inline]\n    fn put_f64(&mut self, n: f64);\n    #[inline]\n    fn put_f64_le(&mut self, n: f64);\n    #[inline]\n    fn put_f64_ne(&mut self, n: f64);\n    #[inline]\n    fn limit(self, limit: usize) -> Limit<Self>\n    where\n        Self: Sized,\n    {\n        limit::new(self, limit)\n    }\n    #[cfg(feature = \"std\")]\n    #[cfg_attr(docsrs, doc(cfg(feature = \"std\")))]\n    #[inline]\n    fn writer(self) -> Writer<Self>\n    where\n        Self: Sized,\n    {\n        writer::new(self)\n    }\n    #[inline]\n    fn chain_mut<U: BufMut>(self, next: U) -> Chain<Self, U>\n    where\n        Self: Sized,\n    {\n        Chain::new(self, next)\n    }\n}\npub struct Bytes {\n    ptr: *const u8,\n    len: usize,\n    data: AtomicPtr<()>,\n    vtable: &'static Vtable,\n}\nimpl<'a> IntoIterator for &'a BytesMut {\n    type Item = &'a u8;\n    type IntoIter = core::slice::Iter<'a, u8>;\n    fn into_iter(self) -> Self::IntoIter {\n        self.as_ref().iter()\n    }\n}\nimpl Bytes {\n    #[inline]\n    #[cfg(not(all(loom, test)))]\n    pub const fn new() -> Self {\n        const EMPTY: &[u8] = &[];\n        Bytes::from_static(EMPTY)\n    }\n    #[cfg(all(loom, test))]\n    pub fn new() -> Self {\n        const EMPTY: &[u8] = &[];\n        Bytes::from_static(EMPTY)\n    }\n    #[inline]\n    #[cfg(not(all(loom, test)))]\n    pub const fn from_static(bytes: &'static [u8]) -> Self {\n        Bytes {\n            ptr: bytes.as_ptr(),\n            len: bytes.len(),\n            data: AtomicPtr::new(ptr::null_mut()),\n            vtable: &STATIC_VTABLE,\n        }\n    }\n    #[cfg(all(loom, test))]\n    pub fn from_static(bytes: &'static [u8]) -> Self {\n        Bytes {\n            ptr: bytes.as_ptr(),\n            len: bytes.len(),\n            data: AtomicPtr::new(ptr::null_mut()),\n            vtable: &STATIC_VTABLE,\n        }\n    }\n    fn new_empty_with_ptr(ptr: *const u8) -> Self {\n        debug_assert!(! ptr.is_null());\n        let ptr = without_provenance(ptr as usize);\n        Bytes {\n            ptr,\n            len: 0,\n            data: AtomicPtr::new(ptr::null_mut()),\n            vtable: &STATIC_VTABLE,\n        }\n    }\n    pub fn from_owner<T>(owner: T) -> Self\n    where\n        T: AsRef<[u8]> + Send + 'static,\n    {\n        let owned = Box::into_raw(\n            Box::new(Owned {\n                lifetime: OwnedLifetime {\n                    ref_cnt: AtomicUsize::new(1),\n                    drop: owned_box_and_drop::<T>,\n                },\n                owner,\n            }),\n        );\n        let mut ret = Bytes {\n            ptr: NonNull::dangling().as_ptr(),\n            len: 0,\n            data: AtomicPtr::new(owned.cast()),\n            vtable: &OWNED_VTABLE,\n        };\n        let buf = unsafe { &*owned }.owner.as_ref();\n        ret.ptr = buf.as_ptr();\n        ret.len = buf.len();\n        ret\n    }\n    #[inline]\n    pub const fn len(&self) -> usize {}\n    #[inline]\n    pub const fn is_empty(&self) -> bool {}\n    pub fn is_unique(&self) -> bool {}\n    pub fn copy_from_slice(data: &[u8]) -> Self {\n        data.to_vec().into()\n    }\n    pub fn slice(&self, range: impl RangeBounds<usize>) -> Self {\n        use core::ops::Bound;\n        let len = self.len();\n        let begin = match range.start_bound() {\n            Bound::Included(&n) => n,\n            Bound::Excluded(&n) => n.checked_add(1).expect(\"out of range\"),\n            Bound::Unbounded => 0,\n        };\n        let end = match range.end_bound() {\n            Bound::Included(&n) => n.checked_add(1).expect(\"out of range\"),\n            Bound::Excluded(&n) => n,\n            Bound::Unbounded => len,\n        };\n        assert!(\n            begin <= end, \"range start must not be greater than end: {:?} <= {:?}\",\n            begin, end,\n        );\n        assert!(end <= len, \"range end out of bounds: {:?} <= {:?}\", end, len,);\n        if end == begin {\n            return Bytes::new();\n        }\n        let mut ret = self.clone();\n        ret.len = end - begin;\n        ret.ptr = unsafe { ret.ptr.add(begin) };\n        ret\n    }\n    pub fn slice_ref(&self, subset: &[u8]) -> Self {\n        if subset.is_empty() {\n            return Bytes::new();\n        }\n        let bytes_p = self.as_ptr() as usize;\n        let bytes_len = self.len();\n        let sub_p = subset.as_ptr() as usize;\n        let sub_len = subset.len();\n        assert!(\n            sub_p >= bytes_p,\n            \"subset pointer ({:p}) is smaller than self pointer ({:p})\", subset.as_ptr(),\n            self.as_ptr(),\n        );\n        assert!(\n            sub_p + sub_len <= bytes_p + bytes_len,\n            \"subset is out of bounds: self = ({:p}, {}), subset = ({:p}, {})\", self\n            .as_ptr(), bytes_len, subset.as_ptr(), sub_len,\n        );\n        let sub_offset = sub_p - bytes_p;\n        self.slice(sub_offset..(sub_offset + sub_len))\n    }\n    #[must_use = \"consider Bytes::truncate if you don't need the other half\"]\n    pub fn split_off(&mut self, at: usize) -> Self {\n        if at == self.len() {\n            return Bytes::new_empty_with_ptr(self.ptr.wrapping_add(at));\n        }\n        if at == 0 {\n            return mem::replace(self, Bytes::new_empty_with_ptr(self.ptr));\n        }\n        assert!(\n            at <= self.len(), \"split_off out of bounds: {:?} <= {:?}\", at, self.len(),\n        );\n        let mut ret = self.clone();\n        self.len = at;\n        unsafe { ret.inc_start(at) };\n        ret\n    }\n    #[must_use = \"consider Bytes::advance if you don't need the other half\"]\n    pub fn split_to(&mut self, at: usize) -> Self {\n        if at == self.len() {\n            let end_ptr = self.ptr.wrapping_add(at);\n            return mem::replace(self, Bytes::new_empty_with_ptr(end_ptr));\n        }\n        if at == 0 {\n            return Bytes::new_empty_with_ptr(self.ptr);\n        }\n        assert!(\n            at <= self.len(), \"split_to out of bounds: {:?} <= {:?}\", at, self.len(),\n        );\n        let mut ret = self.clone();\n        unsafe { self.inc_start(at) };\n        ret.len = at;\n        ret\n    }\n    #[inline]\n    pub fn truncate(&mut self, len: usize) {}\n    #[inline]\n    pub fn clear(&mut self) {}\n    pub fn try_into_mut(self) -> Result<BytesMut, Bytes> {}\n    #[inline]\n    pub(crate) unsafe fn with_vtable(\n        ptr: *const u8,\n        len: usize,\n        data: AtomicPtr<()>,\n        vtable: &'static Vtable,\n    ) -> Bytes {}\n    #[inline]\n    fn as_slice(&self) -> &[u8] {\n        unsafe { slice::from_raw_parts(self.ptr, self.len) }\n    }\n    #[inline]\n    unsafe fn inc_start(&mut self, by: usize) {}\n}\n\nThe function to be tested is presented with each line formatted as 'line number + code':\n772 fn into_iter(self) -> Self::IntoIter {\n773     self.as_slice().iter()\n774 }\n\nThe path conditions that the generated test functions should satisfy are as follows:\n"
}