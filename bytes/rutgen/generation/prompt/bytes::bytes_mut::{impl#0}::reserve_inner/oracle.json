{
  "system_pt": "As a software testing expert, please generate accurate test oracles code based on the provided information. Follow these guidelines:\n1. Generate executable test oracle codes in plain text format, one per line. Do not output complete test functions, avoid additional explanations, and do not use Markdown.\n2. Combine the given function under test, context, relevant documentation, preconditions, expected return values or types, test input conditions or ranges, and existing test function prefixes to infer and generate corresponding test oracles code.\n3. Only generate necessary test oracles to ensure comprehensive validation.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/bytes_mut.rs\n// crate name is bytes\nuse core::iter::FromIterator;\nuse core::mem::{self, ManuallyDrop, MaybeUninit};\nuse core::ops::{Deref, DerefMut};\nuse core::ptr::{self, NonNull};\nuse core::{cmp, fmt, hash, isize, slice, usize};\nuse alloc::{\n    borrow::{Borrow, BorrowMut},\n    boxed::Box, string::String, vec, vec::Vec,\n};\nuse crate::buf::{IntoIter, UninitSlice};\nuse crate::bytes::Vtable;\n#[allow(unused)]\nuse crate::loom::sync::atomic::AtomicMut;\nuse crate::loom::sync::atomic::{AtomicPtr, AtomicUsize, Ordering};\nuse crate::{offset_from, Buf, BufMut, Bytes, TryGetError};\nstatic SHARED_VTABLE: Vtable = Vtable {\n    clone: shared_v_clone,\n    into_vec: shared_v_to_vec,\n    into_mut: shared_v_to_mut,\n    is_unique: shared_v_is_unique,\n    drop: shared_v_drop,\n};\nconst _: [(); 0 - mem::align_of::<Shared>() % 2] = [];\nconst KIND_ARC: usize = 0b0;\nconst KIND_VEC: usize = 0b1;\nconst KIND_MASK: usize = 0b1;\nconst MAX_ORIGINAL_CAPACITY_WIDTH: usize = 17;\nconst MIN_ORIGINAL_CAPACITY_WIDTH: usize = 10;\nconst ORIGINAL_CAPACITY_MASK: usize = 0b11100;\nconst ORIGINAL_CAPACITY_OFFSET: usize = 2;\nconst VEC_POS_OFFSET: usize = 5;\nconst MAX_VEC_POS: usize = usize::MAX >> VEC_POS_OFFSET;\nconst NOT_VEC_POS_MASK: usize = 0b11111;\n#[cfg(target_pointer_width = \"64\")]\nconst PTR_WIDTH: usize = 64;\n#[cfg(target_pointer_width = \"32\")]\nconst PTR_WIDTH: usize = 32;\npub struct BytesMut {\n    ptr: NonNull<u8>,\n    len: usize,\n    cap: usize,\n    data: *mut Shared,\n}\nstruct Shared {\n    vec: Vec<u8>,\n    original_capacity_repr: usize,\n    ref_count: AtomicUsize,\n}\nstruct Shared {\n    buf: *mut u8,\n    cap: usize,\n    ref_cnt: AtomicUsize,\n}\nimpl BytesMut {\n    #[inline]\n    pub fn with_capacity(capacity: usize) -> BytesMut {}\n    #[inline]\n    pub fn new() -> BytesMut {}\n    #[inline]\n    pub fn len(&self) -> usize {\n        self.len\n    }\n    #[inline]\n    pub fn is_empty(&self) -> bool {}\n    #[inline]\n    pub fn capacity(&self) -> usize {\n        self.cap\n    }\n    #[inline]\n    pub fn freeze(self) -> Bytes {}\n    pub fn zeroed(len: usize) -> BytesMut {}\n    #[must_use = \"consider BytesMut::truncate if you don't need the other half\"]\n    pub fn split_off(&mut self, at: usize) -> BytesMut {}\n    #[must_use = \"consider BytesMut::clear if you don't need the other half\"]\n    pub fn split(&mut self) -> BytesMut {}\n    #[must_use = \"consider BytesMut::advance if you don't need the other half\"]\n    pub fn split_to(&mut self, at: usize) -> BytesMut {}\n    pub fn truncate(&mut self, len: usize) {}\n    pub fn clear(&mut self) {}\n    pub fn resize(&mut self, new_len: usize, value: u8) {}\n    #[inline]\n    pub unsafe fn set_len(&mut self, len: usize) {}\n    #[inline]\n    pub fn reserve(&mut self, additional: usize) {}\n    fn reserve_inner(&mut self, additional: usize, allocate: bool) -> bool {\n        let len = self.len();\n        let kind = self.kind();\n        if kind == KIND_VEC {\n            unsafe {\n                let off = self.get_vec_pos();\n                if self.capacity() - self.len() + off >= additional && off >= self.len()\n                {\n                    let base_ptr = self.ptr.as_ptr().sub(off);\n                    ptr::copy_nonoverlapping(self.ptr.as_ptr(), base_ptr, self.len);\n                    self.ptr = vptr(base_ptr);\n                    self.set_vec_pos(0);\n                    self.cap += off;\n                } else {\n                    if !allocate {\n                        return false;\n                    }\n                    let mut v = ManuallyDrop::new(\n                        rebuild_vec(self.ptr.as_ptr(), self.len, self.cap, off),\n                    );\n                    v.reserve(additional);\n                    self.ptr = vptr(v.as_mut_ptr().add(off));\n                    self.cap = v.capacity() - off;\n                    debug_assert_eq!(self.len, v.len() - off);\n                }\n                return true;\n            }\n        }\n        debug_assert_eq!(kind, KIND_ARC);\n        let shared: *mut Shared = self.data;\n        let mut new_cap = match len.checked_add(additional) {\n            Some(new_cap) => new_cap,\n            None if !allocate => return false,\n            None => panic!(\"overflow\"),\n        };\n        unsafe {\n            if (*shared).is_unique() {\n                let v = &mut (*shared).vec;\n                let v_capacity = v.capacity();\n                let ptr = v.as_mut_ptr();\n                let offset = offset_from(self.ptr.as_ptr(), ptr);\n                if v_capacity >= new_cap + offset {\n                    self.cap = new_cap;\n                } else if v_capacity >= new_cap && offset >= len {\n                    ptr::copy_nonoverlapping(self.ptr.as_ptr(), ptr, len);\n                    self.ptr = vptr(ptr);\n                    self.cap = v.capacity();\n                } else {\n                    if !allocate {\n                        return false;\n                    }\n                    let off = (self.ptr.as_ptr() as usize) - (v.as_ptr() as usize);\n                    new_cap = new_cap.checked_add(off).expect(\"overflow\");\n                    let double = v.capacity().checked_shl(1).unwrap_or(new_cap);\n                    new_cap = cmp::max(double, new_cap);\n                    debug_assert!(off + len <= v.capacity());\n                    v.set_len(off + len);\n                    v.reserve(new_cap - v.len());\n                    self.ptr = vptr(v.as_mut_ptr().add(off));\n                    self.cap = v.capacity() - off;\n                }\n                return true;\n            }\n        }\n        if !allocate {\n            return false;\n        }\n        let original_capacity_repr = unsafe { (*shared).original_capacity_repr };\n        let original_capacity = original_capacity_from_repr(original_capacity_repr);\n        new_cap = cmp::max(new_cap, original_capacity);\n        let mut v = ManuallyDrop::new(Vec::with_capacity(new_cap));\n        v.extend_from_slice(self.as_ref());\n        unsafe { release_shared(shared) };\n        let data = (original_capacity_repr << ORIGINAL_CAPACITY_OFFSET) | KIND_VEC;\n        self.data = invalid_ptr(data);\n        self.ptr = vptr(v.as_mut_ptr());\n        self.cap = v.capacity();\n        debug_assert_eq!(self.len, v.len());\n        return true;\n    }\n    #[inline]\n    #[must_use = \"consider BytesMut::reserve if you need an infallible reservation\"]\n    pub fn try_reclaim(&mut self, additional: usize) -> bool {}\n    #[inline]\n    pub fn extend_from_slice(&mut self, extend: &[u8]) {}\n    pub fn unsplit(&mut self, other: BytesMut) {}\n    #[inline]\n    pub(crate) fn from_vec(vec: Vec<u8>) -> BytesMut {}\n    #[inline]\n    fn as_slice(&self) -> &[u8] {}\n    #[inline]\n    fn as_slice_mut(&mut self) -> &mut [u8] {}\n    pub(crate) unsafe fn advance_unchecked(&mut self, count: usize) {}\n    fn try_unsplit(&mut self, other: BytesMut) -> Result<(), BytesMut> {}\n    #[inline]\n    fn kind(&self) -> usize {\n        self.data as usize & KIND_MASK\n    }\n    unsafe fn promote_to_shared(&mut self, ref_cnt: usize) {}\n    #[inline]\n    unsafe fn shallow_clone(&mut self) -> BytesMut {}\n    #[inline]\n    unsafe fn get_vec_pos(&self) -> usize {\n        debug_assert_eq!(self.kind(), KIND_VEC);\n        self.data as usize >> VEC_POS_OFFSET\n    }\n    #[inline]\n    unsafe fn set_vec_pos(&mut self, pos: usize) {\n        debug_assert_eq!(self.kind(), KIND_VEC);\n        debug_assert!(pos <= MAX_VEC_POS);\n        self.data = invalid_ptr(\n            (pos << VEC_POS_OFFSET) | (self.data as usize & NOT_VEC_POS_MASK),\n        );\n    }\n    #[inline]\n    pub fn spare_capacity_mut(&mut self) -> &mut [MaybeUninit<u8>] {}\n}\nimpl Shared {\n    fn is_unique(&self) -> bool {\n        self.ref_count.load(Ordering::Acquire) == 1\n    }\n}\n#[inline]\nfn vptr(ptr: *mut u8) -> NonNull<u8> {\n    if cfg!(debug_assertions) {\n        NonNull::new(ptr).expect(\"Vec pointer should be non-null\")\n    } else {\n        unsafe { NonNull::new_unchecked(ptr) }\n    }\n}\n#[inline]\nfn offset_from(dst: *const u8, original: *const u8) -> usize {\n    dst as usize - original as usize\n}\n#[inline]\nfn invalid_ptr<T>(addr: usize) -> *mut T {\n    let ptr = core::ptr::null_mut::<u8>().wrapping_add(addr);\n    debug_assert_eq!(ptr as usize, addr);\n    ptr.cast::<T>()\n}\nfn original_capacity_from_repr(repr: usize) -> usize {\n    if repr == 0 {\n        return 0;\n    }\n    1 << (repr + (MIN_ORIGINAL_CAPACITY_WIDTH - 1))\n}\nunsafe fn release_shared(ptr: *mut Shared) {\n    if (*ptr).ref_count.fetch_sub(1, Ordering::Release) != 1 {\n        return;\n    }\n    (*ptr).ref_count.load(Ordering::Acquire);\n    drop(Box::from_raw(ptr));\n}\nunsafe fn rebuild_vec(\n    ptr: *mut u8,\n    mut len: usize,\n    mut cap: usize,\n    off: usize,\n) -> Vec<u8> {\n    let ptr = ptr.sub(off);\n    len += off;\n    cap += off;\n    Vec::from_raw_parts(ptr, len, cap)\n}\n\nThe function to be tested is presented with each line formatted as 'line number + code':\n608 fn reserve_inner(&mut self, additional: usize, allocate: bool) -> bool {\n609     let len = self.len();\n610     let kind = self.kind();\n611 \n612     if kind == KIND_VEC {\n613         // If there's enough free space before the start of the buffer, then\n614         // just copy the data backwards and reuse the already-allocated\n615         // space.\n616         //\n617         // Otherwise, since backed by a vector, use `Vec::reserve`\n618         //\n619         // We need to make sure that this optimization does not kill the\n620         // amortized runtimes of BytesMut's operations.\n621         unsafe {\n622             let off = self.get_vec_pos();\n623 \n624             // Only reuse space if we can satisfy the requested additional space.\n625             //\n626             // Also check if the value of `off` suggests that enough bytes\n627             // have been read to account for the overhead of shifting all\n628             // the data (in an amortized analysis).\n629             // Hence the condition `off >= self.len()`.\n630             //\n631             // This condition also already implies that the buffer is going\n632             // to be (at least) half-empty in the end; so we do not break\n633             // the (amortized) runtime with future resizes of the underlying\n634             // `Vec`.\n635             //\n636             // [For more details check issue #524, and PR #525.]\n637             if self.capacity() - self.len() + off >= additional && off >= self.len() {\n638                 // There's enough space, and it's not too much overhead:\n639                 // reuse the space!\n640                 //\n641                 // Just move the pointer back to the start after copying\n642                 // data back.\n643                 let base_ptr = self.ptr.as_ptr().sub(off);\n644                 // Since `off >= self.len()`, the two regions don't overlap.\n645                 ptr::copy_nonoverlapping(self.ptr.as_ptr(), base_ptr, self.len);\n646                 self.ptr = vptr(base_ptr);\n647                 self.set_vec_pos(0);\n648 \n649                 // Length stays constant, but since we moved backwards we\n650                 // can gain capacity back.\n651                 self.cap += off;\n652             } else {\n653                 if !allocate {\n654                     return false;\n655                 }\n656                 // Not enough space, or reusing might be too much overhead:\n657                 // allocate more space!\n658                 let mut v =\n659                     ManuallyDrop::new(rebuild_vec(self.ptr.as_ptr(), self.len, self.cap, off));\n660                 v.reserve(additional);\n661 \n662                 // Update the info\n663                 self.ptr = vptr(v.as_mut_ptr().add(off));\n664                 self.cap = v.capacity() - off;\n665                 debug_assert_eq!(self.len, v.len() - off);\n666             }\n667 \n668             return true;\n669         }\n670     }\n671 \n672     debug_assert_eq!(kind, KIND_ARC);\n673     let shared: *mut Shared = self.data;\n674 \n675     // Reserving involves abandoning the currently shared buffer and\n676     // allocating a new vector with the requested capacity.\n677     //\n678     // Compute the new capacity\n679     let mut new_cap = match len.checked_add(additional) {\n680         Some(new_cap) => new_cap,\n681         None if !allocate => return false,\n682         None => panic!(\"overflow\"),\n683     };\n684 \n685     unsafe {\n686         // First, try to reclaim the buffer. This is possible if the current\n687         // handle is the only outstanding handle pointing to the buffer.\n688         if (*shared).is_unique() {\n689             // This is the only handle to the buffer. It can be reclaimed.\n690             // However, before doing the work of copying data, check to make\n691             // sure that the vector has enough capacity.\n692             let v = &mut (*shared).vec;\n693 \n694             let v_capacity = v.capacity();\n695             let ptr = v.as_mut_ptr();\n696 \n697             let offset = offset_from(self.ptr.as_ptr(), ptr);\n698 \n699             // Compare the condition in the `kind == KIND_VEC` case above\n700             // for more details.\n701             if v_capacity >= new_cap + offset {\n702                 self.cap = new_cap;\n703                 // no copy is necessary\n704             } else if v_capacity >= new_cap && offset >= len {\n705                 // The capacity is sufficient, and copying is not too much\n706                 // overhead: reclaim the buffer!\n707 \n708                 // `offset >= len` means: no overlap\n709                 ptr::copy_nonoverlapping(self.ptr.as_ptr(), ptr, len);\n710 \n711                 self.ptr = vptr(ptr);\n712                 self.cap = v.capacity();\n713             } else {\n714                 if !allocate {\n715                     return false;\n716                 }\n717                 // calculate offset\n718                 let off = (self.ptr.as_ptr() as usize) - (v.as_ptr() as usize);\n719 \n720                 // new_cap is calculated in terms of `BytesMut`, not the underlying\n721                 // `Vec`, so it does not take the offset into account.\n722                 //\n723                 // Thus we have to manually add it here.\n724                 new_cap = new_cap.checked_add(off).expect(\"overflow\");\n725 \n726                 // The vector capacity is not sufficient. The reserve request is\n727                 // asking for more than the initial buffer capacity. Allocate more\n728                 // than requested if `new_cap` is not much bigger than the current\n729                 // capacity.\n730                 //\n731                 // There are some situations, using `reserve_exact` that the\n732                 // buffer capacity could be below `original_capacity`, so do a\n733                 // check.\n734                 let double = v.capacity().checked_shl(1).unwrap_or(new_cap);\n735 \n736                 new_cap = cmp::max(double, new_cap);\n737 \n738                 // No space - allocate more\n739                 //\n740                 // The length field of `Shared::vec` is not used by the `BytesMut`;\n741                 // instead we use the `len` field in the `BytesMut` itself. However,\n742                 // when calling `reserve`, it doesn't guarantee that data stored in\n743                 // the unused capacity of the vector is copied over to the new\n744                 // allocation, so we need to ensure that we don't have any data we\n745                 // care about in the unused capacity before calling `reserve`.\n746                 debug_assert!(off + len <= v.capacity());\n747                 v.set_len(off + len);\n748                 v.reserve(new_cap - v.len());\n749 \n750                 // Update the info\n751                 self.ptr = vptr(v.as_mut_ptr().add(off));\n752                 self.cap = v.capacity() - off;\n753             }\n754 \n755             return true;\n756         }\n757     }\n758     if !allocate {\n759         return false;\n760     }\n761 \n762     let original_capacity_repr = unsafe { (*shared).original_capacity_repr };\n763     let original_capacity = original_capacity_from_repr(original_capacity_repr);\n764 \n765     new_cap = cmp::max(new_cap, original_capacity);\n766 \n767     // Create a new vector to store the data\n768     let mut v = ManuallyDrop::new(Vec::with_capacity(new_cap));\n769 \n770     // Copy the bytes\n771     v.extend_from_slice(self.as_ref());\n772 \n773     // Release the shared handle. This must be done *after* the bytes are\n774     // copied.\n775     unsafe { release_shared(shared) };\n776 \n777     // Update self\n778     let data = (original_capacity_repr << ORIGINAL_CAPACITY_OFFSET) | KIND_VEC;\n779     self.data = invalid_ptr(data);\n780     self.ptr = vptr(v.as_mut_ptr());\n781     self.cap = v.capacity();\n782     debug_assert_eq!(self.len, v.len());\n783     return true;\n784 }\n\nThe path conditions that the generated test functions should satisfy are as follows:\n"
}