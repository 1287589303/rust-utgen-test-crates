{
  "system_pt": "As a software testing expert, please generate Rust test functions based on the following guidelines, focusing solely on constructing test inputs and calling the functions under test:\n1. Provide the code in plain text format, without explanations or Markdown.\n2. If the method under test belongs to a trait, construct appropriate structs within the test function, but avoid method overrides. If the method under test uses generics, instantiate them with suitable types based on the context.\n3. Generate test code with minimal scope: avoid creating external structures or implementations. Instead, define any necessary helper structures or implementations directly within the test function when required.\n4. Whenever possible, initialize the corresponding data structures using the initialization methods provided in the context.\n5. Ensure the generated function is fully implemented and can be compiled and executed directly without any missing parts.\n6. Create a minimal yet complete set of test functions, ensuring they adhere to all provided preconditions and cover boundary conditions, and reference the inferred test input conditions or ranges.\n7. Ensure the test inputs are designed to cover the inferred test input conditions or ranges as comprehensively as possible, with particular emphasis on boundary cases.\n8. Focus on crafting test inputs that effectively reveal potential bugs while meeting the specified requirements.\n9. Omit test oracles and assertions; concentrate on generating test inputs and function calls.\n10. Do not create a test module, but include intrinsic attributes like #[test] or #[should_panic] where necessary.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/bytes_mut.rs\n// crate name is bytes\nuse core::iter::FromIterator;\nuse core::mem::{self, ManuallyDrop, MaybeUninit};\nuse core::ops::{Deref, DerefMut};\nuse core::ptr::{self, NonNull};\nuse core::{cmp, fmt, hash, isize, slice, usize};\nuse alloc::{\n    borrow::{Borrow, BorrowMut},\n    boxed::Box, string::String, vec, vec::Vec,\n};\nuse crate::buf::{IntoIter, UninitSlice};\nuse crate::bytes::Vtable;\n#[allow(unused)]\nuse crate::loom::sync::atomic::AtomicMut;\nuse crate::loom::sync::atomic::{AtomicPtr, AtomicUsize, Ordering};\nuse crate::{offset_from, Buf, BufMut, Bytes, TryGetError};\nstatic SHARED_VTABLE: Vtable = Vtable {\n    clone: shared_v_clone,\n    into_vec: shared_v_to_vec,\n    into_mut: shared_v_to_mut,\n    is_unique: shared_v_is_unique,\n    drop: shared_v_drop,\n};\nconst _: [(); 0 - mem::align_of::<Shared>() % 2] = [];\nconst KIND_ARC: usize = 0b0;\nconst KIND_VEC: usize = 0b1;\nconst KIND_MASK: usize = 0b1;\nconst MAX_ORIGINAL_CAPACITY_WIDTH: usize = 17;\nconst MIN_ORIGINAL_CAPACITY_WIDTH: usize = 10;\nconst ORIGINAL_CAPACITY_MASK: usize = 0b11100;\nconst ORIGINAL_CAPACITY_OFFSET: usize = 2;\nconst VEC_POS_OFFSET: usize = 5;\nconst MAX_VEC_POS: usize = usize::MAX >> VEC_POS_OFFSET;\nconst NOT_VEC_POS_MASK: usize = 0b11111;\n#[cfg(target_pointer_width = \"64\")]\nconst PTR_WIDTH: usize = 64;\n#[cfg(target_pointer_width = \"32\")]\nconst PTR_WIDTH: usize = 32;\npub unsafe trait BufMut {\n    fn remaining_mut(&self) -> usize;\n    unsafe fn advance_mut(&mut self, cnt: usize);\n    #[inline]\n    fn has_remaining_mut(&self) -> bool;\n    #[cfg_attr(docsrs, doc(alias = \"bytes_mut\"))]\n    fn chunk_mut(&mut self) -> &mut UninitSlice;\n    #[inline]\n    fn put<T: super::Buf>(&mut self, mut src: T)\n    where\n        Self: Sized;\n    #[inline]\n    fn put_slice(&mut self, mut src: &[u8]);\n    #[inline]\n    fn put_bytes(&mut self, val: u8, mut cnt: usize);\n    #[inline]\n    fn put_u8(&mut self, n: u8) {\n        let src = [n];\n        self.put_slice(&src);\n    }\n    #[inline]\n    fn put_i8(&mut self, n: i8);\n    #[inline]\n    fn put_u16(&mut self, n: u16);\n    #[inline]\n    fn put_u16_le(&mut self, n: u16);\n    #[inline]\n    fn put_u16_ne(&mut self, n: u16);\n    #[inline]\n    fn put_i16(&mut self, n: i16);\n    #[inline]\n    fn put_i16_le(&mut self, n: i16);\n    #[inline]\n    fn put_i16_ne(&mut self, n: i16);\n    #[inline]\n    fn put_u32(&mut self, n: u32);\n    #[inline]\n    fn put_u32_le(&mut self, n: u32);\n    #[inline]\n    fn put_u32_ne(&mut self, n: u32);\n    #[inline]\n    fn put_i32(&mut self, n: i32);\n    #[inline]\n    fn put_i32_le(&mut self, n: i32);\n    #[inline]\n    fn put_i32_ne(&mut self, n: i32);\n    #[inline]\n    fn put_u64(&mut self, n: u64);\n    #[inline]\n    fn put_u64_le(&mut self, n: u64);\n    #[inline]\n    fn put_u64_ne(&mut self, n: u64);\n    #[inline]\n    fn put_i64(&mut self, n: i64);\n    #[inline]\n    fn put_i64_le(&mut self, n: i64);\n    #[inline]\n    fn put_i64_ne(&mut self, n: i64);\n    #[inline]\n    fn put_u128(&mut self, n: u128);\n    #[inline]\n    fn put_u128_le(&mut self, n: u128);\n    #[inline]\n    fn put_u128_ne(&mut self, n: u128);\n    #[inline]\n    fn put_i128(&mut self, n: i128);\n    #[inline]\n    fn put_i128_le(&mut self, n: i128);\n    #[inline]\n    fn put_i128_ne(&mut self, n: i128);\n    #[inline]\n    fn put_uint(&mut self, n: u64, nbytes: usize);\n    #[inline]\n    fn put_uint_le(&mut self, n: u64, nbytes: usize);\n    #[inline]\n    fn put_uint_ne(&mut self, n: u64, nbytes: usize);\n    #[inline]\n    fn put_int(&mut self, n: i64, nbytes: usize);\n    #[inline]\n    fn put_int_le(&mut self, n: i64, nbytes: usize);\n    #[inline]\n    fn put_int_ne(&mut self, n: i64, nbytes: usize);\n    #[inline]\n    fn put_f32(&mut self, n: f32);\n    #[inline]\n    fn put_f32_le(&mut self, n: f32);\n    #[inline]\n    fn put_f32_ne(&mut self, n: f32);\n    #[inline]\n    fn put_f64(&mut self, n: f64);\n    #[inline]\n    fn put_f64_le(&mut self, n: f64);\n    #[inline]\n    fn put_f64_ne(&mut self, n: f64);\n    #[inline]\n    fn limit(self, limit: usize) -> Limit<Self>\n    where\n        Self: Sized,\n    {\n        limit::new(self, limit)\n    }\n    #[cfg(feature = \"std\")]\n    #[cfg_attr(docsrs, doc(cfg(feature = \"std\")))]\n    #[inline]\n    fn writer(self) -> Writer<Self>\n    where\n        Self: Sized,\n    {\n        writer::new(self)\n    }\n    #[inline]\n    fn chain_mut<U: BufMut>(self, next: U) -> Chain<Self, U>\n    where\n        Self: Sized,\n    {\n        Chain::new(self, next)\n    }\n}\npub struct BytesMut {\n    ptr: NonNull<u8>,\n    len: usize,\n    cap: usize,\n    data: *mut Shared,\n}\nstruct Shared {\n    vec: Vec<u8>,\n    original_capacity_repr: usize,\n    ref_count: AtomicUsize,\n}\nstruct Shared {\n    buf: *mut u8,\n    cap: usize,\n    ref_cnt: AtomicUsize,\n}\nimpl Extend<Bytes> for BytesMut {\n    fn extend<T>(&mut self, iter: T)\n    where\n        T: IntoIterator<Item = Bytes>,\n    {\n        for bytes in iter {\n            self.extend_from_slice(&bytes)\n        }\n    }\n}\nimpl BytesMut {\n    #[inline]\n    pub fn with_capacity(capacity: usize) -> BytesMut {}\n    #[inline]\n    pub fn new() -> BytesMut {}\n    #[inline]\n    pub fn len(&self) -> usize {}\n    #[inline]\n    pub fn is_empty(&self) -> bool {}\n    #[inline]\n    pub fn capacity(&self) -> usize {}\n    #[inline]\n    pub fn freeze(self) -> Bytes {}\n    pub fn zeroed(len: usize) -> BytesMut {}\n    #[must_use = \"consider BytesMut::truncate if you don't need the other half\"]\n    pub fn split_off(&mut self, at: usize) -> BytesMut {}\n    #[must_use = \"consider BytesMut::clear if you don't need the other half\"]\n    pub fn split(&mut self) -> BytesMut {}\n    #[must_use = \"consider BytesMut::advance if you don't need the other half\"]\n    pub fn split_to(&mut self, at: usize) -> BytesMut {}\n    pub fn truncate(&mut self, len: usize) {}\n    pub fn clear(&mut self) {}\n    pub fn resize(&mut self, new_len: usize, value: u8) {}\n    #[inline]\n    pub unsafe fn set_len(&mut self, len: usize) {}\n    #[inline]\n    pub fn reserve(&mut self, additional: usize) {\n        let len = self.len();\n        let rem = self.capacity() - len;\n        if additional <= rem {\n            return;\n        }\n        let _ = self.reserve_inner(additional, true);\n    }\n    fn reserve_inner(&mut self, additional: usize, allocate: bool) -> bool {}\n    #[inline]\n    #[must_use = \"consider BytesMut::reserve if you need an infallible reservation\"]\n    pub fn try_reclaim(&mut self, additional: usize) -> bool {}\n    #[inline]\n    pub fn extend_from_slice(&mut self, extend: &[u8]) {}\n    pub fn unsplit(&mut self, other: BytesMut) {}\n    #[inline]\n    pub(crate) fn from_vec(vec: Vec<u8>) -> BytesMut {}\n    #[inline]\n    fn as_slice(&self) -> &[u8] {}\n    #[inline]\n    fn as_slice_mut(&mut self) -> &mut [u8] {}\n    pub(crate) unsafe fn advance_unchecked(&mut self, count: usize) {}\n    fn try_unsplit(&mut self, other: BytesMut) -> Result<(), BytesMut> {}\n    #[inline]\n    fn kind(&self) -> usize {}\n    unsafe fn promote_to_shared(&mut self, ref_cnt: usize) {}\n    #[inline]\n    unsafe fn shallow_clone(&mut self) -> BytesMut {}\n    #[inline]\n    unsafe fn get_vec_pos(&self) -> usize {}\n    #[inline]\n    unsafe fn set_vec_pos(&mut self, pos: usize) {}\n    #[inline]\n    pub fn spare_capacity_mut(&mut self) -> &mut [MaybeUninit<u8>] {}\n}\n\nThe function to be tested is presented with each line formatted as 'line number + code':\n1370 fn extend<T>(&mut self, iter: T)\n1371 where\n1372     T: IntoIterator<Item = u8>,\n1373 {\n1374     let iter = iter.into_iter();\n1375 \n1376     let (lower, _) = iter.size_hint();\n1377     self.reserve(lower);\n1378 \n1379     // TODO: optimize\n1380     // 1. If self.kind() == KIND_VEC, use Vec::extend\n1381     for b in iter {\n1382         self.put_u8(b);\n1383     }\n1384 }\n\nGenerate each test function in such a manner that it concurrently satisfies all the following preconditions:\n",
  "depend_pt": ""
}