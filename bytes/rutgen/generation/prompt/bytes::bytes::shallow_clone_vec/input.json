{
  "system_pt": "As a software testing expert, infer the test input conditions or ranges based on the provided information. Follow these guidelines:\n1. Provide test input conditions or ranges in one line in plain text only, without additional explanations or Markdown formatting\n2. Analyze the function under test, context, preconditions, and expected return values or types to determine appropriate test input conditions or ranges\n3. The inferred test input conditions or ranges should comprehensively satisfy all provided preconditions simultaneously.\n4. Ensure the test input conditions or ranges cover boundary cases and edge scenarios\n",
  "static_pt": "The context for the focal function is as follows:\n// src/bytes.rs\n// crate name is bytes\nuse core::iter::FromIterator;\nuse core::mem::{self, ManuallyDrop};\nuse core::ops::{Deref, RangeBounds};\nuse core::ptr::NonNull;\nuse core::{cmp, fmt, hash, ptr, slice, usize};\nuse alloc::{\n    alloc::{dealloc, Layout},\n    borrow::Borrow, boxed::Box, string::String, vec::Vec,\n};\nuse crate::buf::IntoIter;\n#[allow(unused)]\nuse crate::loom::sync::atomic::AtomicMut;\nuse crate::loom::sync::atomic::{AtomicPtr, AtomicUsize, Ordering};\nuse crate::{offset_from, Buf, BytesMut};\nstatic OWNED_VTABLE: Vtable = Vtable {\n    clone: owned_clone,\n    into_vec: owned_to_vec,\n    into_mut: owned_to_mut,\n    is_unique: owned_is_unique,\n    drop: owned_drop,\n};\nstatic PROMOTABLE_EVEN_VTABLE: Vtable = Vtable {\n    clone: promotable_even_clone,\n    into_vec: promotable_even_to_vec,\n    into_mut: promotable_even_to_mut,\n    is_unique: promotable_is_unique,\n    drop: promotable_even_drop,\n};\nstatic PROMOTABLE_ODD_VTABLE: Vtable = Vtable {\n    clone: promotable_odd_clone,\n    into_vec: promotable_odd_to_vec,\n    into_mut: promotable_odd_to_mut,\n    is_unique: promotable_is_unique,\n    drop: promotable_odd_drop,\n};\nstatic SHARED_VTABLE: Vtable = Vtable {\n    clone: shared_clone,\n    into_vec: shared_to_vec,\n    into_mut: shared_to_mut,\n    is_unique: shared_is_unique,\n    drop: shared_drop,\n};\nconst STATIC_VTABLE: Vtable = Vtable {\n    clone: static_clone,\n    into_vec: static_to_vec,\n    into_mut: static_to_mut,\n    is_unique: static_is_unique,\n    drop: static_drop,\n};\nconst _: [(); 0 - mem::align_of::<Shared>() % 2] = [];\nconst KIND_ARC: usize = 0b0;\nconst KIND_VEC: usize = 0b1;\nconst KIND_MASK: usize = 0b1;\npub(crate) struct Vtable {\n    /// fn(data, ptr, len)\n    pub clone: unsafe fn(&AtomicPtr<()>, *const u8, usize) -> Bytes,\n    /// fn(data, ptr, len)\n    ///\n    /// `into_*` consumes the `Bytes`, returning the respective value.\n    pub into_vec: unsafe fn(&AtomicPtr<()>, *const u8, usize) -> Vec<u8>,\n    pub into_mut: unsafe fn(&AtomicPtr<()>, *const u8, usize) -> BytesMut,\n    /// fn(data)\n    pub is_unique: unsafe fn(&AtomicPtr<()>) -> bool,\n    /// fn(data, ptr, len)\n    pub drop: unsafe fn(&mut AtomicPtr<()>, *const u8, usize),\n}\nstruct Shared {\n    buf: *mut u8,\n    cap: usize,\n    ref_cnt: AtomicUsize,\n}\npub struct Bytes {\n    ptr: *const u8,\n    len: usize,\n    data: AtomicPtr<()>,\n    vtable: &'static Vtable,\n}\n#[cold]\nunsafe fn shallow_clone_vec(\n    atom: &AtomicPtr<()>,\n    ptr: *const (),\n    buf: *mut u8,\n    offset: *const u8,\n    len: usize,\n) -> Bytes {\n    let shared = Box::new(Shared {\n        buf,\n        cap: offset_from(offset, buf) + len,\n        ref_cnt: AtomicUsize::new(2),\n    });\n    let shared = Box::into_raw(shared);\n    debug_assert!(\n        0 == (shared as usize & KIND_MASK),\n        \"internal: Box<Shared> should have an aligned pointer\",\n    );\n    match atom\n        .compare_exchange(ptr as _, shared as _, Ordering::AcqRel, Ordering::Acquire)\n    {\n        Ok(actual) => {\n            debug_assert!(actual as usize == ptr as usize);\n            Bytes {\n                ptr: offset,\n                len,\n                data: AtomicPtr::new(shared as _),\n                vtable: &SHARED_VTABLE,\n            }\n        }\n        Err(actual) => {\n            let shared = Box::from_raw(shared);\n            mem::forget(*shared);\n            shallow_clone_arc(actual as _, offset, len)\n        }\n    }\n}\n#[inline]\nfn offset_from(dst: *const u8, original: *const u8) -> usize {\n    dst as usize - original as usize\n}\nunsafe fn shallow_clone_arc(shared: *mut Shared, ptr: *const u8, len: usize) -> Bytes {\n    let old_size = (*shared).ref_cnt.fetch_add(1, Ordering::Relaxed);\n    if old_size > usize::MAX >> 1 {\n        crate::abort();\n    }\n    Bytes {\n        ptr,\n        len,\n        data: AtomicPtr::new(shared as _),\n        vtable: &SHARED_VTABLE,\n    }\n}\n\nThe function to be tested is presented with each line formatted as 'line number + code':\n1493 unsafe fn shallow_clone_vec(\n1494     atom: &AtomicPtr<()>,\n1495     ptr: *const (),\n1496     buf: *mut u8,\n1497     offset: *const u8,\n1498     len: usize,\n1499 ) -> Bytes {\n1500     // If the buffer is still tracked in a `Vec<u8>`. It is time to\n1501     // promote the vec to an `Arc`. This could potentially be called\n1502     // concurrently, so some care must be taken.\n1503 \n1504     // First, allocate a new `Shared` instance containing the\n1505     // `Vec` fields. It's important to note that `ptr`, `len`,\n1506     // and `cap` cannot be mutated without having `&mut self`.\n1507     // This means that these fields will not be concurrently\n1508     // updated and since the buffer hasn't been promoted to an\n1509     // `Arc`, those three fields still are the components of the\n1510     // vector.\n1511     let shared = Box::new(Shared {\n1512         buf,\n1513         cap: offset_from(offset, buf) + len,\n1514         // Initialize refcount to 2. One for this reference, and one\n1515         // for the new clone that will be returned from\n1516         // `shallow_clone`.\n1517         ref_cnt: AtomicUsize::new(2),\n1518     });\n1519 \n1520     let shared = Box::into_raw(shared);\n1521 \n1522     // The pointer should be aligned, so this assert should\n1523     // always succeed.\n1524     debug_assert!(\n1525         0 == (shared as usize & KIND_MASK),\n1526         \"internal: Box<Shared> should have an aligned pointer\",\n1527     );\n1528 \n1529     // Try compare & swapping the pointer into the `arc` field.\n1530     // `Release` is used synchronize with other threads that\n1531     // will load the `arc` field.\n1532     //\n1533     // If the `compare_exchange` fails, then the thread lost the\n1534     // race to promote the buffer to shared. The `Acquire`\n1535     // ordering will synchronize with the `compare_exchange`\n1536     // that happened in the other thread and the `Shared`\n1537     // pointed to by `actual` will be visible.\n1538     match atom.compare_exchange(ptr as _, shared as _, Ordering::AcqRel, Ordering::Acquire) {\n1539         Ok(actual) => {\n1540             debug_assert!(actual as usize == ptr as usize);\n1541             // The upgrade was successful, the new handle can be\n1542             // returned.\n1543             Bytes {\n1544                 ptr: offset,\n1545                 len,\n1546                 data: AtomicPtr::new(shared as _),\n1547                 vtable: &SHARED_VTABLE,\n1548             }\n1549         }\n1550         Err(actual) => {\n1551             // The upgrade failed, a concurrent clone happened. Release\n1552             // the allocation that was made in this thread, it will not\n1553             // be needed.\n1554             let shared = Box::from_raw(shared);\n1555             mem::forget(*shared);\n1556 \n1557             // Buffer already promoted to shared storage, so increment ref\n1558             // count.\n1559             shallow_clone_arc(actual as _, offset, len)\n1560         }\n1561     }\n1562 }\n\nWhen inferring test input conditions or ranges, consider the following preconditions and expected return values or types:\n"
}