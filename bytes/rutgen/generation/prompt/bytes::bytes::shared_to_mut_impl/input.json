{
  "system_pt": "As a software testing expert, infer the test input conditions or ranges based on the provided information. Follow these guidelines:\n1. Provide test input conditions or ranges in one line in plain text only, without additional explanations or Markdown formatting\n2. Analyze the function under test, context, preconditions, and expected return values or types to determine appropriate test input conditions or ranges\n3. The inferred test input conditions or ranges should comprehensively satisfy all provided preconditions simultaneously.\n4. Ensure the test input conditions or ranges cover boundary cases and edge scenarios\n",
  "static_pt": "The context for the focal function is as follows:\n// src/bytes.rs\n// crate name is bytes\nuse core::iter::FromIterator;\nuse core::mem::{self, ManuallyDrop};\nuse core::ops::{Deref, RangeBounds};\nuse core::ptr::NonNull;\nuse core::{cmp, fmt, hash, ptr, slice, usize};\nuse alloc::{\n    alloc::{dealloc, Layout},\n    borrow::Borrow, boxed::Box, string::String, vec::Vec,\n};\nuse crate::buf::IntoIter;\n#[allow(unused)]\nuse crate::loom::sync::atomic::AtomicMut;\nuse crate::loom::sync::atomic::{AtomicPtr, AtomicUsize, Ordering};\nuse crate::{offset_from, Buf, BytesMut};\nstatic OWNED_VTABLE: Vtable = Vtable {\n    clone: owned_clone,\n    into_vec: owned_to_vec,\n    into_mut: owned_to_mut,\n    is_unique: owned_is_unique,\n    drop: owned_drop,\n};\nstatic PROMOTABLE_EVEN_VTABLE: Vtable = Vtable {\n    clone: promotable_even_clone,\n    into_vec: promotable_even_to_vec,\n    into_mut: promotable_even_to_mut,\n    is_unique: promotable_is_unique,\n    drop: promotable_even_drop,\n};\nstatic PROMOTABLE_ODD_VTABLE: Vtable = Vtable {\n    clone: promotable_odd_clone,\n    into_vec: promotable_odd_to_vec,\n    into_mut: promotable_odd_to_mut,\n    is_unique: promotable_is_unique,\n    drop: promotable_odd_drop,\n};\nstatic SHARED_VTABLE: Vtable = Vtable {\n    clone: shared_clone,\n    into_vec: shared_to_vec,\n    into_mut: shared_to_mut,\n    is_unique: shared_is_unique,\n    drop: shared_drop,\n};\nconst STATIC_VTABLE: Vtable = Vtable {\n    clone: static_clone,\n    into_vec: static_to_vec,\n    into_mut: static_to_mut,\n    is_unique: static_is_unique,\n    drop: static_drop,\n};\nconst _: [(); 0 - mem::align_of::<Shared>() % 2] = [];\nconst KIND_ARC: usize = 0b0;\nconst KIND_VEC: usize = 0b1;\nconst KIND_MASK: usize = 0b1;\npub struct BytesMut {\n    ptr: NonNull<u8>,\n    len: usize,\n    cap: usize,\n    data: *mut Shared,\n}\nstruct Shared {\n    buf: *mut u8,\n    cap: usize,\n    ref_cnt: AtomicUsize,\n}\nimpl BytesMut {\n    #[inline]\n    pub fn with_capacity(capacity: usize) -> BytesMut {}\n    #[inline]\n    pub fn new() -> BytesMut {}\n    #[inline]\n    pub fn len(&self) -> usize {}\n    #[inline]\n    pub fn is_empty(&self) -> bool {}\n    #[inline]\n    pub fn capacity(&self) -> usize {}\n    #[inline]\n    pub fn freeze(self) -> Bytes {}\n    pub fn zeroed(len: usize) -> BytesMut {}\n    #[must_use = \"consider BytesMut::truncate if you don't need the other half\"]\n    pub fn split_off(&mut self, at: usize) -> BytesMut {}\n    #[must_use = \"consider BytesMut::clear if you don't need the other half\"]\n    pub fn split(&mut self) -> BytesMut {}\n    #[must_use = \"consider BytesMut::advance if you don't need the other half\"]\n    pub fn split_to(&mut self, at: usize) -> BytesMut {}\n    pub fn truncate(&mut self, len: usize) {}\n    pub fn clear(&mut self) {}\n    pub fn resize(&mut self, new_len: usize, value: u8) {}\n    #[inline]\n    pub unsafe fn set_len(&mut self, len: usize) {}\n    #[inline]\n    pub fn reserve(&mut self, additional: usize) {}\n    fn reserve_inner(&mut self, additional: usize, allocate: bool) -> bool {}\n    #[inline]\n    #[must_use = \"consider BytesMut::reserve if you need an infallible reservation\"]\n    pub fn try_reclaim(&mut self, additional: usize) -> bool {}\n    #[inline]\n    pub fn extend_from_slice(&mut self, extend: &[u8]) {}\n    pub fn unsplit(&mut self, other: BytesMut) {}\n    #[inline]\n    pub(crate) fn from_vec(vec: Vec<u8>) -> BytesMut {\n        let mut vec = ManuallyDrop::new(vec);\n        let ptr = vptr(vec.as_mut_ptr());\n        let len = vec.len();\n        let cap = vec.capacity();\n        let original_capacity_repr = original_capacity_to_repr(cap);\n        let data = (original_capacity_repr << ORIGINAL_CAPACITY_OFFSET) | KIND_VEC;\n        BytesMut {\n            ptr,\n            len,\n            cap,\n            data: invalid_ptr(data),\n        }\n    }\n    #[inline]\n    fn as_slice(&self) -> &[u8] {}\n    #[inline]\n    fn as_slice_mut(&mut self) -> &mut [u8] {}\n    pub(crate) unsafe fn advance_unchecked(&mut self, count: usize) {\n        if count == 0 {\n            return;\n        }\n        debug_assert!(count <= self.cap, \"internal: set_start out of bounds\");\n        let kind = self.kind();\n        if kind == KIND_VEC {\n            let pos = self.get_vec_pos() + count;\n            if pos <= MAX_VEC_POS {\n                self.set_vec_pos(pos);\n            } else {\n                self.promote_to_shared(1);\n            }\n        }\n        self.ptr = vptr(self.ptr.as_ptr().add(count));\n        self.len = self.len.checked_sub(count).unwrap_or(0);\n        self.cap -= count;\n    }\n    fn try_unsplit(&mut self, other: BytesMut) -> Result<(), BytesMut> {}\n    #[inline]\n    fn kind(&self) -> usize {}\n    unsafe fn promote_to_shared(&mut self, ref_cnt: usize) {}\n    #[inline]\n    unsafe fn shallow_clone(&mut self) -> BytesMut {}\n    #[inline]\n    unsafe fn get_vec_pos(&self) -> usize {}\n    #[inline]\n    unsafe fn set_vec_pos(&mut self, pos: usize) {}\n    #[inline]\n    pub fn spare_capacity_mut(&mut self) -> &mut [MaybeUninit<u8>] {}\n}\nunsafe fn shared_to_mut_impl(\n    shared: *mut Shared,\n    ptr: *const u8,\n    len: usize,\n) -> BytesMut {\n    if (*shared).ref_cnt.load(Ordering::Acquire) == 1 {\n        let shared = *Box::from_raw(shared);\n        let shared = ManuallyDrop::new(shared);\n        let buf = shared.buf;\n        let cap = shared.cap;\n        let off = offset_from(ptr, buf);\n        let v = Vec::from_raw_parts(buf, len + off, cap);\n        let mut b = BytesMut::from_vec(v);\n        b.advance_unchecked(off);\n        b\n    } else {\n        let v = slice::from_raw_parts(ptr, len).to_vec();\n        release_shared(shared);\n        BytesMut::from_vec(v)\n    }\n}\nunsafe fn release_shared(ptr: *mut Shared) {\n    if (*ptr).ref_cnt.fetch_sub(1, Ordering::Release) != 1 {\n        return;\n    }\n    (*ptr).ref_cnt.load(Ordering::Acquire);\n    drop(Box::from_raw(ptr));\n}\n#[inline]\nfn offset_from(dst: *const u8, original: *const u8) -> usize {\n    dst as usize - original as usize\n}\n\nThe function to be tested is presented with each line formatted as 'line number + code':\n1426 unsafe fn shared_to_mut_impl(shared: *mut Shared, ptr: *const u8, len: usize) -> BytesMut {\n1427     // The goal is to check if the current handle is the only handle\n1428     // that currently has access to the buffer. This is done by\n1429     // checking if the `ref_cnt` is currently 1.\n1430     //\n1431     // The `Acquire` ordering synchronizes with the `Release` as\n1432     // part of the `fetch_sub` in `release_shared`. The `fetch_sub`\n1433     // operation guarantees that any mutations done in other threads\n1434     // are ordered before the `ref_cnt` is decremented. As such,\n1435     // this `Acquire` will guarantee that those mutations are\n1436     // visible to the current thread.\n1437     //\n1438     // Otherwise, we take the other branch, copy the data and call `release_shared`.\n1439     if (*shared).ref_cnt.load(Ordering::Acquire) == 1 {\n1440         // Deallocate the `Shared` instance without running its destructor.\n1441         let shared = *Box::from_raw(shared);\n1442         let shared = ManuallyDrop::new(shared);\n1443         let buf = shared.buf;\n1444         let cap = shared.cap;\n1445 \n1446         // Rebuild Vec\n1447         let off = offset_from(ptr, buf);\n1448         let v = Vec::from_raw_parts(buf, len + off, cap);\n1449 \n1450         let mut b = BytesMut::from_vec(v);\n1451         b.advance_unchecked(off);\n1452         b\n1453     } else {\n1454         // Copy the data from Shared in a new Vec, then release it\n1455         let v = slice::from_raw_parts(ptr, len).to_vec();\n1456         release_shared(shared);\n1457         BytesMut::from_vec(v)\n1458     }\n1459 }\n\nWhen inferring test input conditions or ranges, consider the following preconditions and expected return values or types:\n"
}