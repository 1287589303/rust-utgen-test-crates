{
  "system_pt": "As a software testing expert, please generate accurate test oracles code based on the provided information. Follow these guidelines:\n1. Generate executable test oracle codes in plain text format, one per line. Do not output complete test functions, avoid additional explanations, and do not use Markdown.\n2. Combine the given function under test, context, relevant documentation, preconditions, expected return values or types, test input conditions or ranges, and existing test function prefixes to infer and generate corresponding test oracles code.\n3. Only generate necessary test oracles to ensure comprehensive validation.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/bytes.rs\n// crate name is bytes\nuse core::iter::FromIterator;\nuse core::mem::{self, ManuallyDrop};\nuse core::ops::{Deref, RangeBounds};\nuse core::ptr::NonNull;\nuse core::{cmp, fmt, hash, ptr, slice, usize};\nuse alloc::{\n    alloc::{dealloc, Layout},\n    borrow::Borrow, boxed::Box, string::String, vec::Vec,\n};\nuse crate::buf::IntoIter;\n#[allow(unused)]\nuse crate::loom::sync::atomic::AtomicMut;\nuse crate::loom::sync::atomic::{AtomicPtr, AtomicUsize, Ordering};\nuse crate::{offset_from, Buf, BytesMut};\nstatic OWNED_VTABLE: Vtable = Vtable {\n    clone: owned_clone,\n    into_vec: owned_to_vec,\n    into_mut: owned_to_mut,\n    is_unique: owned_is_unique,\n    drop: owned_drop,\n};\nstatic PROMOTABLE_EVEN_VTABLE: Vtable = Vtable {\n    clone: promotable_even_clone,\n    into_vec: promotable_even_to_vec,\n    into_mut: promotable_even_to_mut,\n    is_unique: promotable_is_unique,\n    drop: promotable_even_drop,\n};\nstatic PROMOTABLE_ODD_VTABLE: Vtable = Vtable {\n    clone: promotable_odd_clone,\n    into_vec: promotable_odd_to_vec,\n    into_mut: promotable_odd_to_mut,\n    is_unique: promotable_is_unique,\n    drop: promotable_odd_drop,\n};\nstatic SHARED_VTABLE: Vtable = Vtable {\n    clone: shared_clone,\n    into_vec: shared_to_vec,\n    into_mut: shared_to_mut,\n    is_unique: shared_is_unique,\n    drop: shared_drop,\n};\nconst STATIC_VTABLE: Vtable = Vtable {\n    clone: static_clone,\n    into_vec: static_to_vec,\n    into_mut: static_to_mut,\n    is_unique: static_is_unique,\n    drop: static_drop,\n};\nconst _: [(); 0 - mem::align_of::<Shared>() % 2] = [];\nconst KIND_ARC: usize = 0b0;\nconst KIND_VEC: usize = 0b1;\nconst KIND_MASK: usize = 0b1;\nstruct Shared {\n    buf: *mut u8,\n    cap: usize,\n    ref_cnt: AtomicUsize,\n}\npub struct Bytes {\n    ptr: *const u8,\n    len: usize,\n    data: AtomicPtr<()>,\n    vtable: &'static Vtable,\n}\nunsafe fn promotable_odd_clone(\n    data: &AtomicPtr<()>,\n    ptr: *const u8,\n    len: usize,\n) -> Bytes {\n    let shared = data.load(Ordering::Acquire);\n    let kind = shared as usize & KIND_MASK;\n    if kind == KIND_ARC {\n        shallow_clone_arc(shared as _, ptr, len)\n    } else {\n        debug_assert_eq!(kind, KIND_VEC);\n        shallow_clone_vec(data, shared, shared.cast(), ptr, len)\n    }\n}\n#[cold]\nunsafe fn shallow_clone_vec(\n    atom: &AtomicPtr<()>,\n    ptr: *const (),\n    buf: *mut u8,\n    offset: *const u8,\n    len: usize,\n) -> Bytes {\n    let shared = Box::new(Shared {\n        buf,\n        cap: offset_from(offset, buf) + len,\n        ref_cnt: AtomicUsize::new(2),\n    });\n    let shared = Box::into_raw(shared);\n    debug_assert!(\n        0 == (shared as usize & KIND_MASK),\n        \"internal: Box<Shared> should have an aligned pointer\",\n    );\n    match atom\n        .compare_exchange(ptr as _, shared as _, Ordering::AcqRel, Ordering::Acquire)\n    {\n        Ok(actual) => {\n            debug_assert!(actual as usize == ptr as usize);\n            Bytes {\n                ptr: offset,\n                len,\n                data: AtomicPtr::new(shared as _),\n                vtable: &SHARED_VTABLE,\n            }\n        }\n        Err(actual) => {\n            let shared = Box::from_raw(shared);\n            mem::forget(*shared);\n            shallow_clone_arc(actual as _, offset, len)\n        }\n    }\n}\nunsafe fn shallow_clone_arc(shared: *mut Shared, ptr: *const u8, len: usize) -> Bytes {\n    let old_size = (*shared).ref_cnt.fetch_add(1, Ordering::Relaxed);\n    if old_size > usize::MAX >> 1 {\n        crate::abort();\n    }\n    Bytes {\n        ptr,\n        len,\n        data: AtomicPtr::new(shared as _),\n        vtable: &SHARED_VTABLE,\n    }\n}\n\nThe function to be tested is presented with each line formatted as 'line number + code':\n1303 unsafe fn promotable_odd_clone(data: &AtomicPtr<()>, ptr: *const u8, len: usize) -> Bytes {\n1304     let shared = data.load(Ordering::Acquire);\n1305     let kind = shared as usize & KIND_MASK;\n1306 \n1307     if kind == KIND_ARC {\n1308         shallow_clone_arc(shared as _, ptr, len)\n1309     } else {\n1310         debug_assert_eq!(kind, KIND_VEC);\n1311         shallow_clone_vec(data, shared, shared.cast(), ptr, len)\n1312     }\n1313 }\n\nThe path conditions that the generated test functions should satisfy are as follows:\n"
}