{
  "system_pt": "As a software testing expert, please generate Rust test functions based on the following guidelines, focusing solely on constructing test inputs and calling the functions under test:\n1. Provide the code in plain text format, without explanations or Markdown.\n2. If the method under test belongs to a trait, construct appropriate structs within the test function, but avoid method overrides. If the method under test uses generics, instantiate them with suitable types based on the context.\n3. Generate test code with minimal scope: avoid creating external structures or implementations. Instead, define any necessary helper structures or implementations directly within the test function when required.\n4. Whenever possible, initialize the corresponding data structures using the initialization methods provided in the context.\n5. Ensure the generated function is fully implemented and can be compiled and executed directly without any missing parts.\n6. Create a minimal yet complete set of test functions, ensuring they adhere to all provided preconditions and cover boundary conditions, and reference the inferred test input conditions or ranges.\n7. Ensure the test inputs are designed to cover the inferred test input conditions or ranges as comprehensively as possible, with particular emphasis on boundary cases.\n8. Focus on crafting test inputs that effectively reveal potential bugs while meeting the specified requirements.\n9. Omit test oracles and assertions; concentrate on generating test inputs and function calls.\n10. Do not create a test module, but include intrinsic attributes like #[test] or #[should_panic] where necessary.\n",
  "static_pt": "The context for the focal function is as follows:\n// regex-automata/src/util/captures.rs\n// crate name is regex_automata\n#[cfg(feature = \"std\")]\ntype CaptureNameMap = std::collections::HashMap<Arc<str>, SmallIndex>;\n#[cfg(not(feature = \"std\"))]\ntype CaptureNameMap = alloc::collections::BTreeMap<Arc<str>, SmallIndex>;\nuse alloc::{string::String, sync::Arc, vec, vec::Vec};\nuse crate::util::{\n    interpolate,\n    primitives::{NonMaxUsize, PatternID, PatternIDError, PatternIDIter, SmallIndex},\n    search::{Match, Span},\n};\n#[derive(Debug, Default)]\nstruct GroupInfoInner {\n    slot_ranges: Vec<(SmallIndex, SmallIndex)>,\n    name_to_index: Vec<CaptureNameMap>,\n    index_to_name: Vec<Vec<Option<Arc<str>>>>,\n    memory_extra: usize,\n}\n#[derive(Clone, Copy, Debug, Default, Eq, Hash, PartialEq, PartialOrd, Ord)]\n#[repr(transparent)]\npub struct SmallIndex(u32);\n#[derive(Clone, Debug)]\npub struct GroupInfoError {\n    kind: GroupInfoErrorKind,\n}\n#[derive(Clone, Debug, Eq, PartialEq)]\npub struct SmallIndexError {\n    attempted: u64,\n}\n#[derive(Clone, Copy, Default, Eq, Hash, PartialEq, PartialOrd, Ord)]\n#[repr(transparent)]\npub struct PatternID(SmallIndex);\nimpl GroupInfoInner {\n    fn add_first_group(&mut self, pid: PatternID) {}\n    fn add_explicit_group<N: AsRef<str>>(\n        &mut self,\n        pid: PatternID,\n        group: SmallIndex,\n        maybe_name: Option<N>,\n    ) -> Result<(), GroupInfoError> {\n        let end = &mut self.slot_ranges[pid].1;\n        *end = SmallIndex::new(end.as_usize() + 2)\n            .map_err(|_| { GroupInfoError::too_many_groups(pid, group.as_usize()) })?;\n        if let Some(name) = maybe_name {\n            let name = Arc::<str>::from(name.as_ref());\n            if self.name_to_index[pid].contains_key(&*name) {\n                return Err(GroupInfoError::duplicate(pid, &name));\n            }\n            let len = name.len();\n            self.name_to_index[pid].insert(Arc::clone(&name), group);\n            self.index_to_name[pid].push(Some(name));\n            self.memory_extra += 2 * (len + core::mem::size_of::<Option<Arc<str>>>());\n            self.memory_extra += core::mem::size_of::<SmallIndex>();\n        } else {\n            self.index_to_name[pid].push(None);\n            self.memory_extra += core::mem::size_of::<Option<Arc<str>>>();\n        }\n        assert_eq!(group.one_more(), self.group_len(pid));\n        assert_eq!(group.one_more(), self.index_to_name[pid].len());\n        Ok(())\n    }\n    fn fixup_slot_ranges(&mut self) -> Result<(), GroupInfoError> {}\n    fn pattern_len(&self) -> usize {}\n    fn group_len(&self, pid: PatternID) -> usize {\n        let (start, end) = match self.slot_ranges.get(pid.as_usize()) {\n            None => return 0,\n            Some(range) => range,\n        };\n        1 + ((end.as_usize() - start.as_usize()) / 2)\n    }\n    fn small_slot_len(&self) -> SmallIndex {}\n}\nimpl SmallIndex {\n    #[cfg(any(target_pointer_width = \"32\", target_pointer_width = \"64\"))]\n    pub const MAX: SmallIndex = SmallIndex::new_unchecked(core::i32::MAX as usize - 1);\n    #[cfg(target_pointer_width = \"16\")]\n    pub const MAX: SmallIndex = SmallIndex::new_unchecked(core::isize::MAX - 1);\n    pub const LIMIT: usize = SmallIndex::MAX.as_usize() + 1;\n    pub const ZERO: SmallIndex = SmallIndex::new_unchecked(0);\n    pub const SIZE: usize = core::mem::size_of::<SmallIndex>();\n    #[inline]\n    pub fn new(index: usize) -> Result<SmallIndex, SmallIndexError> {\n        SmallIndex::try_from(index)\n    }\n    #[inline]\n    pub const fn new_unchecked(index: usize) -> SmallIndex {}\n    #[inline]\n    pub fn must(index: usize) -> SmallIndex {}\n    #[inline]\n    pub const fn as_usize(&self) -> usize {\n        self.0 as usize\n    }\n    #[inline]\n    pub const fn as_u64(&self) -> u64 {}\n    #[inline]\n    pub const fn as_u32(&self) -> u32 {}\n    #[inline]\n    pub const fn as_i32(&self) -> i32 {}\n    #[inline]\n    pub fn one_more(&self) -> usize {\n        self.as_usize() + 1\n    }\n    #[inline]\n    pub fn from_ne_bytes(bytes: [u8; 4]) -> Result<SmallIndex, SmallIndexError> {}\n    #[inline]\n    pub fn from_ne_bytes_unchecked(bytes: [u8; 4]) -> SmallIndex {}\n    #[inline]\n    pub fn to_ne_bytes(&self) -> [u8; 4] {}\n}\nimpl GroupInfoError {\n    fn too_many_patterns(err: PatternIDError) -> GroupInfoError {}\n    fn too_many_groups(pattern: PatternID, minimum: usize) -> GroupInfoError {}\n    fn missing_groups(pattern: PatternID) -> GroupInfoError {}\n    fn first_must_be_unnamed(pattern: PatternID) -> GroupInfoError {}\n    fn duplicate(pattern: PatternID, name: &str) -> GroupInfoError {\n        GroupInfoError {\n            kind: GroupInfoErrorKind::Duplicate {\n                pattern,\n                name: String::from(name),\n            },\n        }\n    }\n}\n\nThe function to be tested is presented with each line formatted as 'line number + code':\n/// Add an explicit capturing group for the given pattern with the given\n/// index. If the group has a name, then that must be given as well.\n///\n/// Note that every capturing group except for the first or zeroth group is\n/// explicit.\n///\n/// This returns an error if adding this group would result in overflowing\n/// slot indices or if a capturing group with the same name for this\n/// pattern has already been added.\n2217 fn add_explicit_group<N: AsRef<str>>(\n2218     &mut self,\n2219     pid: PatternID,\n2220     group: SmallIndex,\n2221     maybe_name: Option<N>,\n2222 ) -> Result<(), GroupInfoError> {\n2223     // We also need to check that the slot index generated for\n2224     // this group is also valid. Although, this is a little weird\n2225     // because we offset these indices below, at which point, we'll\n2226     // have to recheck them. Gosh this is annoying. Note that\n2227     // the '+2' below is OK because 'end' is guaranteed to be less\n2228     // than isize::MAX.\n2229     let end = &mut self.slot_ranges[pid].1;\n2230     *end = SmallIndex::new(end.as_usize() + 2).map_err(|_| {\n2231         GroupInfoError::too_many_groups(pid, group.as_usize())\n2232     })?;\n2233     if let Some(name) = maybe_name {\n2234         let name = Arc::<str>::from(name.as_ref());\n2235         if self.name_to_index[pid].contains_key(&*name) {\n2236             return Err(GroupInfoError::duplicate(pid, &name));\n2237         }\n2238         let len = name.len();\n2239         self.name_to_index[pid].insert(Arc::clone(&name), group);\n2240         self.index_to_name[pid].push(Some(name));\n2241         // Adds the memory used by the Arc<str> in both maps.\n2242         self.memory_extra +=\n2243             2 * (len + core::mem::size_of::<Option<Arc<str>>>());\n2244         // And also the value entry for the 'name_to_index' map.\n2245         // This is probably an underestimate for 'name_to_index' since\n2246         // hashmaps/btrees likely have some non-zero overhead, but we\n2247         // assume here that they have zero overhead.\n2248         self.memory_extra += core::mem::size_of::<SmallIndex>();\n2249     } else {\n2250         self.index_to_name[pid].push(None);\n2251         self.memory_extra += core::mem::size_of::<Option<Arc<str>>>();\n2252     }\n2253     // This is a sanity assert that checks that our group index\n2254     // is in line with the number of groups added so far for this\n2255     // pattern.\n2256     assert_eq!(group.one_more(), self.group_len(pid));\n2257     // And is also in line with the 'index_to_name' map.\n2258     assert_eq!(group.one_more(), self.index_to_name[pid].len());\n2259     Ok(())\n2260 }\n\nGenerate each test function in such a manner that it concurrently satisfies all the following preconditions:\n",
  "depend_pt": ""
}