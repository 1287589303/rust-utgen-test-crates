{
  "system_pt": "As a software testing expert, infer the test input conditions or ranges based on the provided information. Follow these guidelines:\n1. Provide test input conditions or ranges in one line in plain text only, without additional explanations or Markdown formatting\n2. Analyze the function under test, context, preconditions, and expected return values or types to determine appropriate test input conditions or ranges\n3. The inferred test input conditions or ranges should comprehensively satisfy all provided preconditions simultaneously.\n4. Ensure the test input conditions or ranges cover boundary cases and edge scenarios\n",
  "static_pt": "The context for the focal function is as follows:\n// regex-automata/src/nfa/thompson/backtrack.rs\n// crate name is regex_automata\nuse alloc::{vec, vec::Vec};\nuse crate::{\n    nfa::thompson::{self, BuildError, State, NFA},\n    util::{\n        captures::Captures, empty, iter, prefilter::Prefilter,\n        primitives::{NonMaxUsize, PatternID, SmallIndex, StateID},\n        search::{Anchored, HalfMatch, Input, Match, MatchError, Span},\n    },\n};\n#[derive(Clone, Debug)]\npub struct Cache {\n    /// Stack used on the heap for doing backtracking instead of the\n    /// traditional recursive approach. We don't want recursion because then\n    /// we're likely to hit a stack overflow for bigger regexes.\n    stack: Vec<Frame>,\n    /// The set of (StateID, HaystackOffset) pairs that have been visited\n    /// by the backtracker within a single search. If such a pair has been\n    /// visited, then we avoid doing the work for that pair again. This is\n    /// what \"bounds\" the backtracking and prevents it from having worst case\n    /// exponential time.\n    visited: Visited,\n}\n#[derive(Clone, Debug)]\nstruct Visited {\n    /// The actual underlying bitset. Each element in the bitset corresponds\n    /// to a particular (StateID, offset) pair. States correspond to the rows\n    /// and the offsets correspond to the columns.\n    ///\n    /// If our underlying NFA has N states and the haystack we're searching\n    /// has M bytes, then we have N*(M+1) entries in our bitset table. The\n    /// M+1 occurs because our matches are delayed by one byte (to support\n    /// look-around), and so we need to handle the end position itself rather\n    /// than stopping just before the end. (If there is no end position, then\n    /// it's treated as \"end-of-input,\" which is matched by things like '$'.)\n    ///\n    /// Given BITS=N*(M+1), we wind up with div_ceil(BITS, sizeof(usize))\n    /// blocks.\n    ///\n    /// We use 'usize' to represent our blocks because it makes some of the\n    /// arithmetic in 'insert' a bit nicer. For example, if we used 'u32' for\n    /// our block, we'd either need to cast u32s to usizes or usizes to u32s.\n    bitset: Vec<usize>,\n    /// The stride represents one plus length of the haystack we're searching\n    /// (as described above). The stride must be initialized for each search.\n    stride: usize,\n}\n#[derive(Clone, Debug)]\npub struct BoundedBacktracker {\n    config: Config,\n    nfa: NFA,\n}\n#[derive(Clone)]\npub struct Input<'h> {\n    haystack: &'h [u8],\n    span: Span,\n    anchored: Anchored,\n    earliest: bool,\n}\n#[derive(Debug)]\nstruct Frame<'a> {\n    /// The remaining chunks to visit for a trie state.\n    chunks: StateChunksIter<'a>,\n    /// The transitions of the current chunk that we're iterating over. Since\n    /// every trie state has at least one chunk, every frame is initialized\n    /// with the first chunk's transitions ready to be consumed.\n    transitions: core::slice::Iter<'a, Transition>,\n    /// The NFA state IDs pointing to the start of each chunk compiled by\n    /// this trie state. This ultimately gets converted to an NFA union once\n    /// the entire trie state (and all of its children) have been compiled.\n    /// The order of these matters for leftmost-first match semantics, since\n    /// earlier matches in the union are preferred over later ones.\n    union: Vec<StateID>,\n    /// The actual NFA transitions for a single chunk in a trie state. This\n    /// gets converted to an NFA sparse state, and its corresponding NFA state\n    /// ID should get added to 'union'.\n    sparse: Vec<thompson::Transition>,\n}\n#[derive(Clone, Debug, Eq, PartialEq)]\npub struct MatchError(\n    #[cfg(feature = \"alloc\")]\n    alloc::boxed::Box<MatchErrorKind>,\n    #[cfg(not(feature = \"alloc\"))]\n    MatchErrorKind,\n);\n#[derive(Clone, Debug)]\nenum Frame {\n    /// Look for a match starting at `sid` and the given position in the\n    /// haystack.\n    Step { sid: StateID, at: usize },\n    /// Reset the given `slot` to the given `offset` (which might be `None`).\n    /// This effectively gives a \"scope\" to capturing groups, such that an\n    /// offset for a particular group only gets returned if the match goes\n    /// through that capturing group. If backtracking ends up going down a\n    /// different branch that results in a different offset (or perhaps none at\n    /// all), then this \"restore capture\" frame will cause the offset to get\n    /// reset.\n    RestoreCapture { slot: SmallIndex, offset: Option<NonMaxUsize> },\n}\nimpl Cache {\n    pub fn new(re: &BoundedBacktracker) -> Cache {}\n    pub fn reset(&mut self, re: &BoundedBacktracker) {}\n    pub fn memory_usage(&self) -> usize {}\n    fn setup_search(\n        &mut self,\n        re: &BoundedBacktracker,\n        input: &Input<'_>,\n    ) -> Result<(), MatchError> {\n        self.stack.clear();\n        self.visited.setup_search(re, input)?;\n        Ok(())\n    }\n}\nimpl Visited {\n    const BLOCK_SIZE: usize = 8 * core::mem::size_of::<usize>();\n    fn new(re: &BoundedBacktracker) -> Visited {}\n    fn insert(&mut self, sid: StateID, at: usize) -> bool {}\n    fn reset(&mut self, _: &BoundedBacktracker) {}\n    fn setup_search(\n        &mut self,\n        re: &BoundedBacktracker,\n        input: &Input<'_>,\n    ) -> Result<(), MatchError> {\n        let haylen = input.get_span().len();\n        let err = || MatchError::haystack_too_long(haylen);\n        self.stride = haylen + 1;\n        let needed_capacity = match re.get_nfa().states().len().checked_mul(self.stride)\n        {\n            None => return Err(err()),\n            Some(capacity) => capacity,\n        };\n        let max_capacity = 8 * re.get_config().get_visited_capacity();\n        if needed_capacity > max_capacity {\n            return Err(err());\n        }\n        let needed_blocks = div_ceil(needed_capacity, Visited::BLOCK_SIZE);\n        self.bitset.truncate(needed_blocks);\n        for block in self.bitset.iter_mut() {\n            *block = 0;\n        }\n        if needed_blocks > self.bitset.len() {\n            self.bitset.resize(needed_blocks, 0);\n        }\n        Ok(())\n    }\n    fn memory_usage(&self) -> usize {}\n}\n\nThe function to be tested is presented with each line formatted as 'line number + code':\n/// Clears this cache. This should be called at the start of every search\n/// to ensure we start with a clean slate.\n///\n/// This also sets the length of the capturing groups used in the current\n/// search. This permits an optimization where by 'SlotTable::for_state'\n/// only returns the number of slots equivalent to the number of slots\n/// given in the 'Captures' value. This may be less than the total number\n/// of possible slots, e.g., when one only wants to track overall match\n/// offsets. This in turn permits less copying of capturing group spans\n/// in the BoundedBacktracker.\n1744 fn setup_search(\n1745     &mut self,\n1746     re: &BoundedBacktracker,\n1747     input: &Input<'_>,\n1748 ) -> Result<(), MatchError> {\n1749     self.stack.clear();\n1750     self.visited.setup_search(re, input)?;\n1751     Ok(())\n1752 }\n\nWhen inferring test input conditions or ranges, consider the following preconditions and expected return values or types:\n"
}