{
  "system_pt": "As a software testing expert, please generate Rust test functions based on the following guidelines, focusing solely on constructing test inputs and calling the functions under test:\n1. Provide the code in plain text format, without explanations or Markdown.\n2. If the method under test belongs to a trait, construct appropriate structs within the test function, but avoid method overrides. If the method under test uses generics, instantiate them with suitable types based on the context.\n3. Generate test code with minimal scope: avoid creating external structures or implementations. Instead, define any necessary helper structures or implementations directly within the test function when required.\n4. Whenever possible, initialize the corresponding data structures using the initialization methods provided in the context.\n5. Ensure the generated function is fully implemented and can be compiled and executed directly without any missing parts.\n6. Create a minimal yet complete set of test functions, ensuring they adhere to all provided preconditions and cover boundary conditions, and reference the inferred test input conditions or ranges.\n7. Ensure the test inputs are designed to cover the inferred test input conditions or ranges as comprehensively as possible, with particular emphasis on boundary cases.\n8. Focus on crafting test inputs that effectively reveal potential bugs while meeting the specified requirements.\n9. Omit test oracles and assertions; concentrate on generating test inputs and function calls.\n10. Do not create a test module, but include intrinsic attributes like #[test] or #[should_panic] where necessary.\n",
  "static_pt": "The context for the focal function is as follows:\n// regex-automata/src/nfa/thompson/backtrack.rs\n// crate name is regex_automata\nuse alloc::{vec, vec::Vec};\nuse crate::{\n    nfa::thompson::{self, BuildError, State, NFA},\n    util::{\n        captures::Captures, empty, iter, prefilter::Prefilter,\n        primitives::{NonMaxUsize, PatternID, SmallIndex, StateID},\n        search::{Anchored, HalfMatch, Input, Match, MatchError, Span},\n    },\n};\n#[derive(Clone, Debug)]\npub struct Cache {\n    /// Stack used on the heap for doing backtracking instead of the\n    /// traditional recursive approach. We don't want recursion because then\n    /// we're likely to hit a stack overflow for bigger regexes.\n    stack: Vec<Frame>,\n    /// The set of (StateID, HaystackOffset) pairs that have been visited\n    /// by the backtracker within a single search. If such a pair has been\n    /// visited, then we avoid doing the work for that pair again. This is\n    /// what \"bounds\" the backtracking and prevents it from having worst case\n    /// exponential time.\n    visited: Visited,\n}\n#[derive(Clone, Debug)]\nstruct Visited {\n    /// The actual underlying bitset. Each element in the bitset corresponds\n    /// to a particular (StateID, offset) pair. States correspond to the rows\n    /// and the offsets correspond to the columns.\n    ///\n    /// If our underlying NFA has N states and the haystack we're searching\n    /// has M bytes, then we have N*(M+1) entries in our bitset table. The\n    /// M+1 occurs because our matches are delayed by one byte (to support\n    /// look-around), and so we need to handle the end position itself rather\n    /// than stopping just before the end. (If there is no end position, then\n    /// it's treated as \"end-of-input,\" which is matched by things like '$'.)\n    ///\n    /// Given BITS=N*(M+1), we wind up with div_ceil(BITS, sizeof(usize))\n    /// blocks.\n    ///\n    /// We use 'usize' to represent our blocks because it makes some of the\n    /// arithmetic in 'insert' a bit nicer. For example, if we used 'u32' for\n    /// our block, we'd either need to cast u32s to usizes or usizes to u32s.\n    bitset: Vec<usize>,\n    /// The stride represents one plus length of the haystack we're searching\n    /// (as described above). The stride must be initialized for each search.\n    stride: usize,\n}\n#[derive(Clone, Debug)]\npub struct BoundedBacktracker {\n    config: Config,\n    nfa: NFA,\n}\n#[derive(Debug)]\nstruct Frame<'a> {\n    /// The remaining chunks to visit for a trie state.\n    chunks: StateChunksIter<'a>,\n    /// The transitions of the current chunk that we're iterating over. Since\n    /// every trie state has at least one chunk, every frame is initialized\n    /// with the first chunk's transitions ready to be consumed.\n    transitions: core::slice::Iter<'a, Transition>,\n    /// The NFA state IDs pointing to the start of each chunk compiled by\n    /// this trie state. This ultimately gets converted to an NFA union once\n    /// the entire trie state (and all of its children) have been compiled.\n    /// The order of these matters for leftmost-first match semantics, since\n    /// earlier matches in the union are preferred over later ones.\n    union: Vec<StateID>,\n    /// The actual NFA transitions for a single chunk in a trie state. This\n    /// gets converted to an NFA sparse state, and its corresponding NFA state\n    /// ID should get added to 'union'.\n    sparse: Vec<thompson::Transition>,\n}\n#[derive(Clone, Debug)]\nenum Frame {\n    /// Look for a match starting at `sid` and the given position in the\n    /// haystack.\n    Step { sid: StateID, at: usize },\n    /// Reset the given `slot` to the given `offset` (which might be `None`).\n    /// This effectively gives a \"scope\" to capturing groups, such that an\n    /// offset for a particular group only gets returned if the match goes\n    /// through that capturing group. If backtracking ends up going down a\n    /// different branch that results in a different offset (or perhaps none at\n    /// all), then this \"restore capture\" frame will cause the offset to get\n    /// reset.\n    RestoreCapture { slot: SmallIndex, offset: Option<NonMaxUsize> },\n}\nimpl Cache {\n    pub fn new(re: &BoundedBacktracker) -> Cache {\n        Cache {\n            stack: vec![],\n            visited: Visited::new(re),\n        }\n    }\n    pub fn reset(&mut self, re: &BoundedBacktracker) {}\n    pub fn memory_usage(&self) -> usize {}\n    fn setup_search(\n        &mut self,\n        re: &BoundedBacktracker,\n        input: &Input<'_>,\n    ) -> Result<(), MatchError> {}\n}\nimpl Visited {\n    const BLOCK_SIZE: usize = 8 * core::mem::size_of::<usize>();\n    fn new(re: &BoundedBacktracker) -> Visited {\n        let mut visited = Visited {\n            bitset: vec![],\n            stride: 0,\n        };\n        visited.reset(re);\n        visited\n    }\n    fn insert(&mut self, sid: StateID, at: usize) -> bool {}\n    fn reset(&mut self, _: &BoundedBacktracker) {}\n    fn setup_search(\n        &mut self,\n        re: &BoundedBacktracker,\n        input: &Input<'_>,\n    ) -> Result<(), MatchError> {}\n    fn memory_usage(&self) -> usize {}\n}\n\nThe function to be tested is presented with each line formatted as 'line number + code':\n/// Create a new [`BoundedBacktracker`] cache.\n///\n/// A potentially more convenient routine to create a cache is\n/// [`BoundedBacktracker::create_cache`], as it does not require also\n/// importing the `Cache` type.\n///\n/// If you want to reuse the returned `Cache` with some other\n/// `BoundedBacktracker`, then you must call [`Cache::reset`] with the\n/// desired `BoundedBacktracker`.\n1676 pub fn new(re: &BoundedBacktracker) -> Cache {\n1677     Cache { stack: vec![], visited: Visited::new(re) }\n1678 }\n\nGenerate each test function in such a manner that it concurrently satisfies all the following preconditions:\n",
  "depend_pt": ""
}