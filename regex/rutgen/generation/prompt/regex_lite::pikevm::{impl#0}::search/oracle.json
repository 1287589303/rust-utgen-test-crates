{
  "system_pt": "As a software testing expert, please generate accurate test oracles code based on the provided information. Follow these guidelines:\n1. Generate executable test oracle codes in plain text format, one per line. Do not output complete test functions, avoid additional explanations, and do not use Markdown.\n2. Combine the given function under test, context, relevant documentation, preconditions, expected return values or types, test input conditions or ranges, and existing test function prefixes to infer and generate corresponding test oracles code.\n3. Only generate necessary test oracles to ensure comprehensive validation.\n",
  "static_pt": "The context for the focal function is as follows:\n// regex-lite/src/pikevm.rs\n// crate name is regex_lite\nuse alloc::{vec, vec::Vec};\nuse crate::{\n    int::{NonMaxUsize, U32},\n    nfa::{State, StateID, NFA},\n    pool::CachePoolGuard, utf8,\n};\n#[derive(Clone, Debug)]\npub(crate) struct PikeVM {\n    nfa: NFA,\n}\n#[derive(Clone)]\nstruct SparseSet {\n    /// The number of elements currently in this set.\n    len: usize,\n    /// Dense contains the ids in the order in which they were inserted.\n    dense: Vec<StateID>,\n    /// Sparse maps ids to their location in dense.\n    ///\n    /// A state ID is in the set if and only if\n    /// sparse[id] < len && id == dense[sparse[id]].\n    ///\n    /// Note that these are indices into 'dense'. It's a little weird to use\n    /// StateID here, but we know our length can never exceed the bounds of\n    /// StateID (enforced by 'resize') and StateID will be at most 4 bytes\n    /// where as a usize is likely double that in most cases.\n    sparse: Vec<StateID>,\n}\n#[derive(Clone, Debug)]\nstruct SlotTable {\n    /// The actual table of offsets.\n    table: Vec<Option<NonMaxUsize>>,\n    /// The number of slots per state, i.e., the table's stride or the length\n    /// of each row.\n    slots_per_state: usize,\n    /// The number of slots in the caller-provided `Captures` value for the\n    /// current search. Setting this to `slots_per_state` is always correct,\n    /// but may be wasteful.\n    slots_for_captures: usize,\n}\n#[derive(Clone)]\npub(crate) struct NFA {\n    /// The pattern string this NFA was generated from.\n    ///\n    /// We put it here for lack of a better place to put it. ¯\\_(ツ)_/¯\n    pattern: String,\n    /// The states that make up this NFA.\n    states: Vec<State>,\n    /// The ID of the start state.\n    start: StateID,\n    /// Whether this NFA can only match at the beginning of a haystack.\n    is_start_anchored: bool,\n    /// Whether this NFA can match the empty string.\n    is_match_empty: bool,\n    /// If every match has the same number of matching capture groups, then\n    /// this corresponds to the number of groups.\n    static_explicit_captures_len: Option<usize>,\n    /// A map from capture group name to its corresponding index.\n    cap_name_to_index: CaptureNameMap,\n    /// A map from capture group index to the corresponding name, if one\n    /// exists.\n    cap_index_to_name: Vec<Option<Arc<str>>>,\n    /// Heap memory used indirectly by NFA states and other things (like the\n    /// various capturing group representations above). Since each state\n    /// might use a different amount of heap, we need to keep track of this\n    /// incrementally.\n    memory_extra: usize,\n}\n#[derive(Clone, Debug)]\npub(crate) struct Cache {\n    /// Stack used while computing epsilon closure. This effectively lets us\n    /// move what is more naturally expressed through recursion to a stack\n    /// on the heap.\n    stack: Vec<FollowEpsilon>,\n    /// The current active states being explored for the current byte in the\n    /// haystack.\n    curr: ActiveStates,\n    /// The next set of states we're building that will be explored for the\n    /// next byte in the haystack.\n    next: ActiveStates,\n}\n#[derive(Clone, Debug)]\nstruct ActiveStates {\n    /// The set of active NFA states. This set preserves insertion order, which\n    /// is critical for simulating the match semantics of backtracking regex\n    /// engines.\n    set: SparseSet,\n    /// The slots for every NFA state, where each slot stores a (possibly\n    /// absent) offset. Every capturing group has two slots. One for a start\n    /// offset and one for an end offset.\n    slot_table: SlotTable,\n}\n#[derive(Clone, Copy, Eq, Hash, PartialEq, PartialOrd, Ord)]\n#[repr(transparent)]\npub(crate) struct NonMaxUsize(NonZeroUsize);\n#[derive(Clone, Debug)]\nenum FollowEpsilon {\n    /// Explore the epsilon transitions from a state ID.\n    Explore(StateID),\n    /// Reset the given `slot` to the given `offset` (which might be `None`).\n    RestoreCapture { slot: u32, offset: Option<NonMaxUsize> },\n}\nimpl PikeVM {\n    pub(crate) fn new(nfa: NFA) -> PikeVM {}\n    pub(crate) fn nfa(&self) -> &NFA {\n        &self.nfa\n    }\n    pub(crate) fn find_iter<'r, 'h>(\n        &'r self,\n        cache: CachePoolGuard<'r>,\n        haystack: &'h [u8],\n    ) -> FindMatches<'r, 'h> {}\n    pub(crate) fn captures_iter<'r, 'h>(\n        &'r self,\n        cache: CachePoolGuard<'r>,\n        haystack: &'h [u8],\n    ) -> CapturesMatches<'r, 'h> {}\n    pub(crate) fn search(\n        &self,\n        cache: &mut Cache,\n        haystack: &[u8],\n        start: usize,\n        end: usize,\n        earliest: bool,\n        slots: &mut [Option<NonMaxUsize>],\n    ) -> bool {\n        cache.setup_search(slots.len());\n        if start > end {\n            return false;\n        }\n        assert!(\n            haystack.len() < core::usize::MAX,\n            \"byte slice lengths must be less than usize MAX\",\n        );\n        let Cache { ref mut stack, ref mut curr, ref mut next } = cache;\n        let start_id = self.nfa().start();\n        let anchored = self.nfa().is_start_anchored();\n        let mut matched = false;\n        let mut at = start;\n        while at <= end {\n            if curr.set.is_empty() {\n                if matched {\n                    break;\n                }\n                if anchored && at > start {\n                    break;\n                }\n            }\n            if !matched {\n                let slots = next.slot_table.all_absent();\n                self.epsilon_closure(stack, slots, curr, haystack, at, start_id);\n            }\n            let (ch, len) = utf8::decode_lossy(&haystack[at..]);\n            if self.nexts(stack, curr, next, haystack, at, ch, len, slots) {\n                matched = true;\n            }\n            if (earliest && matched) || len == 0 {\n                break;\n            }\n            core::mem::swap(curr, next);\n            next.set.clear();\n            at += len;\n        }\n        matched\n    }\n    fn nexts(\n        &self,\n        stack: &mut Vec<FollowEpsilon>,\n        curr: &mut ActiveStates,\n        next: &mut ActiveStates,\n        haystack: &[u8],\n        at: usize,\n        at_ch: char,\n        at_len: usize,\n        slots: &mut [Option<NonMaxUsize>],\n    ) -> bool {\n        let ActiveStates { ref set, ref mut slot_table } = *curr;\n        for sid in set.iter() {\n            if self.next(stack, slot_table, next, haystack, at, at_ch, at_len, sid) {\n                slots.copy_from_slice(slot_table.for_state(sid));\n                return true;\n            }\n        }\n        false\n    }\n    fn next(\n        &self,\n        stack: &mut Vec<FollowEpsilon>,\n        curr_slot_table: &mut SlotTable,\n        next: &mut ActiveStates,\n        haystack: &[u8],\n        at: usize,\n        at_ch: char,\n        at_len: usize,\n        sid: StateID,\n    ) -> bool {}\n    fn epsilon_closure(\n        &self,\n        stack: &mut Vec<FollowEpsilon>,\n        curr_slots: &mut [Option<NonMaxUsize>],\n        next: &mut ActiveStates,\n        haystack: &[u8],\n        at: usize,\n        sid: StateID,\n    ) {\n        stack.push(FollowEpsilon::Explore(sid));\n        while let Some(frame) = stack.pop() {\n            match frame {\n                FollowEpsilon::RestoreCapture { slot, offset } => {\n                    curr_slots[slot.as_usize()] = offset;\n                }\n                FollowEpsilon::Explore(sid) => {\n                    self.epsilon_closure_explore(\n                        stack,\n                        curr_slots,\n                        next,\n                        haystack,\n                        at,\n                        sid,\n                    );\n                }\n            }\n        }\n    }\n    fn epsilon_closure_explore(\n        &self,\n        stack: &mut Vec<FollowEpsilon>,\n        curr_slots: &mut [Option<NonMaxUsize>],\n        next: &mut ActiveStates,\n        haystack: &[u8],\n        at: usize,\n        mut sid: StateID,\n    ) {}\n}\nimpl SparseSet {\n    fn new(capacity: usize) -> SparseSet {}\n    fn resize(&mut self, new_capacity: usize) {}\n    fn capacity(&self) -> usize {}\n    fn len(&self) -> usize {}\n    fn is_empty(&self) -> bool {\n        self.len() == 0\n    }\n    fn insert(&mut self, id: StateID) -> bool {}\n    fn contains(&self, id: StateID) -> bool {}\n    fn clear(&mut self) {\n        self.len = 0;\n    }\n    fn iter(&self) -> SparseSetIter<'_> {}\n}\nimpl SlotTable {\n    fn new() -> SlotTable {}\n    fn reset(&mut self, re: &PikeVM) {}\n    fn setup_search(&mut self, captures_slot_len: usize) {}\n    fn for_state(&mut self, sid: StateID) -> &mut [Option<NonMaxUsize>] {}\n    fn all_absent(&mut self) -> &mut [Option<NonMaxUsize>] {\n        let i = self.table.len() - self.slots_per_state;\n        &mut self.table[i..i + self.slots_for_captures]\n    }\n}\nimpl NFA {\n    pub(crate) fn new(config: Config, pattern: String, hir: &Hir) -> Result<NFA, Error> {}\n    pub(crate) fn pattern(&self) -> &str {}\n    pub(crate) fn state(&self, id: StateID) -> &State {}\n    pub(crate) fn len(&self) -> usize {}\n    pub(crate) fn start(&self) -> StateID {\n        self.start\n    }\n    pub(crate) fn to_index(&self, name: &str) -> Option<usize> {}\n    pub(crate) fn capture_names(&self) -> CaptureNames<'_> {}\n    pub(crate) fn group_len(&self) -> usize {}\n    pub(crate) fn is_start_anchored(&self) -> bool {\n        self.is_start_anchored\n    }\n    pub(crate) fn static_explicit_captures_len(&self) -> Option<usize> {}\n    fn memory_usage(&self) -> usize {}\n}\nimpl Cache {\n    pub(crate) fn new(re: &PikeVM) -> Cache {}\n    fn setup_search(&mut self, captures_slot_len: usize) {\n        self.stack.clear();\n        self.curr.setup_search(captures_slot_len);\n        self.next.setup_search(captures_slot_len);\n    }\n}\npub(crate) fn decode_lossy<B: AsRef<[u8]>>(slice: B) -> (char, usize) {\n    match decode(slice) {\n        (Some(ch), size) => (ch, size),\n        (None, size) => ('\\u{FFFD}', size),\n    }\n}\n\nThe function to be tested is presented with each line formatted as 'line number + code':\n/// The implementation of standard leftmost search.\n///\n/// Capturing group spans are written to `slots`, but only if requested.\n/// `slots` can be any length. Any slot in the NFA that is activated but\n/// which is out of bounds for the given `slots` is ignored.\n 72 pub(crate) fn search(\n 73     &self,\n 74     cache: &mut Cache,\n 75     haystack: &[u8],\n 76     start: usize,\n 77     end: usize,\n 78     earliest: bool,\n 79     slots: &mut [Option<NonMaxUsize>],\n 80 ) -> bool {\n 81     cache.setup_search(slots.len());\n 82     if start > end {\n 83         return false;\n 84     }\n 85     // Why do we even care about this? Well, in our `slots` representation,\n 86     // we use usize::MAX as a sentinel to indicate \"no match.\" This isn't\n 87     // problematic so long as our haystack doesn't have a maximal length.\n 88     // Byte slices are guaranteed by Rust to have a length that fits into\n 89     // isize, and so this assert should always pass. But we put it here to\n 90     // make our assumption explicit.\n 91     assert!(\n 92         haystack.len() < core::usize::MAX,\n 93         \"byte slice lengths must be less than usize MAX\",\n 94     );\n 95 \n 96     let Cache { ref mut stack, ref mut curr, ref mut next } = cache;\n 97     let start_id = self.nfa().start();\n 98     let anchored = self.nfa().is_start_anchored();\n 99     let mut matched = false;\n100     // Yes, our search doesn't end at `end`, but includes it. This is\n101     // necessary because matches are delayed by one byte. The delay is used\n102     // to handle look-behind assertions. In the case of the PikeVM, the\n103     // delay is implemented by not considering a match to exist until it\n104     // is visited in `nexts`. Technically, we know a match exists in the\n105     // previous iteration via `epsilon_closure`.\n106     let mut at = start;\n107     while at <= end {\n108         // If we have no states left to visit, then there are some cases\n109         // where we know we can quit early or even skip ahead.\n110         if curr.set.is_empty() {\n111             // We have a match so we can quit.\n112             if matched {\n113                 break;\n114             }\n115             // If we're running an anchored search and we've advanced\n116             // beyond the start position with no other states to try, then\n117             // we will never observe a match and thus can stop.\n118             if anchored && at > start {\n119                 break;\n120             }\n121         }\n122         // Instead of using a hypothetical unanchored start state in the\n123         // NFA (which doesn't exist, but we could add it), we actually\n124         // always use its anchored starting state. As a result, when doing\n125         // an unanchored search, we need to simulate our own '(?s:.)*?'\n126         // prefix, to permit a match to appear anywhere.\n127         //\n128         // Now, we don't *have* to do things this way. We could create\n129         // a proper unanchored start state in the NFA and do one\n130         // `epsilon_closure` call from that starting state before the main\n131         // loop here. And that is just as correct. However, it turns out to\n132         // be slower than our approach here because it slightly increases\n133         // the cost of processing each byte by requiring us to visit\n134         // more NFA states to deal with the additional NFA states in the\n135         // unanchored prefix. By simulating it explicitly here, we lower\n136         // those costs substantially. The cost is itself small, but it adds\n137         // up for large haystacks.\n138         //\n139         // In order to simulate the '(?s:.)*?' prefix---which is not\n140         // greedy---we are careful not to perform an epsilon closure on\n141         // the start state if we already have a match. Namely, if we\n142         // did otherwise, we would never reach a terminating condition\n143         // because there would always be additional states to process.\n144         if !matched {\n145             // Since we are adding to the 'curr' active states and since\n146             // this is for the start ID, we use a slots slice that is\n147             // guaranteed to have the right length but where every element\n148             // is absent. This is exactly what we want, because this\n149             // epsilon closure is responsible for simulating an unanchored\n150             // '(?s:.)*?' prefix. It is specifically outside of any\n151             // capturing groups, and thus, using slots that are always\n152             // absent is correct.\n153             //\n154             // Note though that we can't just use `&mut []` here, since\n155             // this epsilon closure may traverse through `Capture` states\n156             // transitions, and thus must be able to write offsets to the\n157             // slots given which are later copied to slot values in `curr`.\n158             let slots = next.slot_table.all_absent();\n159             self.epsilon_closure(\n160                 stack, slots, curr, haystack, at, start_id,\n161             );\n162         }\n163         let (ch, len) = utf8::decode_lossy(&haystack[at..]);\n164         if self.nexts(stack, curr, next, haystack, at, ch, len, slots) {\n165             matched = true;\n166         }\n167         // Unless the caller asked us to return early, we need to mush\n168         // on to see if we can extend our match. (But note that 'nexts'\n169         // will quit right after seeing a match, as is consistent with\n170         // leftmost-first match priority.)\n171         if (earliest && matched) || len == 0 {\n172             break;\n173         }\n174         core::mem::swap(curr, next);\n175         next.set.clear();\n176         at += len;\n177     }\n178     matched\n179 }\n\nThe path conditions that the generated test functions should satisfy are as follows:\n"
}